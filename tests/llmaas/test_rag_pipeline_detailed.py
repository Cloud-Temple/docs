#!/usr/bin/env python3
"""
Test RAG Pipeline DÃ©taillÃ© - Cloud Temple LLMaaS
Script spÃ©cialisÃ© pour valider le pipeline RAG complet avec documentation exhaustive
"""

import os
import sys
import json
import time
import tempfile
import shutil
import requests
from typing import Optional, List, Any, Dict
from pathlib import Path
from dotenv import load_dotenv

# --- Imports LangChain ---
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.schema import HumanMessage

# --- Configuration ---
load_dotenv(dotenv_path=Path(__file__).parent / ".env")
API_KEY = os.getenv("LLMAAS_API_KEY", "")
BASE_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "granite-embedding:278m")
LLM_MODEL = os.getenv("DEFAULT_MODEL", "granite3.3:8b")

class RAGTestLogger:
    """Logger spÃ©cialisÃ© pour les tests RAG"""
    
    def __init__(self):
        self.steps = []
        self.start_time = time.time()
    
    def log_step(self, step: str, status: str, details: str = ""):
        """Enregistre une Ã©tape du test"""
        timestamp = time.time() - self.start_time
        self.steps.append({
            "step": step,
            "status": status,
            "details": details,
            "timestamp": f"{timestamp:.2f}s"
        })
        
        status_icon = "âœ…" if status == "SUCCESS" else "âŒ" if status == "ERROR" else "ğŸ”„"
        print(f"{status_icon} [{timestamp:6.2f}s] {step}")
        if details:
            print(f"    ğŸ“ {details}")
    
    def print_summary(self):
        """Affiche le rÃ©sumÃ© des tests"""
        total_time = time.time() - self.start_time
        success_count = sum(1 for step in self.steps if step["status"] == "SUCCESS")
        total_steps = len(self.steps)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š RÃ‰SUMÃ‰ TEST RAG PIPELINE")
        print(f"{'='*60}")
        print(f"â±ï¸  DurÃ©e totale: {total_time:.2f}s")
        print(f"âœ… Ã‰tapes rÃ©ussies: {success_count}/{total_steps}")
        print(f"ğŸ“ˆ Taux de succÃ¨s: {success_count/total_steps*100:.1f}%")
        
        return success_count == total_steps

def check_dependencies():
    """VÃ©rification des dÃ©pendances RAG"""
    logger = RAGTestLogger()
    logger.log_step("VÃ©rification dÃ©pendances", "PROGRESS")
    
    required_packages = {
        "langchain": "langchain",
        "langchain-community": "langchain_community",
        "langchain-openai": "langchain_openai",
        "faiss-cpu": "faiss",
        "requests": "requests",
        "httpx": "httpx",
        "python-dotenv": "dotenv"
    }
    
    all_ok = True
    for pkg_name, import_name in required_packages.items():
        try:
            __import__(import_name)
            logger.log_step(f"âœ“ {pkg_name}", "SUCCESS", "Disponible")
        except ImportError:
            logger.log_step(f"âœ— {pkg_name}", "ERROR", "Manquant. Installez avec: pip install " + pkg_name)
            all_ok = False
            
    if all_ok:
        logger.log_step("DÃ©pendances validÃ©es", "SUCCESS", f"{len(required_packages)} packages OK")
    
    return all_ok

def test_api_connection():
    """Test de connexion Ã  l'API LLMaaS"""
    logger = RAGTestLogger()
    logger.log_step("Test connexion API", "PROGRESS")
    
    try:
        headers = {"Authorization": f"Bearer {API_KEY}"}
        response = requests.get(f"{BASE_URL}/models", headers=headers, timeout=10)
        
        if response.status_code == 200:
            models = response.json()
            model_count = len(models.get('data', []))
            logger.log_step("Connexion API", "SUCCESS", f"{model_count} modÃ¨les disponibles")
            
            # VÃ©rifier que granite3.3:8b est disponible
            available_models = [m.get('id', '') for m in models.get('data', [])]
            if 'granite3.3:8b' in available_models:
                logger.log_step("ModÃ¨le granite3.3:8b", "SUCCESS", "Disponible")
            else:
                logger.log_step("ModÃ¨le granite3.3:8b", "ERROR", "Non trouvÃ©")
                return False
            
            return True
        else:
            logger.log_step("Connexion API", "ERROR", f"Status {response.status_code}")
            return False
            
    except Exception as e:
        logger.log_step("Connexion API", "ERROR", str(e))
        return False

class LLMaaSEmbeddings(Embeddings):
    """Classe d'embedding personnalisÃ©e pour l'API LLMaaS de Cloud Temple."""
    
    def __init__(self, api_key: str, model_name: str, logger: RAGTestLogger):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = BASE_URL
        self.logger = logger
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

    def _embed(self, texts: List[str]) -> List[List[float]]:
        """GÃ©nÃ¨re les embeddings pour une liste de textes."""
        self.logger.log_step("GÃ©nÃ©ration Embeddings", "PROGRESS", f"Vectorisation de {len(texts)} chunk(s) via LLMaaS...")
        payload = {"input": texts, "model": self.model_name}
        try:
            import httpx
            with httpx.Client(timeout=60.0) as client:
                response = client.post(f"{self.base_url}/embeddings", headers=self.headers, json=payload)
                response.raise_for_status()
                data = response.json()['data']
                data.sort(key=lambda e: e['index'])
                self.logger.log_step("Embeddings gÃ©nÃ©rÃ©s", "SUCCESS", f"{len(data)} vecteurs reÃ§us")
                return [item['embedding'] for item in data]
        except Exception as e:
            self.logger.log_step("Erreur Embeddings", "ERROR", str(e))
            raise

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embed(texts)

    def embed_query(self, text: str) -> List[float]:
        return self._embed([text])[0]

def create_test_documents():
    """CrÃ©ation de documents de test pour le RAG"""
    logger = RAGTestLogger()
    logger.log_step("CrÃ©ation documents test", "PROGRESS")
    
    # CrÃ©er un rÃ©pertoire temporaire
    temp_dir = tempfile.mkdtemp(prefix="rag_test_")
    
    # Documents de test avec contenu Cloud Temple
    documents = {
        "cloud_temple_overview.txt": """
        Cloud Temple est un fournisseur de cloud souverain franÃ§ais avec la qualification SecNumCloud de l'ANSSI.
        Cette qualification garantit le plus haut niveau de sÃ©curitÃ© et de conformitÃ© pour l'hÃ©bergement de donnÃ©es sensibles.
        Cloud Temple propose une infrastructure 100% franÃ§aise avec un contrÃ´le total des donnÃ©es.
        """,
        
        "llmaas_service.txt": """
        L'API LLMaaS de Cloud Temple permet d'accÃ©der Ã  36 modÃ¨les d'intelligence artificielle.
        Les tarifs sont de 0.9â‚¬ pour l'input et 4â‚¬ pour l'output par million de tokens.
        Le service inclut des modÃ¨les comme Granite 3.3, Qwen3, DeepSeek-R1, et Gemma 3.
        Tous les modÃ¨les sont hÃ©bergÃ©s en France avec conformitÃ© SecNumCloud.
        """,
        
        "security_compliance.txt": """
        La sÃ©curitÃ© Cloud Temple est garantie par plusieurs certifications :
        - SecNumCloud : Qualification ANSSI pour donnÃ©es sensibles
        - HDS : HÃ©bergement de DonnÃ©es de SantÃ©
        - ISO 27001 : Management de la sÃ©curitÃ© de l'information
        - RGPD : ConformitÃ© rÃ©glementation europÃ©enne
        Le chiffrement end-to-end protÃ¨ge toutes les communications.
        """,
        
        "technical_architecture.txt": """
        L'architecture technique Cloud Temple repose sur :
        - Datacenters certifiÃ©s Tier III+ en France
        - Infrastructure redondante multi-sites
        - RÃ©seau privÃ© sÃ©curisÃ© avec chiffrement
        - API REST compatible OpenAI pour LLMaaS
        - Monitoring 24/7 avec SLA 99.9%
        """,
        
        "use_cases.txt": """
        Les cas d'usage principaux de Cloud Temple LLMaaS incluent :
        - Dialogue multilingue avec 8+ langues supportÃ©es
        - Analyse de documents longs jusqu'Ã  120k tokens
        - GÃ©nÃ©ration de code dans 15+ langages
        - Analyse visuelle sans preprocessing OCR
        - Applications sÃ©curisÃ©es avec audit complet
        - DÃ©ploiements embarquÃ©s et edge computing
        """
    }
    
    # Ã‰crire les fichiers
    for filename, content in documents.items():
        file_path = Path(temp_dir) / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content.strip())
        
        logger.log_step(f"Document {filename}", "SUCCESS", f"{len(content)} caractÃ¨res")
    
    logger.log_step("Documents crÃ©Ã©s", "SUCCESS", f"{len(documents)} fichiers dans {temp_dir}")
    return temp_dir

def setup_rag_pipeline(documents_dir: str):
    """Configuration complÃ¨te du pipeline RAG avec les outils LLMaaS."""
    logger = RAGTestLogger()
    logger.log_step("Configuration pipeline RAG", "PROGRESS")
    
    try:
        # 1. Chargement des documents
        logger.log_step("Chargement documents", "PROGRESS")
        loader = DirectoryLoader(documents_dir, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
        documents = loader.load()
        logger.log_step("Documents chargÃ©s", "SUCCESS", f"{len(documents)} fichiers")
        
        # 2. Division en chunks
        logger.log_step("Division en chunks", "PROGRESS")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(documents)
        logger.log_step("Chunks crÃ©Ã©s", "SUCCESS", f"{len(splits)} chunks")
        
        # 3. CrÃ©ation des embeddings avec LLMaaS
        logger.log_step("Initialisation embeddings LLMaaS", "PROGRESS")
        embeddings = LLMaaSEmbeddings(api_key=API_KEY, model_name=EMBEDDING_MODEL, logger=logger)
        logger.log_step("Embeddings initialisÃ©s", "SUCCESS", f"ModÃ¨le: {EMBEDDING_MODEL}")
        
        # 4. CrÃ©ation de l'index vectoriel FAISS
        logger.log_step("CrÃ©ation index FAISS", "PROGRESS")
        vectorstore = FAISS.from_documents(splits, embeddings)
        logger.log_step("Index FAISS crÃ©Ã©", "SUCCESS", f"{vectorstore.index.ntotal} vecteurs")
        
        # 5. Configuration du LLM Cloud Temple avec ChatOpenAI
        logger.log_step("Configuration LLM", "PROGRESS")
        
        # Correction pour les problÃ¨mes de dÃ©finition Pydantic dans LangChain
        from langchain_core.caches import BaseCache
        from langchain_core.callbacks.base import Callbacks
        ChatOpenAI.model_rebuild()

        llm = ChatOpenAI(
            api_key=API_KEY,
            base_url=BASE_URL,
            model=LLM_MODEL,
            temperature=0.3,
            model_kwargs={"max_tokens": 300}
        )
        logger.log_step("LLM configurÃ©", "SUCCESS", f"ModÃ¨le: {LLM_MODEL}")
        
        # 6. CrÃ©ation de la chaÃ®ne RAG
        logger.log_step("CrÃ©ation chaÃ®ne RAG", "PROGRESS")
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )
        logger.log_step("ChaÃ®ne RAG crÃ©Ã©e", "SUCCESS", "RetrievalQA avec k=3")
        
        return qa_chain, vectorstore, logger
        
    except Exception as e:
        logger.log_step("Erreur pipeline", "ERROR", str(e))
        import traceback
        traceback.print_exc()
        raise

def test_rag_queries(qa_chain, vectorstore):
    """Test de requÃªtes RAG avec diffÃ©rents scÃ©narios"""
    logger = RAGTestLogger()
    logger.log_step("Tests requÃªtes RAG", "PROGRESS")
    
    # Questions de test avec rÃ©ponses attendues
    test_queries = [
        {
            "question": "Qu'est-ce que Cloud Temple et quelle est sa qualification principale ?",
            "expected_keywords": ["cloud temple", "secnumcloud", "anssi", "franÃ§ais", "souverain"],
            "category": "PrÃ©sentation gÃ©nÃ©rale"
        },
        {
            "question": "Combien coÃ»te l'utilisation de l'API LLMaaS ?",
            "expected_keywords": ["0.9", "4", "euro", "input", "output", "token"],
            "category": "Tarification"
        },
        {
            "question": "Quelles sont les certifications de sÃ©curitÃ© de Cloud Temple ?",
            "expected_keywords": ["secnumcloud", "hds", "iso", "rgpd", "anssi"],
            "category": "SÃ©curitÃ©"
        },
        {
            "question": "Quels sont les cas d'usage principaux de LLMaaS ?",
            "expected_keywords": ["dialogue", "multilingue", "code", "analyse", "documents"],
            "category": "Cas d'usage"
        },
        {
            "question": "Comment fonctionne l'architecture technique de Cloud Temple ?",
            "expected_keywords": ["datacenter", "tier", "france", "redondant", "api"],
            "category": "Architecture"
        }
    ]
    
    results = []
    
    for i, test_query in enumerate(test_queries, 1):
        question = test_query["question"]
        expected_keywords = test_query["expected_keywords"]
        category = test_query["category"]
        
        logger.log_step(f"Question {i}/{len(test_queries)}", "PROGRESS", f"{category}: {question}")
        
        try:
            # ExÃ©cution de la requÃªte RAG
            start_time = time.time()
            result = qa_chain({"query": question})
            query_time = time.time() - start_time
            
            # Extraction des rÃ©sultats
            answer = result['result']
            source_docs = result['source_documents']
            
            # Validation des mots-clÃ©s
            answer_lower = answer.lower()
            found_keywords = [kw for kw in expected_keywords if kw in answer_lower]
            keyword_score = len(found_keywords) / len(expected_keywords)
            
            # Test de recherche vectorielle
            similar_docs = vectorstore.similarity_search(question, k=3)
            
            # RÃ©sultats du test
            test_result = {
                "question": question,
                "category": category,
                "answer": answer,
                "answer_length": len(answer),
                "query_time": query_time,
                "source_count": len(source_docs),
                "keyword_score": keyword_score,
                "found_keywords": found_keywords,
                "missing_keywords": [kw for kw in expected_keywords if kw not in found_keywords],
                "similarity_docs": len(similar_docs),
                "success": keyword_score >= 0.4  # Au moins 40% des mots-clÃ©s
            }
            
            results.append(test_result)
            
            # Log des rÃ©sultats
            status = "SUCCESS" if test_result["success"] else "ERROR"
            details = f"Score: {keyword_score:.1%}, Temps: {query_time:.2f}s, Sources: {len(source_docs)}"
            logger.log_step(f"RÃ©ponse {i}", status, details)
            
            # Affichage dÃ©taillÃ©
            print(f"    ğŸ’¬ Question: {question}")
            print(f"    ğŸ¤– RÃ©ponse: {answer[:100]}...")
            print(f"    ğŸ“Š Mots-clÃ©s trouvÃ©s: {found_keywords}")
            print(f"    ğŸ“š Sources utilisÃ©es: {len(source_docs)}")
            print(f"    â±ï¸  Temps de rÃ©ponse: {query_time:.2f}s")
            print()
            
        except Exception as e:
            logger.log_step(f"Erreur question {i}", "ERROR", str(e))
            results.append({
                "question": question,
                "category": category,
                "success": False,
                "error": str(e)
            })
    
    # RÃ©sumÃ© des tests
    successful_queries = sum(1 for r in results if r.get("success", False))
    total_queries = len(results)
    
    logger.log_step("Tests terminÃ©s", "SUCCESS" if successful_queries == total_queries else "ERROR",
                   f"{successful_queries}/{total_queries} requÃªtes rÃ©ussies")
    
    return results, logger

def analyze_rag_performance(results: List[Dict], vectorstore):
    """Analyse des performances du pipeline RAG"""
    logger = RAGTestLogger()
    logger.log_step("Analyse performances", "PROGRESS")
    
    # MÃ©triques globales
    successful_results = [r for r in results if r.get("success", False)]
    
    if not successful_results:
        logger.log_step("Aucun rÃ©sultat", "ERROR", "Impossible d'analyser les performances")
        return False
    
    # Calculs de performance
    avg_query_time = sum(r.get("query_time", 0) for r in successful_results) / len(successful_results)
    avg_answer_length = sum(r.get("answer_length", 0) for r in successful_results) / len(successful_results)
    avg_keyword_score = sum(r.get("keyword_score", 0) for r in successful_results) / len(successful_results)
    avg_source_count = sum(r.get("source_count", 0) for r in successful_results) / len(successful_results)
    
    # Analyse par catÃ©gorie
    categories = {}
    for result in successful_results:
        category = result.get("category", "Unknown")
        if category not in categories:
            categories[category] = []
        categories[category].append(result)
    
    # MÃ©triques vectorstore
    total_vectors = vectorstore.index.ntotal if hasattr(vectorstore, 'index') else 0
    
    # Affichage des mÃ©triques
    print(f"\nğŸ“ˆ ANALYSE PERFORMANCES RAG")
    print(f"{'='*50}")
    print(f"â±ï¸  Temps moyen par requÃªte: {avg_query_time:.2f}s")
    print(f"ğŸ“ Longueur moyenne rÃ©ponse: {avg_answer_length:.0f} caractÃ¨res")
    print(f"ğŸ¯ Score moyen mots-clÃ©s: {avg_keyword_score:.1%}")
    print(f"ğŸ“š Sources moyennes utilisÃ©es: {avg_source_count:.1f}")
    print(f"ğŸ” Vecteurs dans l'index: {total_vectors}")
    print(f"âœ… Taux de succÃ¨s: {len(successful_results)}/{len(results)} ({len(successful_results)/len(results):.1%})")
    
    print(f"\nğŸ“Š PERFORMANCE PAR CATÃ‰GORIE")
    print(f"{'='*50}")
    for category, cat_results in categories.items():
        cat_score = sum(r.get("keyword_score", 0) for r in cat_results) / len(cat_results)
        cat_time = sum(r.get("query_time", 0) for r in cat_results) / len(cat_results)
        print(f"ğŸ·ï¸  {category}: Score {cat_score:.1%}, Temps {cat_time:.2f}s")
    
    # CritÃ¨res de validation
    performance_criteria = {
        "Temps de rÃ©ponse": avg_query_time < 8.0,  # < 8 secondes (seuil ajustÃ©)
        "Score mots-clÃ©s": avg_keyword_score >= 0.6,  # >= 60%
        "Utilisation sources": avg_source_count >= 1.0,  # Au moins 1 source
        "Taux de succÃ¨s": len(successful_results) / len(results) >= 0.8  # >= 80%
    }
    
    print(f"\nâœ… CRITÃˆRES DE VALIDATION")
    print(f"{'='*50}")
    all_criteria_met = True
    for criterion, met in performance_criteria.items():
        status = "âœ… PASS" if met else "âŒ FAIL"
        print(f"{status} {criterion}")
        if not met:
            all_criteria_met = False
    
    logger.log_step("Analyse terminÃ©e", "SUCCESS" if all_criteria_met else "ERROR",
                   f"CritÃ¨res: {sum(performance_criteria.values())}/{len(performance_criteria)}")
    
    return all_criteria_met

def main():
    """Fonction principale du test RAG"""
    print("ğŸš€ TEST RAG PIPELINE DÃ‰TAILLÃ‰ - CLOUD TEMPLE LLMAAS")
    print("=" * 70)
    print(f"ğŸ“… Date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”‘ API Key: {'âœ… ConfigurÃ©e' if API_KEY != 'your-api-key-here' else 'âŒ Non configurÃ©e'}")
    print(f"ğŸŒ Base URL: {BASE_URL}")
    print("=" * 70)
    
    # Variables pour cleanup
    temp_dir = None
    
    try:
        # 1. VÃ©rification des dÃ©pendances
        if not check_dependencies():
            print("âŒ Ã‰chec vÃ©rification dÃ©pendances")
            return False
        
        # 2. Test de connexion API
        if not test_api_connection():
            print("âŒ Ã‰chec connexion API")
            return False
        
        # 3. CrÃ©ation des documents de test
        temp_dir = create_test_documents()
        
        # 4. Configuration du pipeline RAG
        qa_chain, vectorstore, setup_logger = setup_rag_pipeline(temp_dir)
        
        # 5. Tests de requÃªtes
        results, query_logger = test_rag_queries(qa_chain, vectorstore)
        
        # 6. Analyse des performances
        performance_ok = analyze_rag_performance(results, vectorstore)
        
        # 7. RÃ©sumÃ© final
        print(f"\nğŸ RÃ‰SUMÃ‰ FINAL")
        print(f"{'='*70}")
        
        if performance_ok:
            print("ğŸ‰ SUCCÃˆS - Pipeline RAG entiÃ¨rement validÃ©!")
            print("âœ… Tous les critÃ¨res de performance sont respectÃ©s")
            print("âœ… Documentation tutorials.md RAG section validÃ©e")
            return True
        else:
            print("âš ï¸  PARTIEL - Pipeline RAG fonctionne mais performances Ã  amÃ©liorer")
            print("ğŸ”§ Certains critÃ¨res de performance ne sont pas respectÃ©s")
            return False
        
    except Exception as e:
        print(f"\nâŒ ERREUR CRITIQUE: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # Cleanup
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                print(f"\nğŸ§¹ Cleanup: RÃ©pertoire temporaire supprimÃ©")
            except Exception as e:
                print(f"\nâš ï¸  Cleanup warning: {e}")

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
