#!/usr/bin/env python3
"""
Test RAG Pipeline DÃ©taillÃ© - Cloud Temple LLMaaS
Script spÃ©cialisÃ© pour valider le pipeline RAG complet avec documentation exhaustive
"""

import os
import sys
import json
import time
import tempfile
import shutil
import requests
from typing import Optional, List, Any, Dict
from pathlib import Path

# Configuration
API_KEY = os.getenv("LLMAAS_API_KEY", "WolFH3xGSCMPvlfEru5JAt_KWZIrYreQOm1dDB2x5X4")
BASE_URL = "https://api.ai.cloud-temple.com/v1"

class RAGTestLogger:
    """Logger spÃ©cialisÃ© pour les tests RAG"""
    
    def __init__(self):
        self.steps = []
        self.start_time = time.time()
    
    def log_step(self, step: str, status: str, details: str = ""):
        """Enregistre une Ã©tape du test"""
        timestamp = time.time() - self.start_time
        self.steps.append({
            "step": step,
            "status": status,
            "details": details,
            "timestamp": f"{timestamp:.2f}s"
        })
        
        status_icon = "âœ…" if status == "SUCCESS" else "âŒ" if status == "ERROR" else "ğŸ”„"
        print(f"{status_icon} [{timestamp:6.2f}s] {step}")
        if details:
            print(f"    ğŸ“ {details}")
    
    def print_summary(self):
        """Affiche le rÃ©sumÃ© des tests"""
        total_time = time.time() - self.start_time
        success_count = sum(1 for step in self.steps if step["status"] == "SUCCESS")
        total_steps = len(self.steps)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š RÃ‰SUMÃ‰ TEST RAG PIPELINE")
        print(f"{'='*60}")
        print(f"â±ï¸  DurÃ©e totale: {total_time:.2f}s")
        print(f"âœ… Ã‰tapes rÃ©ussies: {success_count}/{total_steps}")
        print(f"ğŸ“ˆ Taux de succÃ¨s: {success_count/total_steps*100:.1f}%")
        
        return success_count == total_steps

def check_dependencies():
    """VÃ©rification et installation des dÃ©pendances RAG"""
    logger = RAGTestLogger()
    logger.log_step("VÃ©rification dÃ©pendances", "PROGRESS")
    
    required_packages = [
        "langchain",
        "langchain-community", 
        "langchain-openai",
        "faiss-cpu",
        "sentence-transformers",
        "requests"
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            if package == "langchain-community":
                import langchain_community
            elif package == "langchain-openai":
                import langchain_openai
            elif package == "faiss-cpu":
                import faiss
            elif package == "sentence-transformers":
                import sentence_transformers
            elif package == "langchain":
                import langchain
            elif package == "requests":
                import requests
            
            logger.log_step(f"âœ“ {package}", "SUCCESS", "Disponible")
        except ImportError:
            missing_packages.append(package)
            logger.log_step(f"âœ— {package}", "ERROR", "Manquant")
    
    if missing_packages:
        logger.log_step("Installation dÃ©pendances", "PROGRESS", f"Installation de {len(missing_packages)} packages")
        
        import subprocess
        for package in missing_packages:
            try:
                subprocess.check_call([
                    sys.executable, "-m", "pip", "install", package
                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                logger.log_step(f"InstallÃ© {package}", "SUCCESS")
            except subprocess.CalledProcessError:
                logger.log_step(f"Ã‰chec {package}", "ERROR")
                return False
    
    logger.log_step("DÃ©pendances validÃ©es", "SUCCESS", f"{len(required_packages)} packages OK")
    return True

def test_api_connection():
    """Test de connexion Ã  l'API LLMaaS"""
    logger = RAGTestLogger()
    logger.log_step("Test connexion API", "PROGRESS")
    
    try:
        headers = {"Authorization": f"Bearer {API_KEY}"}
        response = requests.get(f"{BASE_URL}/models", headers=headers, timeout=10)
        
        if response.status_code == 200:
            models = response.json()
            model_count = len(models.get('data', []))
            logger.log_step("Connexion API", "SUCCESS", f"{model_count} modÃ¨les disponibles")
            
            # VÃ©rifier que granite3.3:8b est disponible
            available_models = [m.get('id', '') for m in models.get('data', [])]
            if 'granite3.3:8b' in available_models:
                logger.log_step("ModÃ¨le granite3.3:8b", "SUCCESS", "Disponible")
            else:
                logger.log_step("ModÃ¨le granite3.3:8b", "ERROR", "Non trouvÃ©")
                return False
            
            return True
        else:
            logger.log_step("Connexion API", "ERROR", f"Status {response.status_code}")
            return False
            
    except Exception as e:
        logger.log_step("Connexion API", "ERROR", str(e))
        return False

class CloudTempleLLM:
    """
    Wrapper LangChain pour Cloud Temple LLMaaS
    ImplÃ©mentation complÃ¨te compatible avec l'interface LangChain LLM
    """
    
    def __init__(self, api_key: str, model_name: str = "granite3.3:8b", **kwargs):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = BASE_URL
        self.temperature = kwargs.get('temperature', 0.7)
        self.max_tokens = kwargs.get('max_tokens', 500)
        self.logger = kwargs.get('logger', RAGTestLogger())
    
    @property
    def _llm_type(self) -> str:
        return "cloud_temple_llmaas"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """Appel direct Ã  l'API LLMaaS"""
        self.logger.log_step("Appel LLM", "PROGRESS", f"Prompt: {prompt[:50]}...")
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        if stop:
            payload["stop"] = stop
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            response.raise_for_status()
            result = response.json()
            
            content = result['choices'][0]['message']['content']
            usage = result.get('usage', {})
            
            self.logger.log_step("RÃ©ponse LLM", "SUCCESS", 
                               f"Tokens: {usage.get('total_tokens', 'N/A')}, Longueur: {len(content)} chars")
            
            return content
            
        except Exception as e:
            self.logger.log_step("Erreur LLM", "ERROR", str(e))
            raise

def create_test_documents():
    """CrÃ©ation de documents de test pour le RAG"""
    logger = RAGTestLogger()
    logger.log_step("CrÃ©ation documents test", "PROGRESS")
    
    # CrÃ©er un rÃ©pertoire temporaire
    temp_dir = tempfile.mkdtemp(prefix="rag_test_")
    
    # Documents de test avec contenu Cloud Temple
    documents = {
        "cloud_temple_overview.txt": """
        Cloud Temple est un fournisseur de cloud souverain franÃ§ais avec la qualification SecNumCloud de l'ANSSI.
        Cette qualification garantit le plus haut niveau de sÃ©curitÃ© et de conformitÃ© pour l'hÃ©bergement de donnÃ©es sensibles.
        Cloud Temple propose une infrastructure 100% franÃ§aise avec un contrÃ´le total des donnÃ©es.
        """,
        
        "llmaas_service.txt": """
        L'API LLMaaS de Cloud Temple permet d'accÃ©der Ã  36 modÃ¨les d'intelligence artificielle.
        Les tarifs sont de 0.9â‚¬ pour l'input et 4â‚¬ pour l'output par million de tokens.
        Le service inclut des modÃ¨les comme Granite 3.3, Qwen3, DeepSeek-R1, et Gemma 3.
        Tous les modÃ¨les sont hÃ©bergÃ©s en France avec conformitÃ© SecNumCloud.
        """,
        
        "security_compliance.txt": """
        La sÃ©curitÃ© Cloud Temple est garantie par plusieurs certifications :
        - SecNumCloud : Qualification ANSSI pour donnÃ©es sensibles
        - HDS : HÃ©bergement de DonnÃ©es de SantÃ©
        - ISO 27001 : Management de la sÃ©curitÃ© de l'information
        - RGPD : ConformitÃ© rÃ©glementation europÃ©enne
        Le chiffrement end-to-end protÃ¨ge toutes les communications.
        """,
        
        "technical_architecture.txt": """
        L'architecture technique Cloud Temple repose sur :
        - Datacenters certifiÃ©s Tier III+ en France
        - Infrastructure redondante multi-sites
        - RÃ©seau privÃ© sÃ©curisÃ© avec chiffrement
        - API REST compatible OpenAI pour LLMaaS
        - Monitoring 24/7 avec SLA 99.9%
        """,
        
        "use_cases.txt": """
        Les cas d'usage principaux de Cloud Temple LLMaaS incluent :
        - Dialogue multilingue avec 8+ langues supportÃ©es
        - Analyse de documents longs jusqu'Ã  120k tokens
        - GÃ©nÃ©ration de code dans 15+ langages
        - Analyse visuelle sans preprocessing OCR
        - Applications sÃ©curisÃ©es avec audit complet
        - DÃ©ploiements embarquÃ©s et edge computing
        """
    }
    
    # Ã‰crire les fichiers
    for filename, content in documents.items():
        file_path = Path(temp_dir) / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content.strip())
        
        logger.log_step(f"Document {filename}", "SUCCESS", f"{len(content)} caractÃ¨res")
    
    logger.log_step("Documents crÃ©Ã©s", "SUCCESS", f"{len(documents)} fichiers dans {temp_dir}")
    return temp_dir

def setup_rag_pipeline(documents_dir: str):
    """Configuration complÃ¨te du pipeline RAG"""
    logger = RAGTestLogger()
    logger.log_step("Configuration pipeline RAG", "PROGRESS")
    
    try:
        # Imports LangChain
        from langchain_community.document_loaders import DirectoryLoader, TextLoader
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_community.vectorstores import FAISS
        from langchain_community.embeddings import HuggingFaceEmbeddings
        from langchain.chains import RetrievalQA
        
        logger.log_step("Imports LangChain", "SUCCESS")
        
        # 1. Chargement des documents
        logger.log_step("Chargement documents", "PROGRESS")
        loader = DirectoryLoader(
            documents_dir,
            glob="*.txt",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        documents = loader.load()
        logger.log_step("Documents chargÃ©s", "SUCCESS", f"{len(documents)} fichiers")
        
        # 2. Division en chunks
        logger.log_step("Division en chunks", "PROGRESS")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        splits = text_splitter.split_documents(documents)
        logger.log_step("Chunks crÃ©Ã©s", "SUCCESS", f"{len(splits)} chunks")
        
        # 3. CrÃ©ation des embeddings
        logger.log_step("Initialisation embeddings", "PROGRESS")
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        logger.log_step("Embeddings initialisÃ©s", "SUCCESS", "all-MiniLM-L6-v2")
        
        # 4. CrÃ©ation de l'index vectoriel FAISS
        logger.log_step("CrÃ©ation index FAISS", "PROGRESS")
        vectorstore = FAISS.from_documents(splits, embeddings)
        logger.log_step("Index FAISS crÃ©Ã©", "SUCCESS", f"{vectorstore.index.ntotal} vecteurs")
        
        # 5. Configuration du LLM Cloud Temple
        logger.log_step("Configuration LLM", "PROGRESS")
        llm = CloudTempleLLM(
            api_key=API_KEY,
            model_name="granite3.3:8b",
            temperature=0.3,  # Plus prÃ©cis pour RAG
            max_tokens=300,
            logger=logger
        )
        logger.log_step("LLM configurÃ©", "SUCCESS", "granite3.3:8b")
        
        # 6. CrÃ©ation de la chaÃ®ne RAG
        logger.log_step("CrÃ©ation chaÃ®ne RAG", "PROGRESS")
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 3}
            ),
            return_source_documents=True,
            verbose=False
        )
        logger.log_step("ChaÃ®ne RAG crÃ©Ã©e", "SUCCESS", "RetrievalQA avec k=3")
        
        return qa_chain, vectorstore, logger
        
    except Exception as e:
        logger.log_step("Erreur pipeline", "ERROR", str(e))
        raise

def test_rag_queries(qa_chain, vectorstore):
    """Test de requÃªtes RAG avec diffÃ©rents scÃ©narios"""
    logger = RAGTestLogger()
    logger.log_step("Tests requÃªtes RAG", "PROGRESS")
    
    # Questions de test avec rÃ©ponses attendues
    test_queries = [
        {
            "question": "Qu'est-ce que Cloud Temple et quelle est sa qualification principale ?",
            "expected_keywords": ["cloud temple", "secnumcloud", "anssi", "franÃ§ais", "souverain"],
            "category": "PrÃ©sentation gÃ©nÃ©rale"
        },
        {
            "question": "Combien coÃ»te l'utilisation de l'API LLMaaS ?",
            "expected_keywords": ["0.9", "4", "euro", "input", "output", "token"],
            "category": "Tarification"
        },
        {
            "question": "Quelles sont les certifications de sÃ©curitÃ© de Cloud Temple ?",
            "expected_keywords": ["secnumcloud", "hds", "iso", "rgpd", "anssi"],
            "category": "SÃ©curitÃ©"
        },
        {
            "question": "Quels sont les cas d'usage principaux de LLMaaS ?",
            "expected_keywords": ["dialogue", "multilingue", "code", "analyse", "documents"],
            "category": "Cas d'usage"
        },
        {
            "question": "Comment fonctionne l'architecture technique de Cloud Temple ?",
            "expected_keywords": ["datacenter", "tier", "france", "redondant", "api"],
            "category": "Architecture"
        }
    ]
    
    results = []
    
    for i, test_query in enumerate(test_queries, 1):
        question = test_query["question"]
        expected_keywords = test_query["expected_keywords"]
        category = test_query["category"]
        
        logger.log_step(f"Question {i}/{len(test_queries)}", "PROGRESS", f"{category}: {question}")
        
        try:
            # ExÃ©cution de la requÃªte RAG
            start_time = time.time()
            result = qa_chain({"query": question})
            query_time = time.time() - start_time
            
            # Extraction des rÃ©sultats
            answer = result['result']
            source_docs = result['source_documents']
            
            # Validation des mots-clÃ©s
            answer_lower = answer.lower()
            found_keywords = [kw for kw in expected_keywords if kw in answer_lower]
            keyword_score = len(found_keywords) / len(expected_keywords)
            
            # Test de recherche vectorielle
            similar_docs = vectorstore.similarity_search(question, k=3)
            
            # RÃ©sultats du test
            test_result = {
                "question": question,
                "category": category,
                "answer": answer,
                "answer_length": len(answer),
                "query_time": query_time,
                "source_count": len(source_docs),
                "keyword_score": keyword_score,
                "found_keywords": found_keywords,
                "missing_keywords": [kw for kw in expected_keywords if kw not in found_keywords],
                "similarity_docs": len(similar_docs),
                "success": keyword_score >= 0.4  # Au moins 40% des mots-clÃ©s
            }
            
            results.append(test_result)
            
            # Log des rÃ©sultats
            status = "SUCCESS" if test_result["success"] else "ERROR"
            details = f"Score: {keyword_score:.1%}, Temps: {query_time:.2f}s, Sources: {len(source_docs)}"
            logger.log_step(f"RÃ©ponse {i}", status, details)
            
            # Affichage dÃ©taillÃ©
            print(f"    ğŸ’¬ Question: {question}")
            print(f"    ğŸ¤– RÃ©ponse: {answer[:100]}...")
            print(f"    ğŸ“Š Mots-clÃ©s trouvÃ©s: {found_keywords}")
            print(f"    ğŸ“š Sources utilisÃ©es: {len(source_docs)}")
            print(f"    â±ï¸  Temps de rÃ©ponse: {query_time:.2f}s")
            print()
            
        except Exception as e:
            logger.log_step(f"Erreur question {i}", "ERROR", str(e))
            results.append({
                "question": question,
                "category": category,
                "success": False,
                "error": str(e)
            })
    
    # RÃ©sumÃ© des tests
    successful_queries = sum(1 for r in results if r.get("success", False))
    total_queries = len(results)
    
    logger.log_step("Tests terminÃ©s", "SUCCESS" if successful_queries == total_queries else "ERROR",
                   f"{successful_queries}/{total_queries} requÃªtes rÃ©ussies")
    
    return results, logger

def analyze_rag_performance(results: List[Dict], vectorstore):
    """Analyse des performances du pipeline RAG"""
    logger = RAGTestLogger()
    logger.log_step("Analyse performances", "PROGRESS")
    
    # MÃ©triques globales
    successful_results = [r for r in results if r.get("success", False)]
    
    if not successful_results:
        logger.log_step("Aucun rÃ©sultat", "ERROR", "Impossible d'analyser les performances")
        return False
    
    # Calculs de performance
    avg_query_time = sum(r.get("query_time", 0) for r in successful_results) / len(successful_results)
    avg_answer_length = sum(r.get("answer_length", 0) for r in successful_results) / len(successful_results)
    avg_keyword_score = sum(r.get("keyword_score", 0) for r in successful_results) / len(successful_results)
    avg_source_count = sum(r.get("source_count", 0) for r in successful_results) / len(successful_results)
    
    # Analyse par catÃ©gorie
    categories = {}
    for result in successful_results:
        category = result.get("category", "Unknown")
        if category not in categories:
            categories[category] = []
        categories[category].append(result)
    
    # MÃ©triques vectorstore
    total_vectors = vectorstore.index.ntotal if hasattr(vectorstore, 'index') else 0
    
    # Affichage des mÃ©triques
    print(f"\nğŸ“ˆ ANALYSE PERFORMANCES RAG")
    print(f"{'='*50}")
    print(f"â±ï¸  Temps moyen par requÃªte: {avg_query_time:.2f}s")
    print(f"ğŸ“ Longueur moyenne rÃ©ponse: {avg_answer_length:.0f} caractÃ¨res")
    print(f"ğŸ¯ Score moyen mots-clÃ©s: {avg_keyword_score:.1%}")
    print(f"ğŸ“š Sources moyennes utilisÃ©es: {avg_source_count:.1f}")
    print(f"ğŸ” Vecteurs dans l'index: {total_vectors}")
    print(f"âœ… Taux de succÃ¨s: {len(successful_results)}/{len(results)} ({len(successful_results)/len(results):.1%})")
    
    print(f"\nğŸ“Š PERFORMANCE PAR CATÃ‰GORIE")
    print(f"{'='*50}")
    for category, cat_results in categories.items():
        cat_score = sum(r.get("keyword_score", 0) for r in cat_results) / len(cat_results)
        cat_time = sum(r.get("query_time", 0) for r in cat_results) / len(cat_results)
        print(f"ğŸ·ï¸  {category}: Score {cat_score:.1%}, Temps {cat_time:.2f}s")
    
    # CritÃ¨res de validation
    performance_criteria = {
        "Temps de rÃ©ponse": avg_query_time < 5.0,  # < 5 secondes
        "Score mots-clÃ©s": avg_keyword_score >= 0.6,  # >= 60%
        "Utilisation sources": avg_source_count >= 1.0,  # Au moins 1 source
        "Taux de succÃ¨s": len(successful_results) / len(results) >= 0.8  # >= 80%
    }
    
    print(f"\nâœ… CRITÃˆRES DE VALIDATION")
    print(f"{'='*50}")
    all_criteria_met = True
    for criterion, met in performance_criteria.items():
        status = "âœ… PASS" if met else "âŒ FAIL"
        print(f"{status} {criterion}")
        if not met:
            all_criteria_met = False
    
    logger.log_step("Analyse terminÃ©e", "SUCCESS" if all_criteria_met else "ERROR",
                   f"CritÃ¨res: {sum(performance_criteria.values())}/{len(performance_criteria)}")
    
    return all_criteria_met

def main():
    """Fonction principale du test RAG"""
    print("ğŸš€ TEST RAG PIPELINE DÃ‰TAILLÃ‰ - CLOUD TEMPLE LLMAAS")
    print("=" * 70)
    print(f"ğŸ“… Date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”‘ API Key: {'âœ… ConfigurÃ©e' if API_KEY != 'your-api-key-here' else 'âŒ Non configurÃ©e'}")
    print(f"ğŸŒ Base URL: {BASE_URL}")
    print("=" * 70)
    
    # Variables pour cleanup
    temp_dir = None
    
    try:
        # 1. VÃ©rification des dÃ©pendances
        if not check_dependencies():
            print("âŒ Ã‰chec vÃ©rification dÃ©pendances")
            return False
        
        # 2. Test de connexion API
        if not test_api_connection():
            print("âŒ Ã‰chec connexion API")
            return False
        
        # 3. CrÃ©ation des documents de test
        temp_dir = create_test_documents()
        
        # 4. Configuration du pipeline RAG
        qa_chain, vectorstore, setup_logger = setup_rag_pipeline(temp_dir)
        
        # 5. Tests de requÃªtes
        results, query_logger = test_rag_queries(qa_chain, vectorstore)
        
        # 6. Analyse des performances
        performance_ok = analyze_rag_performance(results, vectorstore)
        
        # 7. RÃ©sumÃ© final
        print(f"\nğŸ RÃ‰SUMÃ‰ FINAL")
        print(f"{'='*70}")
        
        if performance_ok:
            print("ğŸ‰ SUCCÃˆS - Pipeline RAG entiÃ¨rement validÃ©!")
            print("âœ… Tous les critÃ¨res de performance sont respectÃ©s")
            print("âœ… Documentation tutorials.md RAG section validÃ©e")
            return True
        else:
            print("âš ï¸  PARTIEL - Pipeline RAG fonctionne mais performances Ã  amÃ©liorer")
            print("ğŸ”§ Certains critÃ¨res de performance ne sont pas respectÃ©s")
            return False
        
    except Exception as e:
        print(f"\nâŒ ERREUR CRITIQUE: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # Cleanup
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                print(f"\nğŸ§¹ Cleanup: RÃ©pertoire temporaire supprimÃ©")
            except Exception as e:
                print(f"\nâš ï¸  Cleanup warning: {e}")

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
