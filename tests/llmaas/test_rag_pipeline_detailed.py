#!/usr/bin/env python3
"""
Test RAG Pipeline D√©taill√© - Cloud Temple LLMaaS
Script sp√©cialis√© pour valider le pipeline RAG complet avec documentation exhaustive
"""

import os
import sys
import json
import time
import tempfile
import shutil
import requests
from typing import Optional, List, Any, Dict
from pathlib import Path

# Configuration
API_KEY = os.getenv("LLMAAS_API_KEY", "WolFH3xGSCMPvlfEru5JAt_KWZIrYreQOm1dDB2x5X4")
BASE_URL = "https://api.ai.cloud-temple.com/v1"

class RAGTestLogger:
    """Logger sp√©cialis√© pour les tests RAG"""
    
    def __init__(self):
        self.steps = []
        self.start_time = time.time()
    
    def log_step(self, step: str, status: str, details: str = ""):
        """Enregistre une √©tape du test"""
        timestamp = time.time() - self.start_time
        self.steps.append({
            "step": step,
            "status": status,
            "details": details,
            "timestamp": f"{timestamp:.2f}s"
        })
        
        status_icon = "‚úÖ" if status == "SUCCESS" else "‚ùå" if status == "ERROR" else "üîÑ"
        print(f"{status_icon} [{timestamp:6.2f}s] {step}")
        if details:
            print(f"    üìù {details}")
    
    def print_summary(self):
        """Affiche le r√©sum√© des tests"""
        total_time = time.time() - self.start_time
        success_count = sum(1 for step in self.steps if step["status"] == "SUCCESS")
        total_steps = len(self.steps)
        
        print(f"\n{'='*60}")
        print(f"üìä R√âSUM√â TEST RAG PIPELINE")
        print(f"{'='*60}")
        print(f"‚è±Ô∏è  Dur√©e totale: {total_time:.2f}s")
        print(f"‚úÖ √âtapes r√©ussies: {success_count}/{total_steps}")
        print(f"üìà Taux de succ√®s: {success_count/total_steps*100:.1f}%")
        
        return success_count == total_steps

def check_dependencies():
    """V√©rification et installation des d√©pendances RAG"""
    logger = RAGTestLogger()
    logger.log_step("V√©rification d√©pendances", "PROGRESS")
    
    required_packages = [
        "langchain",
        "langchain-community", 
        "langchain-openai",
        "faiss-cpu",
        "sentence-transformers",
        "requests"
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            if package == "langchain-community":
                import langchain_community
            elif package == "langchain-openai":
                import langchain_openai
            elif package == "faiss-cpu":
                import faiss
            elif package == "sentence-transformers":
                import sentence_transformers
            elif package == "langchain":
                import langchain
            elif package == "requests":
                import requests
            
            logger.log_step(f"‚úì {package}", "SUCCESS", "Disponible")
        except ImportError:
            missing_packages.append(package)
            logger.log_step(f"‚úó {package}", "ERROR", "Manquant")
    
    if missing_packages:
        logger.log_step("Installation d√©pendances", "PROGRESS", f"Installation de {len(missing_packages)} packages")
        
        import subprocess
        for package in missing_packages:
            try:
                subprocess.check_call([
                    sys.executable, "-m", "pip", "install", package
                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                logger.log_step(f"Install√© {package}", "SUCCESS")
            except subprocess.CalledProcessError:
                logger.log_step(f"√âchec {package}", "ERROR")
                return False
    
    logger.log_step("D√©pendances valid√©es", "SUCCESS", f"{len(required_packages)} packages OK")
    return True

def test_api_connection():
    """Test de connexion √† l'API LLMaaS"""
    logger = RAGTestLogger()
    logger.log_step("Test connexion API", "PROGRESS")
    
    try:
        headers = {"Authorization": f"Bearer {API_KEY}"}
        response = requests.get(f"{BASE_URL}/models", headers=headers, timeout=10)
        
        if response.status_code == 200:
            models = response.json()
            model_count = len(models.get('data', []))
            logger.log_step("Connexion API", "SUCCESS", f"{model_count} mod√®les disponibles")
            
            # V√©rifier que granite3.3:8b est disponible
            available_models = [m.get('id', '') for m in models.get('data', [])]
            if 'granite3.3:8b' in available_models:
                logger.log_step("Mod√®le granite3.3:8b", "SUCCESS", "Disponible")
            else:
                logger.log_step("Mod√®le granite3.3:8b", "ERROR", "Non trouv√©")
                return False
            
            return True
        else:
            logger.log_step("Connexion API", "ERROR", f"Status {response.status_code}")
            return False
            
    except Exception as e:
        logger.log_step("Connexion API", "ERROR", str(e))
        return False

class CloudTempleLLM:
    """
    Wrapper LangChain pour Cloud Temple LLMaaS
    Impl√©mentation compl√®te compatible avec l'interface LangChain LLM
    """
    
    def __init__(self, api_key: str, model_name: str = "granite3.3:8b", **kwargs):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = BASE_URL
        self.temperature = kwargs.get('temperature', 0.7)
        self.max_tokens = kwargs.get('max_tokens', 500)
        self.logger = kwargs.get('logger', RAGTestLogger())
    
    @property
    def _llm_type(self) -> str:
        return "cloud_temple_llmaas"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """Appel direct √† l'API LLMaaS"""
        self.logger.log_step("Appel LLM", "PROGRESS", f"Prompt: {prompt[:50]}...")
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        if stop:
            payload["stop"] = stop
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            response.raise_for_status()
            result = response.json()
            
            content = result['choices'][0]['message']['content']
            usage = result.get('usage', {})
            
            self.logger.log_step("R√©ponse LLM", "SUCCESS", 
                               f"Tokens: {usage.get('total_tokens', 'N/A')}, Longueur: {len(content)} chars")
            
            return content
            
        except Exception as e:
            self.logger.log_step("Erreur LLM", "ERROR", str(e))
            raise

def create_test_documents():
    """Cr√©ation de documents de test pour le RAG"""
    logger = RAGTestLogger()
    logger.log_step("Cr√©ation documents test", "PROGRESS")
    
    # Cr√©er un r√©pertoire temporaire
    temp_dir = tempfile.mkdtemp(prefix="rag_test_")
    
    # Documents de test avec contenu Cloud Temple
    documents = {
        "cloud_temple_overview.txt": """
        Cloud Temple est un fournisseur de cloud souverain fran√ßais avec la qualification SecNumCloud de l'ANSSI.
        Cette qualification garantit le plus haut niveau de s√©curit√© et de conformit√© pour l'h√©bergement de donn√©es sensibles.
        Cloud Temple propose une infrastructure 100% fran√ßaise avec un contr√¥le total des donn√©es.
        """,
        
        "llmaas_service.txt": """
        L'API LLMaaS de Cloud Temple permet d'acc√©der √† 36 mod√®les d'intelligence artificielle.
        Les tarifs sont de 0.9‚Ç¨ pour l'input et 4‚Ç¨ pour l'output par million de tokens.
        Le service inclut des mod√®les comme Granite 3.3, Qwen3, DeepSeek-R1, et Gemma 3.
        Tous les mod√®les sont h√©berg√©s en France avec conformit√© SecNumCloud.
        """,
        
        "security_compliance.txt": """
        La s√©curit√© Cloud Temple est garantie par plusieurs certifications :
        - SecNumCloud : Qualification ANSSI pour donn√©es sensibles
        - HDS : H√©bergement de Donn√©es de Sant√©
        - ISO 27001 : Management de la s√©curit√© de l'information
        - RGPD : Conformit√© r√©glementation europ√©enne
        Le chiffrement end-to-end prot√®ge toutes les communications.
        """,
        
        "technical_architecture.txt": """
        L'architecture technique Cloud Temple repose sur :
        - Datacenters certifi√©s Tier III+ en France
        - Infrastructure redondante multi-sites
        - R√©seau priv√© s√©curis√© avec chiffrement
        - API REST compatible OpenAI pour LLMaaS
        - Monitoring 24/7 avec SLA 99.9%
        """,
        
        "use_cases.txt": """
        Les cas d'usage principaux de Cloud Temple LLMaaS incluent :
        - Dialogue multilingue avec 8+ langues support√©es
        - Analyse de documents longs jusqu'√† 120k tokens
        - G√©n√©ration de code dans 15+ langages
        - Analyse visuelle sans preprocessing OCR
        - Applications s√©curis√©es avec audit complet
        - D√©ploiements embarqu√©s et edge computing
        """
    }
    
    # √âcrire les fichiers
    for filename, content in documents.items():
        file_path = Path(temp_dir) / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content.strip())
        
        logger.log_step(f"Document {filename}", "SUCCESS", f"{len(content)} caract√®res")
    
    logger.log_step("Documents cr√©√©s", "SUCCESS", f"{len(documents)} fichiers dans {temp_dir}")
    return temp_dir

def setup_rag_pipeline(documents_dir: str):
    """Configuration compl√®te du pipeline RAG"""
    logger = RAGTestLogger()
    logger.log_step("Configuration pipeline RAG", "PROGRESS")
    
    try:
        # Imports LangChain
        from langchain_community.document_loaders import DirectoryLoader, TextLoader
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_community.vectorstores import FAISS
        from langchain_community.embeddings import HuggingFaceEmbeddings
        from langchain.chains import RetrievalQA
        
        logger.log_step("Imports LangChain", "SUCCESS")
        
        # 1. Chargement des documents
        logger.log_step("Chargement documents", "PROGRESS")
        loader = DirectoryLoader(
            documents_dir,
            glob="*.txt",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        documents = loader.load()
        logger.log_step("Documents charg√©s", "SUCCESS", f"{len(documents)} fichiers")
        
        # 2. Division en chunks
        logger.log_step("Division en chunks", "PROGRESS")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        splits = text_splitter.split_documents(documents)
        logger.log_step("Chunks cr√©√©s", "SUCCESS", f"{len(splits)} chunks")
        
        # 3. Cr√©ation des embeddings
        logger.log_step("Initialisation embeddings", "PROGRESS")
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        logger.log_step("Embeddings initialis√©s", "SUCCESS", "all-MiniLM-L6-v2")
        
        # 4. Cr√©ation de l'index vectoriel FAISS
        logger.log_step("Cr√©ation index FAISS", "PROGRESS")
        vectorstore = FAISS.from_documents(splits, embeddings)
        logger.log_step("Index FAISS cr√©√©", "SUCCESS", f"{vectorstore.index.ntotal} vecteurs")
        
        # 5. Configuration du LLM Cloud Temple
        logger.log_step("Configuration LLM", "PROGRESS")
        llm = CloudTempleLLM(
            api_key=API_KEY,
            model_name="granite3.3:8b",
            temperature=0.3,  # Plus pr√©cis pour RAG
            max_tokens=300,
            logger=logger
        )
        logger.log_step("LLM configur√©", "SUCCESS", "granite3.3:8b")
        
        # 6. Cr√©ation de la cha√Æne RAG
        logger.log_step("Cr√©ation cha√Æne RAG", "PROGRESS")
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 3}
            ),
            return_source_documents=True,
            verbose=False
        )
        logger.log_step("Cha√Æne RAG cr√©√©e", "SUCCESS", "RetrievalQA avec k=3")
        
        return qa_chain, vectorstore, logger
        
    except Exception as e:
        logger.log_step("Erreur pipeline", "ERROR", str(e))
        raise

def test_rag_queries(qa_chain, vectorstore):
    """Test de requ√™tes RAG avec diff√©rents sc√©narios"""
    logger = RAGTestLogger()
    logger.log_step("Tests requ√™tes RAG", "PROGRESS")
    
    # Questions de test avec r√©ponses attendues
    test_queries = [
        {
            "question": "Qu'est-ce que Cloud Temple et quelle est sa qualification principale ?",
            "expected_keywords": ["cloud temple", "secnumcloud", "anssi", "fran√ßais", "souverain"],
            "category": "Pr√©sentation g√©n√©rale"
        },
        {
            "question": "Combien co√ªte l'utilisation de l'API LLMaaS ?",
            "expected_keywords": ["0.9", "4", "euro", "input", "output", "token"],
            "category": "Tarification"
        },
        {
            "question": "Quelles sont les certifications de s√©curit√© de Cloud Temple ?",
            "expected_keywords": ["secnumcloud", "hds", "iso", "rgpd", "anssi"],
            "category": "S√©curit√©"
        },
        {
            "question": "Quels sont les cas d'usage principaux de LLMaaS ?",
            "expected_keywords": ["dialogue", "multilingue", "code", "analyse", "documents"],
            "category": "Cas d'usage"
        },
        {
            "question": "Comment fonctionne l'architecture technique de Cloud Temple ?",
            "expected_keywords": ["datacenter", "tier", "france", "redondant", "api"],
            "category": "Architecture"
        }
    ]
    
    results = []
    
    for i, test_query in enumerate(test_queries, 1):
        question = test_query["question"]
        expected_keywords = test_query["expected_keywords"]
        category = test_query["category"]
        
        logger.log_step(f"Question {i}/{len(test_queries)}", "PROGRESS", f"{category}: {question}")
        
        try:
            # Ex√©cution de la requ√™te RAG
            start_time = time.time()
            result = qa_chain({"query": question})
            query_time = time.time() - start_time
            
            # Extraction des r√©sultats
            answer = result['result']
            source_docs = result['source_documents']
            
            # Validation des mots-cl√©s
            answer_lower = answer.lower()
            found_keywords = [kw for kw in expected_keywords if kw in answer_lower]
            keyword_score = len(found_keywords) / len(expected_keywords)
            
            # Test de recherche vectorielle
            similar_docs = vectorstore.similarity_search(question, k=3)
            
            # R√©sultats du test
            test_result = {
                "question": question,
                "category": category,
                "answer": answer,
                "answer_length": len(answer),
                "query_time": query_time,
                "source_count": len(source_docs),
                "keyword_score": keyword_score,
                "found_keywords": found_keywords,
                "missing_keywords": [kw for kw in expected_keywords if kw not in found_keywords],
                "similarity_docs": len(similar_docs),
                "success": keyword_score >= 0.4  # Au moins 40% des mots-cl√©s
            }
            
            results.append(test_result)
            
            # Log des r√©sultats
            status = "SUCCESS" if test_result["success"] else "ERROR"
            details = f"Score: {keyword_score:.1%}, Temps: {query_time:.2f}s, Sources: {len(source_docs)}"
            logger.log_step(f"R√©ponse {i}", status, details)
            
            # Affichage d√©taill√©
            print(f"    üí¨ Question: {question}")
            print(f"    ü§ñ R√©ponse: {answer[:100]}...")
            print(f"    üìä Mots-cl√©s trouv√©s: {found_keywords}")
            print(f"    üìö Sources utilis√©es: {len(source_docs)}")
            print(f"    ‚è±Ô∏è  Temps de r√©ponse: {query_time:.2f}s")
            print()
            
        except Exception as e:
            logger.log_step(f"Erreur question {i}", "ERROR", str(e))
            results.append({
                "question": question,
                "category": category,
                "success": False,
                "error": str(e)
            })
    
    # R√©sum√© des tests
    successful_queries = sum(1 for r in results if r.get("success", False))
    total_queries = len(results)
    
    logger.log_step("Tests termin√©s", "SUCCESS" if successful_queries == total_queries else "ERROR",
                   f"{successful_queries}/{total_queries} requ√™tes r√©ussies")
    
    return results, logger

def analyze_rag_performance(results: List[Dict], vectorstore):
    """Analyse des performances du pipeline RAG"""
    logger = RAGTestLogger()
    logger.log_step("Analyse performances", "PROGRESS")
    
    # M√©triques globales
    successful_results = [r for r in results if r.get("success", False)]
    
    if not successful_results:
        logger.log_step("Aucun r√©sultat", "ERROR", "Impossible d'analyser les performances")
        return False
    
    # Calculs de performance
    avg_query_time = sum(r.get("query_time", 0) for r in successful_results) / len(successful_results)
    avg_answer_length = sum(r.get("answer_length", 0) for r in successful_results) / len(successful_results)
    avg_keyword_score = sum(r.get("keyword_score", 0) for r in successful_results) / len(successful_results)
    avg_source_count = sum(r.get("source_count", 0) for r in successful_results) / len(successful_results)
    
    # Analyse par cat√©gorie
    categories = {}
    for result in successful_results:
        category = result.get("category", "Unknown")
        if category not in categories:
            categories[category] = []
        categories[category].append(result)
    
    # M√©triques vectorstore
    total_vectors = vectorstore.index.ntotal if hasattr(vectorstore, 'index') else 0
    
    # Affichage des m√©triques
    print(f"\nüìà ANALYSE PERFORMANCES RAG")
    print(f"{'='*50}")
    print(f"‚è±Ô∏è  Temps moyen par requ√™te: {avg_query_time:.2f}s")
    print(f"üìù Longueur moyenne r√©ponse: {avg_answer_length:.0f} caract√®res")
    print(f"üéØ Score moyen mots-cl√©s: {avg_keyword_score:.1%}")
    print(f"üìö Sources moyennes utilis√©es: {avg_source_count:.1f}")
    print(f"üîç Vecteurs dans l'index: {total_vectors}")
    print(f"‚úÖ Taux de succ√®s: {len(successful_results)}/{len(results)} ({len(successful_results)/len(results):.1%})")
    
    print(f"\nüìä PERFORMANCE PAR CAT√âGORIE")
    print(f"{'='*50}")
    for category, cat_results in categories.items():
        cat_score = sum(r.get("keyword_score", 0) for r in cat_results) / len(cat_results)
        cat_time = sum(r.get("query_time", 0) for r in cat_results) / len(cat_results)
        print(f"üè∑Ô∏è  {category}: Score {cat_score:.1%}, Temps {cat_time:.2f}s")
    
    # Crit√®res de validation
    performance_criteria = {
        "Temps de r√©ponse": avg_query_time < 5.0,  # < 5 secondes
        "Score mots-cl√©s": avg_keyword_score >= 0.6,  # >= 60%
        "Utilisation sources": avg_source_count >= 1.0,  # Au moins 1 source
        "Taux de succ√®s": len(successful_results) / len(results) >= 0.8  # >= 80%
    }
    
    print(f"\n‚úÖ CRIT√àRES DE VALIDATION")
    print(f"{'='*50}")
    all_criteria_met = True
    for criterion, met in performance_criteria.items():
        status = "‚úÖ PASS" if met else "‚ùå FAIL"
        print(f"{status} {criterion}")
        if not met:
            all_criteria_met = False
    
    logger.log_step("Analyse termin√©e", "SUCCESS" if all_criteria_met else "ERROR",
                   f"Crit√®res: {sum(performance_criteria.values())}/{len(performance_criteria)}")
    
    return all_criteria_met

def main():
    """Fonction principale du test RAG"""
    print("üöÄ TEST RAG PIPELINE D√âTAILL√â - CLOUD TEMPLE LLMAAS")
    print("=" * 70)
    print(f"üìÖ Date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üîë API Key: {'‚úÖ Configur√©e' if API_KEY != 'your-api-key-here' else '‚ùå Non configur√©e'}")
    print(f"üåê Base URL: {BASE_URL}")
    print("=" * 70)
    
    # Variables pour cleanup
    temp_dir = None
    
    try:
        # 1. V√©rification des d√©pendances
        if not check_dependencies():
            print("‚ùå √âchec v√©rification d√©pendances")
            return False
        
        # 2. Test de connexion API
        if not test_api_connection():
            print("‚ùå √âchec connexion API")
            return False
        
        # 3. Cr√©ation des documents de test
        temp_dir = create_test_documents()
        
        # 4. Configuration du pipeline RAG
        qa_chain, vectorstore, setup_logger = setup_rag_pipeline(temp_dir)
        
        # 5. Tests de requ√™tes
        results, query_logger = test_rag_queries(qa_chain, vectorstore)
        
        # 6. Analyse des performances
        performance_ok = analyze_rag_performance(results, vectorstore)
        
        # 7. R√©sum√© final
        print(f"\nüèÅ R√âSUM√â FINAL")
        print(f"{'='*70}")
        
        if performance_ok:
            print("üéâ SUCC√àS - Pipeline RAG enti√®rement valid√©!")
            print("‚úÖ Tous les crit√®res de performance sont respect√©s")
            print("‚úÖ Documentation tutorials.md RAG section valid√©e")
            return True
        else:
            print("‚ö†Ô∏è  PARTIEL - Pipeline RAG fonctionne mais performances √† am√©liorer")
            print("üîß Certains crit√®res de performance ne sont pas respect√©s")
            return False
        
    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # Cleanup
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                print(f"\nüßπ Cleanup: R√©pertoire temporaire supprim√©")
            except Exception as e:
                print(f"\n‚ö†Ô∏è  Cleanup warning: {e}")

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
