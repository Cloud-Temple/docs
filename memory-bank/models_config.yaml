# Configuration des mod√®les Cloud Temple MaaS
# Ce fichier d√©crit tous les mod√®les disponibles dans l'offre LLM as a Service

# Statistiques globales
stats:
  total_models: 36 # Ajout qwen3:32b
  min_context: 8192 # Granite Guardian 2B
  max_context: 120000 # Qwen3 / Gemma 3 / Granite 3.3 2B (Forc√©/V√©rifi√©)
  pricing_input: 0.9
  pricing_output: 4
  pricing_reasoning: 21 # Nouveau prix pour le raisonnement
  secnumcloud: true
  hds: true
  sovereign: true
  c5: true

# Mod√®les de grande taille
large_models:
  - name: Llama 3.3 70B
    editor: Meta # Correspondance Table: Meta
    location: FR üá´üá∑ # Mis √† jour
    parameters: 70
    parameters_string: 70B # Ajout√© depuis table
    context_size: 60000 # Correspondance Table: 60k
    speed: 30 tokens/seconde
    energy_per_million_tokens: 8.87 # (958W / 30 t/s) / 3.6 -> kWh/Mtoken (Serveur L40S + Cooling)
    licence: LLAMA 3.3 Community Licence
    description: Mod√®le multilingue de pointe d√©velopp√© par Meta, con√ßu pour exceller dans le dialogue naturel, le raisonnement complexe et la compr√©hension nuanc√©e des instructions.
    details: Combinant une efficacit√© remarquable avec des ressources computationnelles r√©duites, ce mod√®le offre des capacit√©s multilingues √©tendues couvrant 8 langues majeures (anglais, fran√ßais, allemand, espagnol, italien, portugais, hindi et tha√Ø). Sa fen√™tre contextuelle de 60 000 tokens permet l'analyse approfondie de documents complexes et de conversations longues, tout en maintenant une coh√©rence globale exceptionnelle. Optimis√© pour minimiser les biais et les r√©ponses probl√©matiques.
    supports_tools: false # Les mod√®les GEMMA3 ne supportent pas les outils
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: false # Vitesse 28 <= 50
    tags:
      - Agent
      - Dialogue
      - Multilingue
      # Efficient tag removed (conso 9.50 >= 2.0)
    use_cases:
      - Chatbots multilingues prenant en charge 8 langues simultan√©ment
      - Ex√©cution d'instructions complexes encha√Æn√©es (prompt chaining)
      - Traitement d'une fen√™tre de dialogue de 60K tokens pour historique conversationnel
      - Analyse de documents juridiques ou techniques volumineux (>100 pages)
      - G√©n√©ration de textes structur√©s avec fid√©lit√© aux consignes stylistiques

  - name: Qwen3 235B
    editor: Qwen Team
    location: FR üá´üá∑
    parameters: 235
    parameters_string: 235B
    context_size: 32000 # D'apr√®s model_list.md
    speed: 21 # tokens/seconde, arrondi inf√©rieur de 21.19 (tests du 29/05/2025)
    energy_per_million_tokens: 6.35 # (480W Mac Studio / 21 t/s) / 3600
    licence: Apache 2.0 # Suppos√©e, comme les autres Qwen3
    description: Mod√®le tr√®s volumineux de la nouvelle g√©n√©ration Qwen3, offrant des capacit√©s √©tendues pour les t√¢ches les plus complexes.
    details: Fait partie de la s√©rie Qwen3. Ce mod√®le de 235 milliards de param√®tres est con√ßu pour exceller dans les t√¢ches de raisonnement profond, de g√©n√©ration de code complexe, et de compr√©hension nuanc√©e sur de vastes contextes. Supporte plus de 100 langues et int√®gre des modes de pens√©e hybrides.
    supports_tools: true # Capacit√©s Agentiques / MCP
    supports_vision: false # Non mentionn√© pour Qwen3 dense
    supports_reasoning: true # Mode Thinking / Raisonnement am√©lior√©
    supports_security: false # Non mentionn√©
    is_fast: false # Vitesse 21 <= 50
    tags:
      - Agent
      - Raisonnement
      - Multilingue
      - Tr√®s Large
    use_cases:
      - Agents conversationnels tr√®s avanc√©s avec grand contexte et int√©gration d'outils (MCP)
      - R√©solution de probl√®mes extr√™mement complexes (maths, code)
      - Analyse et g√©n√©ration de documents tr√®s volumineux et techniques
      - Applications multilingues (>100 langues) n√©cessitant une compr√©hension et une g√©n√©ration de tr√®s haute fid√©lit√©

  - name: DeepSeek-R1 671B
    editor: DeepSeek AI
    location: FR üá´üá∑
    parameters: 671
    parameters_string: 671B
    context_size: 32000 # D'apr√®s model_list.md
    speed: 16 tokens/seconde # Donn√©es du 04/06/2025
    energy_per_million_tokens: 8.33 # (480W Mac Studio / 16 t/s) / 3.6
    licence: MIT Licence # Suppos√©e, comme les autres DeepSeek
    description: Mod√®le extr√™mement volumineux de DeepSeek AI, con√ßu pour le summum du raisonnement et de la g√©n√©ration.
    details: DeepSeek-R1 671B repr√©sente l'un des plus grands mod√®les ouverts, destin√© aux t√¢ches de raisonnement les plus ardues et √† la g√©n√©ration de texte d'une qualit√© exceptionnelle.
    supports_tools: false # Par d√©faut, √† confirmer
    supports_vision: false # Par d√©faut, √† confirmer
    supports_reasoning: true # Suppos√©, √©tant un mod√®le de cette taille
    supports_security: false # Par d√©faut, √† confirmer
    is_fast: false # Vitesse 16 <= 50
    tags:
      - Raisonnement
      - Extr√™mement Large
      # Efficient tag removed (conso 8.33 >= 2.0)
    use_cases:
      - T√¢ches de raisonnement de pointe
      - G√©n√©ration de texte de qualit√© sup√©rieure
      - Recherche et d√©veloppement en IA

  - name: Gemma 3 27B
    editor: Google # Correspondance Table: Google
    location: FR üá´üá∑ # Mis √† jour
    parameters: 27
    parameters_string: 27B # Ajout√© depuis table
    context_size: 120000 # Correspondance Table: 120k
    speed: 68 tokens/seconde
    energy_per_million_tokens: 3.91 # (958W / 68 t/s) / 3.6 -> kWh/Mtoken (Serveur L40S + Cooling)
    licence: Google Gemma Terms of Use
    description: Mod√®le r√©volutionnaire de Google offrant un √©quilibre optimal entre puissance et efficacit√©, avec un rapport performance/co√ªt exceptionnel pour les applications professionnelles exigeantes.
    details: Dot√© d'une efficacit√© mat√©rielle in√©gal√©e, ce mod√®le int√®gre des capacit√©s multimodales natives et excelle dans la performance multilingue sur plus de 140 langues. Sa fen√™tre contextuelle impressionnante de 120 000 tokens en fait le choix id√©al pour l'analyse de documents tr√®s volumineux, la recherche documentaire, et toute application n√©cessitant la compr√©hension de contextes √©tendus. Son architecture optimis√©e permet un d√©ploiement flexible sans compromettre la qualit√© des r√©sultats.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: true # Correspondance Table: ‚úÖ Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 67 > 50
    tags:
      - Vision # Fusionn√© depuis badges
      - Agent # Fusionn√© depuis badges
      - Rapide # Fusionn√© depuis badges (OK)
      - Grand contexte
      # Efficient tag removed (conso 3.97 >= 2.0)
    use_cases:
      - Analyse de documents avec contexte √©tendu jusqu'√† 120K tokens (environ 400 pages)
      - Indexation et recherche s√©mantique dans des bases documentaires volumineuses
      - Traitement d'images et texte en simultan√© gr√¢ce aux capacit√©s multimodales
      - Extraction structur√©e de donn√©es √† partir de PDF et documents scann√©s
      - Int√©gration avec des outils externes via l'API function calling

  - name: Qwen3 30B-A3B FP8
    editor: Qwen Team
    location: FR üá´üá∑
    parameters: 30
    parameters_string: 30B-A3B
    context_size: 32000
    speed: 103 tokens/seconde
    energy_per_million_tokens: 2.58 # (958W / 103 t/s) / 3.6 -> kWh/Mtoken (Serveur L40S + Cooling)
    licence: Apache 2.0 # Suppos√©e
    description: Mod√®le MoE FP8 (3B activ√©s) nouvelle g√©n√©ration, avec modes de pens√©e hybrides et capacit√©s agentiques avanc√©es.
    details: Version FP8 du mod√®le MoE Qwen3 30B-A3B. Int√®gre un mode "Thinking" pour le raisonnement complexe et un mode "Non-Thinking" rapide. Capacit√©s am√©lior√©es en raisonnement, code, maths et agent (outils/MCP). Supporte plus de 100 langues. Id√©al pour un √©quilibre performance/co√ªt optimal.
    supports_tools: true
    supports_vision: false
    supports_reasoning: true
    supports_security: false
    is_fast: true # Vitesse 81.12 > 50
    tags:
      - MoE
      - Agent
      - Raisonnement
      - Rapide
      - Multilingue
    use_cases:
      - Agents conversationnels avanc√©s avec int√©gration d'outils (MCP)
      - R√©solution de probl√®mes complexes (maths, code) avec mode "Thinking"
      - Applications multilingues (>100 langues)
      - Sc√©narios n√©cessitant un √©quilibre co√ªt/performance (MoE) sur VLLM
      - Dialogue multi-tours engageant et suivi d'instructions pr√©cis

  - name: DeepSeek-R1 70B
    editor: DeepSeek AI
    location: FR üá´üá∑
    parameters: 70
    parameters_string: 70B
    context_size: 32000 # D'apr√®s model_list.md
    speed: 20 tokens/seconde # Nouvelle valeur fournie
    energy_per_million_tokens: 11.44 # (858W [Lame ~258W + 2x A100 300W] / 20.83 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 ia06)
    licence: MIT Licence # Suppos√©e, comme les autres DeepSeek
    description: Mod√®le 70B de DeepSeek AI
    details: DeepSeek-R1 70B est con√ßu pour des t√¢ches complexes de raisonnement et de g√©n√©ration.
    supports_tools: false # Par d√©faut, √† confirmer
    supports_vision: false # Par d√©faut, √† confirmer
    supports_reasoning: true # Suppos√©, √©tant un mod√®le 70B
    supports_security: false # Par d√©faut, √† confirmer
    is_fast: false # Vitesse 20.83 <= 50
    tags:
      - Raisonnement
      - Large
      # Efficient tag removed (conso 11.44 >= 2.0)
    use_cases:
      - T√¢ches de raisonnement complexes
      - G√©n√©ration de texte de haute qualit√©
      - Analyse approfondie de documents (dans la limite du contexte de 27k)

# Mod√®les sp√©cialis√©s
specialized_models:
  - name: Qwen3 14B # Nouveau mod√®le
    editor: Qwen Team
    location: FR üá´üá∑ # Mis √† jour
    parameters: 14
    parameters_string: 14B # Ajout√© depuis table
    context_size: 32000 # Corrig√©
    speed: 69 tokens/seconde
    energy_per_million_tokens: 2.65 # (658W / 69 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: Apache 2.0
    description: Mod√®le dense nouvelle g√©n√©ration Qwen3 (14B), offrant des performances √©quivalentes √† Qwen2.5 32B avec une meilleure efficacit√©.
    details: Fait partie de la s√©rie Qwen3, entra√Æn√© sur ~36T tokens. Capacit√©s am√©lior√©es en raisonnement, code, maths et agent (outils/MCP). Supporte plus de 100 langues et les modes de pens√©e hybrides.
    supports_tools: true # Capacit√©s Agentiques / MCP
    supports_vision: false # Non mentionn√©
    supports_reasoning: true # Mode Thinking / Raisonnement am√©lior√©
    supports_security: false # Non mentionn√©
    is_fast: true # Vitesse 74 > 50
    tags:
      - Agent
      - Raisonnement
      - Rapide # (OK)
      - Multilingue
      # Efficient tag removed (conso 2.47 >= 2.0)
    use_cases:
      - T√¢ches g√©n√©rales n√©cessitant performance et grand contexte
      - G√©n√©ration de contenu cr√©atif et technique
      - Analyse de donn√©es et raisonnement complexe
      - Int√©gration avec des outils externes via function calling

  - name: Gemma 3 12B
    editor: Google # Correspondance Table: Google
    location: FR üá´üá∑ # Mis √† jour
    parameters: 12
    parameters_string: 12B # Ajout√© depuis table
    context_size: 120000 # Correspondance Table: 120k
    speed: 67 tokens/seconde
    energy_per_million_tokens: 2.73 # (658W / 67 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: Google Gemma Terms of Use
    description: Version interm√©diaire du mod√®le Gemma 3 offrant un excellent √©quilibre entre performance et efficacit√©.
    details: Ce mod√®le de taille moyenne combine performances de haute qualit√© et efficacit√© op√©rationnelle, offrant une grande partie des capacit√©s de son grand fr√®re de 27B param√®tres dans un format plus l√©ger. Id√©al pour les d√©ploiements n√©cessitant qualit√© et rapidit√© sans les ressources computationnelles des plus grands mod√®les.
    supports_tools: false # Les mod√®les GEMMA3 ne supportent pas les outils
    supports_vision: true # Correspondance Table: ‚úÖ Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 76 > 50
    tags:
      - Vision # Fusionn√© depuis badges
      # Agent tag removed (ne supporte pas les outils)
      - Rapide # Fusionn√© depuis badges (OK)
      - Grand Contexte
      # Efficient tag removed (conso 2.41 >= 2.0)
    use_cases:
      - Applications multimodales avec contraintes de ressources mod√©r√©es
      - Traitement de documents avec contexte standard (jusqu'√† 100 pages)
      - G√©n√©ration de contenu textuel et analyse d'images combin√©es
      - D√©ploiements sur GPU standard sans infrastructure sp√©cialis√©e
      - Chatbots avanc√©s avec capacit√©s visuelles et textuelles int√©gr√©es

  - name: Gemma 3 4B
    editor: Google # Correspondance Table: Google
    location: FR üá´üá∑ # Mis √† jour
    parameters: 4
    parameters_string: 4B # Ajout√© depuis table
    context_size: 120000 # Correspondance Table: 120k
    speed: 58 tokens/seconde
    energy_per_million_tokens: 0.93 # (195W / 58 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Google Gemma Terms of Use
    description: Mod√®le compact de Google offrant d'excellentes performances dans un format l√©ger et √©conomique.
    details: Cette version compacte du mod√®le Gemma 3 est optimis√©e pour les d√©ploiements avec contraintes de ressources tout en maintenant des performances remarquables pour sa taille. Son architecture efficiente permet une inf√©rence rapide sur du mat√©riel standard, id√©ale pour les applications n√©cessitant r√©activit√© et d√©ploiement √† grande √©chelle. Malgr√© sa taille r√©duite, il maintient des capacit√©s multimodales pour traiter √† la fois texte et images.
    supports_tools: false # Les mod√®les GEMMA3 ne supportent pas les outils
    supports_vision: true # Correspondance Table: ‚úÖ Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 58 > 50
    tags:
      - Vision # Fusionn√© depuis badges
      # Agent tag removed (ne supporte pas les outils)
      - Rapide # Fusionn√© depuis badges (et d√©j√† pr√©sent) (OK)
      - Compact
      - Grand Contexte
      - Efficient # (OK)
    use_cases:
      - Applications embarqu√©es et edge computing avec traitement d'images
      - Chatbots multimodaux r√©actifs n√©cessitant faible latence (<50ms)
      - D√©ploiements √† grande √©chelle avec capacit√©s visuelles et textuelles
      - Applications mobiles avec analyse d'images et textes
      - Traitement de requ√™tes visuelles simples √† moyenne complexit√© avec haute performance

  - name: Gemma 3 1B
    editor: Google # Correspondance Table: Google
    location: FR üá´üá∑ # Mis √† jour
    parameters: 1
    parameters_string: 1B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 41 tokens/seconde
    energy_per_million_tokens: 1.32 # (195W / 41 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Google Gemma Terms of Use
    description: Micro-mod√®le ultra-l√©ger con√ßu pour les d√©ploiements sur appareils √† tr√®s faibles ressources.
    details: Ce mod√®le ultra-compact repr√©sente la quintessence de l'efficience, permettant des d√©ploiements dans des environnements extr√™mement contraints en ressources. Malgr√© sa taille minimale, il offre des capacit√©s de base surprenantes pour des t√¢ches textuelles simples √† mod√©r√©es, avec une vitesse d'inf√©rence exceptionnelle. Il prend √©galement en charge l'int√©gration avec des outils externes via function calling.
    supports_tools: false # Les mod√®les GEMMA3 ne supportent pas les outils
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: false # Vitesse 43 <= 50
    tags:
      # Agent tag removed (ne supporte pas les outils)
      # Rapide tag removed (vitesse 43 <= 50)
      - Ultra-compact
      - Embarqu√©
      - Efficient # (OK)
    use_cases:
      - D√©ploiement sur appareils IoT et syst√®mes embarqu√©s avec int√©gration API
      - Applications n√©cessitant inf√©rence locale sur CPU avec appels √† des fonctions
      - T√¢ches textuelles basiques avec temps de r√©ponse instantan√© et function calling
      - Assistants compacts pour applications grand public avec int√©gration services externes
      - Syst√®mes de contr√¥le intelligents int√©grant plusieurs APIs/services

  - name: Lucie-7B-Instruct # Nouveau mod√®le ajout√©
    editor: OpenLLM-France # Correspondance Table: OpenLLM-France
    location: FR üá´üá∑ # Mis √† jour
    parameters: 7
    parameters_string: 7B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 41 tokens/seconde
    energy_per_million_tokens: 1.32 # (195W / 41 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Apache 2.0
    description: Mod√®le causal multilingue open-source (7B), fine-tun√© depuis Lucie-7B. Optimis√© pour le fran√ßais.
    details: Fine-tun√© sur instructions synth√©tiques (ChatGPT, Gemma) et prompts customis√©s. Non optimis√© pour code/maths. Entra√Æn√© sur contexte 4k mais conserve la capacit√© du mod√®le de base pour 32k. Mod√®le en d√©veloppement.
    supports_tools: false # Correspondance Table: ‚ùå Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: false # Vitesse 41 <= 50
    tags:
      - Fran√ßais
      - Open-Source
      # Rapide tag removed (vitesse 41 <= 50)
      - Efficient # Added (conso 1.32 < 2.0)

  - name: Mistral Small 3.1
    editor: Mistral AI # Correspondance Table: Mistral AI
    location: FR üá´üá∑ # Mis √† jour
    parameters: 24
    parameters_string: 24B # Ajout√© depuis table
    context_size: 60000 # Correspondance Table: 60k
    speed: 14 tokens/seconde
    energy_per_million_tokens: 13.06 # (658W / 14 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: Apache 2.0
    description: Mod√®le compact et r√©actif de Mistral AI, sp√©cialement con√ßu pour offrir une assistance conversationnelle fluide et pertinente avec une vitesse de r√©ponse optimale.
    details: Malgr√© sa taille mod√©r√©e, ce mod√®le affiche une performance remarquable qui rivalise avec celle de nombreux mod√®les propri√©taires bien plus volumineux. Son architecture ing√©nieusement optimis√©e facilite le d√©ploiement local sur des infrastructures vari√©es. Int√©grant des capacit√©s multimodales natives, il peut traiter √† la fois du texte et des images sans recourir √† des syst√®mes externes. Sa licence Apache 2.0 offre une flexibilit√© maximale pour les d√©ploiements commerciaux et les personnalisations, en faisant un choix id√©al pour les entreprises soucieuses d'√©quilibrer performance et contraintes l√©gales.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: true # Correspondance Table: ‚úÖ Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: true # Ajout√© depuis table (‚úÖ)
    is_fast: false # Vitesse 22 <= 50
    tags:
      - Vision # Fusionn√© depuis badges
      - Agent # Fusionn√© depuis badges
      - S√©curit√© # Fusionn√© depuis badges
      # Rapide tag removed (vitesse 22 <= 50)
      # Efficient tag removed (conso 8.31 >= 2.0)
    use_cases:
      - Applications conversationnelles
      - Assistants virtuels combinant analyse d'images et texte (26 tokens/s)
      - Chatbots de support technique avec acc√®s √† la documentation technique
      - Outils de cr√©ation/√©dition de contenu avec r√©ponse imm√©diate (blogs, emails)
      - D√©ploiement sur infrastructures standard (24B de param√®tres)

  - name: DeepCoder
    editor: Agentica x Together AI # Correspondance Table: Agentica x Together AI
    location: FR üá´üá∑ # Mis √† jour
    parameters: 14
    parameters_string: 14B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 62 tokens/seconde
    energy_per_million_tokens: 2.95 # (658W / 62 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: Apache 2.0
    description: Mod√®le IA open source (14B) par Together AI & Agentica, alternative cr√©dible aux mod√®les propri√©taires pour la g√©n√©ration de code. # Description reformul√©e
    details: Performances remarquables en g√©n√©ration de code et raisonnement algorithmique (60.6% LiveCodeBench Pass@1, 1936 Codeforces, 92.6% HumanEval+). Entra√Æn√© via RL (GRPO+) avec allongement progressif du contexte (32k -> 64k). Projet transparent (code, dataset, logs ouverts). Permet l'int√©gration de capacit√©s avanc√©es de g√©n√©ration de code sans d√©pendre de solutions propri√©taires. # D√©tails reformul√©s
    supports_tools: false # Correspondance Table: ‚ùå Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: true # Ajout√© depuis table (‚úÖ)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 69 > 50
    tags:
      - Programmation # Tag principal
      - Raisonnement # Capacit√© mentionn√©e
      - Open-Source # Caract√©ristique cl√©
      - Math√©matiques # Mentionn√© comme domaine li√©
      - Rapide # (OK)
      # Efficient tag removed (conso 2.65 >= 2.0)
    use_cases:
      - G√©n√©ration de code dans plus de 15 langages avec optimisation des performances
      - D√©bogage et refactoring de bases de code existantes avec analyse d'impact
      - Impl√©mentation d'algorithmes complexes (graphes, arbres, heuristiques)
      - Transposition de code entre langages/frameworks (ex: Python vers JavaScript)
      - Cr√©ation automatis√©e de tests unitaires avec couverture de code > 80%

  - name: Granite 3.2 Vision
    editor: IBM # Correspondance Table: IBM
    location: FR üá´üá∑ # Mis √† jour
    parameters: 2
    parameters_string: 2B # Ajout√© depuis table
    context_size: 16384 # Correspondance Table: 16k
    speed: 48 tokens/seconde
    energy_per_million_tokens: 1.13 # (195W / 48 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Apache 2.0 # Correction: licence avec 'c'
    description: Mod√®le compact r√©volutionnaire d'IBM sp√©cialis√© dans la vision par ordinateur, capable d'analyser et comprendre directement les documents visuels sans recourir √† des technologies OCR interm√©diaires.
    details: Ce mod√®le compact r√©alise l'exploit remarquable d'√©galer les performances de mod√®les bien plus volumineux sur un large √©ventail de t√¢ches de compr√©hension visuelle. Sa capacit√© √† interpr√©ter directement le contenu visuel des documents - textes, tableaux, graphiques et diagrammes - sans passer par une √©tape d'OCR traditionnelle repr√©sente une avanc√©e significative en termes d'efficacit√© et de pr√©cision. Cette approche int√©gr√©e r√©duit consid√©rablement les erreurs de reconnaissance et permet une compr√©hension plus contextuelle et plus nuanc√©e du contenu visuel.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: true # Correspondance Table: ‚úÖ Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: true # Ajout√© depuis table (‚úÖ)
    is_fast: true # Vitesse 56 > 50
    tags:
      - Vision # Fusionn√© depuis badges (et d√©j√† pr√©sent)
      - S√©curit√© # Fusionn√© depuis badges
      - Rapide # Fusionn√© depuis badges (OK)
      - Compact
      - Efficient # (OK)
    use_cases:
      - Extraction de donn√©es structur√©es √† partir de factures et formulaires sans OCR
      - Analyse directe de tableaux et graphiques avec interpr√©tation des tendances
      - Lecture et interpr√©tation de diagrammes techniques (√©lectriques, m√©caniques)
      - Traitement de documents manuscrits avec taux de reconnaissance √©lev√©
      - Vision par ordinateur l√©g√®re (2B param√®tres) avec vitesse √©lev√©e (79 tokens/s)

  - name: Granite 3.3 8B
    editor: IBM # Correspondance Table: IBM
    location: FR üá´üá∑ # Mis √† jour
    parameters: 8
    parameters_string: 8B # Ajout√© depuis table
    context_size: 60000 # Correspondance Table: 60k
    speed: 27 tokens/seconde
    energy_per_million_tokens: 2.00 # (195W / 27 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Apache 2.0
    description: Mod√®le Granite 8B fine-tun√© par IBM pour un raisonnement et un suivi d'instructions am√©lior√©s, avec un contexte de 128k tokens.
    details: Cette version 8B du mod√®le Granite 3.3 offre des gains significatifs sur les benchmarks g√©n√©riques (AlpacaEval-2.0, Arena-Hard) et des am√©liorations en math√©matiques, codage et suivi d'instructions. Il supporte 12 langues, le Fill-in-the-Middle (FIM) pour le code, le mode "Thinking" pour la r√©flexion structur√©e, et l'appel de fonctions. Licence Apache 2.0. Id√©al pour les t√¢ches g√©n√©rales et l'int√©gration dans des assistants IA.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: true # Ajout√© depuis table (‚úÖ)
    supports_security: true # Ajout√© depuis table (‚úÖ)
    is_fast: false # Vitesse 28 <= 50
    tags:
      - Agent # Fusionn√© depuis badges
      - Raisonnement # Fusionn√© depuis badges (et d√©j√† pr√©sent)
      - S√©curit√© # Fusionn√© depuis badges
      # Rapide tag removed (vitesse 28 <= 50)
      - Efficient # Added (conso 1.93 < 2.0)
    use_cases:
      - T√¢ches g√©n√©rales d'instruction-following (classification, extraction, Q&A)
      - Assistants IA multilingues (12 langues)
      - Traitement de documents tr√®s longs (128k tokens) : r√©sum√©s, Q&A
      - G√©n√©ration/compl√©tion de code avec Fill-in-the-Middle
      - Int√©gration avec des outils externes via function calling
      - Raisonnement structur√© avec le mode "Thinking"

  - name: Granite 3.3 2B
    editor: IBM # Correspondance Table: IBM
    location: FR üá´üá∑ # Mis √† jour
    parameters: 2
    parameters_string: 2B # Ajout√© depuis table
    context_size: 120000 # Correspondance Table: 120k
    speed: 45 tokens/seconde
    energy_per_million_tokens: 1.20 # (195W / 45 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Apache 2.0
    description: Mod√®le Granite 2B fine-tun√© par IBM, optimis√© pour le raisonnement et le suivi d'instructions, avec un contexte de 128k tokens.
    details: Version compacte de Granite 3.3 (2B param√®tres) offrant les m√™mes am√©liorations en raisonnement, instruction-following, math√©matiques et codage que la version 8B. Supporte 12 langues, le Fill-in-the-Middle (FIM), le mode "Thinking", et l'appel de fonctions. Licence Apache 2.0. Excellent choix pour des d√©ploiements l√©gers n√©cessitant de longues capacit√©s contextuelles et de raisonnement.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: true # Ajout√© depuis table (‚úÖ)
    supports_security: true # Ajout√© depuis table (‚úÖ)
    is_fast: true # Vitesse 57 > 50
    tags:
      - Agent # Fusionn√© depuis badges
      - Raisonnement
      - S√©curit√© # Fusionn√© depuis badges
      - Rapide # Fusionn√© depuis badges (OK)
      - Efficient # (OK)
    use_cases:
      - D√©ploiements l√©gers avec grand contexte (128k tokens)
      - T√¢ches g√©n√©rales d'instruction-following sur ressources limit√©es
      - Assistants IA multilingues compacts
      - Traitement de documents longs sur appareils moins puissants
      - G√©n√©ration/compl√©tion de code FIM sur postes de travail standards

  - name: Granite 3.1 MoE
    editor: IBM # Correspondance Table: IBM
    location: FR üá´üá∑ # Mis √† jour
    parameters: 3
    parameters_string: 3B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 74 tokens/seconde
    energy_per_million_tokens: 0.73 # (195W / 74 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Apache 2.0
    description: Mod√®le innovant d'IBM utilisant l'architecture Mixture-of-Experts (MoE) pour offrir des performances exceptionnelles tout en optimisant drastiquement l'utilisation des ressources computationnelles.
    details: L'architecture MoE (Mixture-of-Experts) de ce mod√®le constitue une avanc√©e significative dans l'optimisation des mod√®les de langage, permettant d'atteindre des performances comparables √† celles de mod√®les bien plus volumineux tout en maintenant une empreinte m√©moire consid√©rablement r√©duite. Cette approche innovante active dynamiquement uniquement les parties pertinentes du r√©seau pour chaque t√¢che sp√©cifique, assurant ainsi une efficacit√© √©nerg√©tique et computationnelle remarquable sans compromis sur la qualit√© des r√©sultats.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: true # Ajout√© depuis table (‚úÖ)
    is_fast: true # Vitesse 71 > 50
    tags:
      - Agent # Fusionn√© depuis badges
      - S√©curit√© # Fusionn√© depuis badges
      - Rapide # Fusionn√© depuis badges (OK)
      - MoE
      - Efficacit√©
      - Efficient # (OK)
    use_cases:
      - Applications g√©n√©ralistes avec co√ªt d'inf√©rence optimis√© (42 tokens/seconde)
      - Traitement de documents dans des environnements CPU avec utilisation RAM limit√©e
      - Analyses sp√©cialis√©es avec activation dynamique des parties pertinentes du mod√®le
      - D√©ploiements haute densit√© avec faible consommation √©nerg√©tique par inf√©rence
      - Traitement parall√®le de plusieurs types de requ√™tes avec sp√©cialisation MoE

  - name: Cogito 14B
    editor: Deep Cogito # Correspondance Table: Deep Cogito
    location: FR üá´üá∑ # Mis √† jour
    parameters: 14
    parameters_string: 14B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 60 tokens/seconde
    energy_per_million_tokens: 3.05 # (658W / 60 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: LLAMA 3.2 Community Licence
    description: Mod√®le de Deep Cogito sp√©cialement con√ßu pour exceller dans les t√¢ches de raisonnement profond et de compr√©hension contextuelle nuanc√©e, id√©al pour les applications analytiques sophistiqu√©es.
    details: Dot√© d'excellentes capacit√©s de raisonnement logique et de compr√©hension s√©mantique approfondie, ce mod√®le se distingue par sa capacit√© √† saisir les subtilit√©s et les implications dans des textes complexes. Sa conception privil√©gie la coh√©rence du raisonnement et la pr√©cision analytique, le rendant particuli√®rement adapt√© aux applications n√©cessitant une analyse minutieuse et contextuelle des informations. Sa taille mod√©r√©e permet un d√©ploiement flexible tout en maintenant des performances de haute qualit√© sur un large √©ventail de t√¢ches analytiques exigeantes.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: true # Ajout√© depuis table (‚úÖ)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 67 > 50
    tags:
      - Agent # Fusionn√© depuis badges
      - Raisonnement # Fusionn√© depuis badges (et d√©j√† pr√©sent)
      - Compr√©hension
      - Analyse
      - Rapide # (OK)
      # Efficient tag removed (conso 2.73 >= 2.0)
    use_cases:
      - Analyse s√©mantique de textes avec identification des implications non explicites
      - Raisonnement causal structur√© avec identification des relations cause-effet
      - Synth√®se de documents complexes avec extraction des informations cl√©s
      - Syst√®mes de question-r√©ponse pr√©cis sur corpus documentaires sp√©cialis√©s
      - Analyse argumentative avec √©valuation de la solidit√© des raisonnements

  - name: Cogito 32B
    editor: Deep Cogito # Correspondance Table: Deep Cogito
    location: FR üá´üá∑ # Mis √† jour
    parameters: 32
    parameters_string: 32B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 32 tokens/seconde
    energy_per_million_tokens: 5.73 # (658W / 32 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: LLAMA 3.2 Community Licence
    description: Version avanc√©e du mod√®le Cogito offrant des capacit√©s de raisonnement et d'analyse consid√©rablement amplifi√©es, con√ßue pour les applications les plus exigeantes en mati√®re d'intelligence artificielle analytique.
    details: Cette version √©tendue du mod√®le Cogito pousse encore plus loin les capacit√©s de raisonnement et de compr√©hension, offrant une profondeur d'analyse in√©gal√©e pour les applications les plus complexes. Sa conception architecturale sophistiqu√©e lui permet d'aborder des raisonnements multi-√©tapes avec rigueur et pr√©cision, tout en maintenant une coh√©rence globale remarquable. Id√©al pour les applications critiques n√©cessitant une intelligence artificielle capable d'un raisonnement nuanc√© et d'une compr√©hension contextuelle approfondie comparable aux analyses d'experts humains dans des domaines sp√©cialis√©s.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: true # Ajout√© depuis table (‚úÖ)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: false # Vitesse 36 <= 50
    tags:
      - Agent # Fusionn√© depuis badges
      - Raisonnement # Fusionn√© depuis badges (et d√©j√† pr√©sent)
      - Compr√©hension
      - Analyse
      # Efficient tag removed (conso 5.08 >= 2.0)
    use_cases:
      - Analyse de sc√©narios multi-factoriels avec √©valuation probabiliste des r√©sultats
      - R√©solution de probl√®mes scientifiques avec d√©monstration formelle des √©tapes
      - Applications √† haute criticit√© n√©cessitant pr√©cision et v√©rifiabilit√© des r√©sultats
      - Syst√®mes experts dans des domaines sp√©cialis√©s (juridique, m√©dical, technique)
      - Analyse avec raisonnement multi-√©tapes et explicabilit√© compl√®te des conclusions

  - name: Qwen3 32B
    editor: Qwen Team
    location: FR üá´üá∑
    parameters: 32
    parameters_string: 32B
    context_size: 40000 # D√©fini par l'utilisateur, ajust√©
    speed: # √Ä d√©terminer apr√®s tests
    energy_per_million_tokens: # √Ä calculer apr√®s tests de vitesse et consommation
    licence: Apache 2.0 # Suppos√©e, comme les autres Qwen3
    description: Mod√®le puissant de la nouvelle g√©n√©ration Qwen3, offrant des capacit√©s avanc√©es en raisonnement, code, et agentique, avec un contexte √©tendu.
    details: Fait partie de la s√©rie Qwen3, entra√Æn√© sur un vaste corpus de donn√©es. Ce mod√®le de 32 milliards de param√®tres est con√ßu pour exceller dans les t√¢ches complexes, supporter plus de 100 langues et int√©grer des modes de pens√©e hybrides pour une meilleure performance.
    supports_tools: true # Capacit√©s Agentiques / MCP
    supports_vision: false # Non mentionn√© pour Qwen3 dense
    supports_reasoning: true # Mode Thinking / Raisonnement am√©lior√©
    supports_security: false # Non mentionn√©
    is_fast: # √Ä d√©terminer apr√®s tests
    tags:
      - Agent
      - Raisonnement
      - Multilingue
      - Grand Contexte
    use_cases:
      - Agents conversationnels avanc√©s avec grand contexte et int√©gration d'outils (MCP)
      - R√©solution de probl√®mes complexes (maths, code) avec mode "Thinking"
      - Analyse et g√©n√©ration de documents volumineux
      - Applications multilingues (>100 langues) n√©cessitant une compr√©hension profonde

  - name: QwQ-32B
    editor: Qwen Team # Mis √† jour depuis table
    location: FR üá´üá∑ # Mis √† jour
    parameters: 32
    parameters_string: 32B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 35 tokens/seconde
    energy_per_million_tokens: 5.22 # (658W / 35 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: Apache 2.0
    description: Mod√®le de 32 milliards de param√®tres am√©lior√© par apprentissage par renforcement (RL) pour exceller dans le raisonnement, le codage, les math√©matiques et les t√¢ches d'agent.
    details: Ce mod√®le utilise une approche RL innovante avec des r√©compenses bas√©es sur les r√©sultats (v√©rificateurs de pr√©cision pour les maths, ex√©cution de code pour le codage) et un entra√Ænement multi-√©tapes pour am√©liorer les capacit√©s g√©n√©rales sans d√©grader les performances sp√©cialis√©es. Il int√®gre des capacit√©s d'agent pour utiliser des outils et adapter son raisonnement. Licence Apache 2.0.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: true # Ajout√© depuis table (‚úÖ)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: false # Vitesse 38 <= 50
    tags:
      - Agent # Fusionn√© depuis badges (et d√©j√† pr√©sent)
      - Raisonnement # Fusionn√© depuis badges (et d√©j√† pr√©sent)
      - Codage
      - Math√©matiques
      # Rapide tag removed (vitesse 38 <= 50)
      # Efficient tag removed (conso 4.81 >= 2.0)
    use_cases:
      - R√©solution de probl√®mes complexes n√©cessitant raisonnement et utilisation d'outils
      - G√©n√©ration et ex√©cution de code avec v√©rification des r√©sultats
      - T√¢ches math√©matiques avanc√©es avec v√©rification de l'exactitude
      - Applications d'agent capables d'interagir avec l'environnement
      - Instruction following am√©lior√© et alignement avec les pr√©f√©rences humaines

  - name: DeepSeek-R1 14B
    editor: DeepSeek AI # Correspondance Table: DeepSeek AI
    location: FR üá´üá∑ # Mis √† jour
    parameters: 14
    parameters_string: 14B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 60 tokens/seconde
    energy_per_million_tokens: 3.05 # (658W / 60 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: MIT licence
    description: Version compacte et efficiente du mod√®le DeepSeek-R1, offrant un excellent compromis entre performance et l√©g√®ret√© pour les d√©ploiements n√©cessitant flexibilit√© et r√©activit√©.
    details: Repr√©sentant un √©quilibre optimal entre performance et efficacit√©, cette version compacte du mod√®le DeepSeek-R1 conserve les principales qualit√©s de raisonnement et d'analyse de son homologue plus volumineux, tout en permettant un d√©ploiement plus l√©ger et plus flexible. Sa conception soigneusement optimis√©e assure des r√©sultats de qualit√© sur un large √©ventail de t√¢ches, tout en minimisant les exigences en ressources computationnelles. Cette combinaison en fait le choix id√©al pour les applications n√©cessitant un d√©ploiement agile sans compromis majeur sur les capacit√©s fondamentales.
    supports_tools: false # Correspondance Table: ‚ùå Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: true # Ajout√© depuis table (‚úÖ)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 67 > 50
    tags:
      - Agent # Fusionn√© depuis badges
      - Raisonnement # Fusionn√© depuis badges
      - Compact
      - Polyvalent
      - Rapide # (OK)
      # Efficient tag removed (conso 2.73 >= 2.0)
    use_cases:
      - Applications g√©n√©ralistes avec besoins d'inf√©rence rapide (44 tokens/s)
      - D√©ploiements sur serveurs standard sans GPU sp√©cialis√© (14B param√®tres)
      - Traitement de texte avec analyse contextuelle et temps de r√©ponse < 2s
      - D√©ploiement sur edge computing avec inf√©rence locale optimis√©e
      - Prototypage rapide d'applications IA avec temps d'it√©ration court

  - name: DeepSeek-R1 32B
    editor: DeepSeek AI # Correspondance Table: DeepSeek AI
    location: FR üá´üá∑ # Mis √† jour
    parameters: 32
    parameters_string: 32B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 33 tokens/seconde
    energy_per_million_tokens: 5.54 # (658W / 33 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: MIT licence
    description: Version interm√©diaire du mod√®le DeepSeek-R1 offrant un √©quilibre strat√©gique entre les capacit√©s avanc√©es de la version 70B et l'efficience de la version 14B, pour une polyvalence et performance optimales.
    details: Cette version interm√©diaire du mod√®le DeepSeek-R1 combine intelligemment puissance et efficacit√©, proposant des performances significativamente am√©lior√©es par rapport √† la version 14B tout en maintenant une empreinte plus l√©g√®re que la version 70B. Cette position strat√©gique dans la gamme en fait une option particuli√®rement int√©ressante pour les d√©ploiements n√©cessitant des capacit√©s de raisonnement avanc√©es sans les exigences mat√©rielles des plus grands mod√®les. Sa polyvalence lui permet d'exceller sur un large √©ventail de t√¢ches, de l'analyse de texte √† la g√©n√©ration de contenu structur√©.
    supports_tools: false # Correspondance Table: ‚ùå Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: true # Ajout√© depuis table (‚úÖ)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: false # Vitesse 37 <= 50
    tags:
      - Agent # Fusionn√© depuis badges
      - Raisonnement # Fusionn√© depuis badges
      - Polyvalent
      # Efficient tag removed (conso 4.94 >= 2.0)
    use_cases:
      - Applications n√©cessitant un bon √©quilibre puissance/co√ªt (32B param√®tres)
      - Traitement de texte professionnel avec analyse des subtilit√©s s√©mantiques
      - G√©n√©ration automatis√©e de rapports structur√©s √† partir de donn√©es brutes
      - Applications combinant analyse de donn√©es et g√©n√©ration de contenus
      - Assistants sp√©cialis√©s pour secteurs techniques (juridique, m√©dical, technique)

  - name: Cogito 3B
    editor: Deep Cogito # Correspondance Table: Deep Cogito
    location: FR üá´üá∑ # Mis √† jour
    parameters: 3
    parameters_string: 3B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 63 tokens/seconde
    energy_per_million_tokens: 0.86 # (195W / 63 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: LLAMA 3.2 Community Licence
    description: Version compacte du mod√®le Cogito, optimis√©e pour le raisonnement sur des appareils √† ressources limit√©es.
    details: Offre les capacit√©s de raisonnement de la famille Cogito dans un format tr√®s l√©ger (3 milliards de param√®tres), id√©al pour les d√©ploiements embarqu√©s ou les environnements CPU.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: true # Ajout√© depuis table (‚úÖ)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 63 > 50
    tags:
      - Raisonnement
      - Compact
      - Embarqu√©
      - Efficient # (OK)
      - Rapide # Added (vitesse 63 > 50)

  - name: Granite Embedding
    editor: IBM # Correspondance Table: IBM
    location: FR üá´üá∑ # Mis √† jour
    parameters: 0.278
    parameters_string: 278M # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    # speed: N/A # Non applicable pour les mod√®les d'embedding
    # energy_per_million_tokens: N/A # Pas de vitesse associ√©e
    licence: Apache 2.0
    description: Mod√®le d'embedding ultra-l√©ger d'IBM pour la recherche s√©mantique et la classification.
    details: Con√ßu pour g√©n√©rer des repr√©sentations vectorielles denses de texte, ce mod√®le est optimis√© pour l'efficacit√© et la performance dans les t√¢ches de similarit√© s√©mantique, de clustering et de classification. Sa taille r√©duite le rend id√©al pour les d√©ploiements √† grande √©chelle.
    supports_tools: false # Correspondance Table: ‚ùå Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: false # Pas de vitesse
    tags:
      - Embedding
      - Compact
      - S√©mantique
      - Efficient # (OK)
    # power_consumption: 145 W # Mac Mini (Removed as per plan)

  - name: Granite 3 Guardian 2B
    editor: IBM # Correspondance Table: IBM
    location: FR üá´üá∑ # Mis √† jour
    parameters: 2
    parameters_string: 2B # Ajout√© depuis table
    context_size: 8192 # Correspondance Table: 8k
    # speed: N/A # Non applicable pour les mod√®les Guardian
    # energy_per_million_tokens: N/A # Pas de vitesse associ√©e
    licence: Apache 2.0
    description: Mod√®le compact d'IBM sp√©cialis√© dans la s√©curit√© et la conformit√©, d√©tectant les risques et les contenus inappropri√©s.
    details: Version l√©g√®re de la famille Guardian, entra√Æn√©e pour identifier et filtrer les contenus nuisibles, les biais et les risques de s√©curit√© dans les interactions textuelles. Offre une protection robuste avec une faible empreinte computationnelle. Contexte limit√© √† 8k tokens.
    supports_tools: false # Correspondance Table: ‚ùå Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: true # Ajout√© depuis table (‚úÖ)
    is_fast: false # Pas de vitesse
    tags:
      - S√©curit√©
      - Conformit√©
      - Compact
      - Filtrage
      - Efficient # (OK)
    # power_consumption: 145 W # Mac Mini (Removed as per plan)

  - name: Granite 3 Guardian 8B
    editor: IBM # Correspondance Table: IBM
    location: FR üá´üá∑ # Mis √† jour
    parameters: 8
    parameters_string: 8B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    # speed: N/A # Non applicable pour les mod√®les Guardian
    # energy_per_million_tokens: N/A # Pas de vitesse associ√©e
    licence: Apache 2.0
    description: Mod√®le d'IBM sp√©cialis√© dans la s√©curit√© et la conformit√©, offrant des capacit√©s avanc√©es de d√©tection des risques.
    details: Mod√®le de taille interm√©diaire de la famille Guardian, fournissant une analyse de s√©curit√© plus approfondie que la version 2B. Id√©al pour les applications n√©cessitant une surveillance rigoureuse du contenu et une conformit√© stricte.
    supports_tools: false # Correspondance Table: ‚ùå Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: true # Ajout√© depuis table (‚úÖ)
    is_fast: false # Pas de vitesse
    tags:
      - S√©curit√©
      - Conformit√©
      - Filtrage
    # Efficient tag removed (pas de vitesse/conso)
    # power_consumption: 480 W # A100 (Removed as per plan)

  - name: Qwen 2.5 0.5B
    editor: Qwen Team # Mis √† jour depuis table
    location: FR üá´üá∑ # Mis √† jour
    parameters: 0.5
    parameters_string: 0.5B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 57 tokens/seconde
    energy_per_million_tokens: 0.95 # (195W / 57 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: MIT licence
    description: Micro-mod√®le ultra-l√©ger de la famille Qwen 2.5, con√ßu pour une efficacit√© maximale sur appareils contraints.
    details: Le plus petit mod√®le de la s√©rie Qwen 2.5, offrant des capacit√©s de base en traitement de langage avec une empreinte minimale. Id√©al pour les t√¢ches tr√®s simples sur des appareils IoT ou mobiles.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 53 > 50
    tags:
      - Ultra-compact
      - Rapide # (OK)
      - Embarqu√©
      - Efficient # (OK)

  - name: Qwen 2.5 1.5B
    editor: Qwen Team # Mis √† jour depuis table
    location: FR üá´üá∑ # Mis √† jour
    parameters: 1.5
    parameters_string: 1.5B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 94 tokens/seconde
    energy_per_million_tokens: 0.58 # (195W / 94 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: MIT licence
    description: Mod√®le tr√®s compact de la famille Qwen 2.5, offrant un bon √©quilibre performance/taille pour les d√©ploiements l√©gers.
    details: Mod√®le l√©g√®rement plus grand que la version 0.5B, offrant des capacit√©s am√©lior√©es tout en restant tr√®s efficace. Convient aux applications mobiles ou embarqu√©es n√©cessitant un peu plus de puissance.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 107 > 50
    tags:
      - Compact
      - Rapide # Added (vitesse 107 > 50)
      - Embarqu√©
      - Efficient # (OK)

  - name: Qwen 2.5 14B
    editor: Qwen Team # Mis √† jour depuis table
    location: FR üá´üá∑ # Mis √† jour
    parameters: 14
    parameters_string: 14B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 61 tokens/seconde
    energy_per_million_tokens: 3.00 # (658W / 61 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: MIT licence
    description: Mod√®le polyvalent de taille moyenne de la famille Qwen 2.5, bon √©quilibre performance/ressources.
    details: Offre de solides capacit√©s multilingues et de compr√©hension g√©n√©rale dans un format 14B. Convient √† une large gamme d'applications n√©cessitant un mod√®le fiable sans les exigences des tr√®s grands mod√®les.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 68 > 50
    tags:
      - Polyvalent
      - Multilingue
      - Rapide # (OK)
      # Efficient tag removed (conso 2.69 >= 2.0)

  - name: Qwen 2.5 32B
    editor: Qwen Team # Mis √† jour depuis table
    location: FR üá´üá∑ # Mis √† jour
    parameters: 32
    parameters_string: 32B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 32 tokens/seconde
    energy_per_million_tokens: 5.73 # (658W / 32 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 + Cooling)
    licence: MIT licence
    description: Mod√®le puissant de la famille Qwen 2.5, offrant des capacit√©s avanc√©es en compr√©hension et g√©n√©ration.
    details: Version 32B de Qwen 2.5, fournissant des performances accrues par rapport √† la version 14B, notamment en raisonnement et en suivi d'instructions complexes, tout en restant plus l√©ger que le mod√®le 72B.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: true # Ajout√© depuis table (‚úÖ)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: false # Vitesse 36 <= 50
    tags:
      - Polyvalent
      - Multilingue
      - Raisonnement
      # Efficient tag removed (conso 5.08 >= 2.0)

  - name: Qwen 2.5 3B
    editor: Qwen Team # Mis √† jour depuis table
    location: FR üá´üá∑ # Mis √† jour
    parameters: 3
    parameters_string: 3B # Ajout√© depuis table
    context_size: 32000 # Correspondance Table: 32k
    speed: 60 tokens/seconde
    energy_per_million_tokens: 0.90 # (195W / 60 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: MIT licence
    description: Mod√®le compact et efficace de la famille Qwen 2.5, adapt√© aux t√¢ches g√©n√©rales sur ressources limit√©es.
    details: Offre un bon compromis entre les capacit√©s des mod√®les 1.5B et 14B. Id√©al pour les applications n√©cessitant une bonne compr√©hension g√©n√©rale dans un format l√©ger et rapide.
    supports_tools: true # Correspondance Table: ‚úÖ Agent
    supports_vision: false # Correspondance Table: ‚ùå Vision
    supports_reasoning: false # Ajout√© depuis table (‚ùå)
    supports_security: false # Ajout√© depuis table (‚ùå)
    is_fast: true # Vitesse 57 > 50
    tags:
      - Compact
      - Rapide # Added (vitesse 57 > 50)
      - Polyvalent
      - Efficient # (OK)

  - name: Qwen3 0.6b # Nouveau mod√®le
    editor: Qwen Team
    location: FR üá´üá∑
    parameters: 0.6
    parameters_string: 0.6B
    context_size: 32000
    speed: 60 tokens/seconde
    energy_per_million_tokens: 0.90 # (195W / 60 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Apache 2.0
    description: Mod√®le compact et efficace de la famille Qwen3, adapt√© aux t√¢ches g√©n√©rales sur ressources limit√©es.
    details: Offre un bon compromis entre les capacit√©s des mod√®les ultra-compacts et les mod√®les plus grands. Id√©al pour les applications n√©cessitant une bonne compr√©hension g√©n√©rale dans un format l√©ger et rapide.
    supports_tools: true
    supports_vision: false
    supports_reasoning: false
    supports_security: false
    is_fast: true # Vitesse 58 > 50
    tags:
      - Compact
      - Rapide # Added (vitesse 58 > 50)
      - Polyvalent
      - Efficient # Added (conso 0.93 < 2.0)

  - name: Qwen3 1.7b # Nouveau mod√®le
    editor: Qwen Team
    location: FR üá´üá∑
    parameters: 1.7
    parameters_string: 1.7B
    context_size: 32000
    speed: 83 tokens/seconde
    energy_per_million_tokens: 0.65 # (195W / 83 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Apache 2.0
    description: Mod√®le tr√®s compact de la famille Qwen3, offrant un bon √©quilibre performance/taille pour les d√©ploiements l√©gers.
    details: Mod√®le l√©g√®rement plus grand que la version 0.6B, offrant des capacit√©s am√©lior√©es tout en restant tr√®s efficace. Convient aux applications mobiles ou embarqu√©es n√©cessitant un peu plus de puissance.
    supports_tools: true
    supports_vision: false
    supports_reasoning: false
    supports_security: false
    is_fast: true # Vitesse 84 > 50
    tags:
      - Compact
      - Rapide # Added (vitesse 84 > 50)
      - Embarqu√©
      - Efficient # Added (conso 0.64 < 2.0)

  - name: Qwen3 4b # Nouveau mod√®le
    editor: Qwen Team
    location: FR üá´üá∑
    parameters: 4
    parameters_string: 4B
    context_size: 32000
    speed: 48 tokens/seconde
    energy_per_million_tokens: 1.13 # (195W / 48 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Apache 2.0
    description: Mod√®le compact de la famille Qwen3 offrant d'excellentes performances dans un format l√©ger et √©conomique.
    details: Cette version compacte du mod√®le Qwen3 est optimis√©e pour les d√©ploiements avec contraintes de ressources tout en maintenant des performances remarquables pour sa taille. Son architecture efficiente permet une inf√©rence rapide sur du mat√©riel standard.
    supports_tools: true
    supports_vision: false
    supports_reasoning: false
    supports_security: false
    is_fast: false # Vitesse 50 <= 50
    tags:
      - Compact
      # Rapide tag removed (vitesse 50 <= 50)
      - Efficient # Added (conso 1.08 < 2.0)

  - name: Qwen3 8b # Nouveau mod√®le
    editor: Qwen Team
    location: FR üá´üá∑
    parameters: 8
    parameters_string: 8B
    context_size: 32000
    speed: 29 tokens/seconde
    energy_per_million_tokens: 1.87 # (195W / 29 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Apache 2.0
    description: Mod√®le Qwen3 8B offrant un bon √©quilibre entre performance et efficacit√© pour les t√¢ches g√©n√©rales.
    details: Version 8B de Qwen3, offrant des capacit√©s am√©lior√©es en raisonnement, code, maths et agent. Supporte plus de 100 langues et les modes de pens√©e hybrides.
    supports_tools: true
    supports_vision: false
    supports_reasoning: true
    supports_security: false
    is_fast: false # Vitesse 34 <= 50
    tags:
      - Raisonnement
      - Agent
      - Multilingue
      # Rapide tag removed (vitesse 34 <= 50)
      - Efficient # Added (conso 1.59 < 2.0)

  - name: Qwen2.5-VL 3B
    editor: Qwen Team
    location: FR üá´üá∑
    parameters: 3.8
    parameters_string: 3.8B
    context_size: 128000
    speed: 65 tokens/seconde
    energy_per_million_tokens: 0.83 # (195W / 65 t/s) / 3.6
    licence: Apache 2.0
    description: Mod√®le Vision-Langage compact, solution performante pour l'IA en p√©riph√©rie (edge AI).
    details: Qwen2.5-VL est le nouveau mod√®le phare vision-langage de Qwen, marquant une avanc√©e significative par rapport √† Qwen2-VL. Caract√©ristiques cl√©s - Compr√©hension visuelle (objets communs, textes, graphiques, ic√¥nes, mises en page). Capacit√©s d'agent visuel (raisonnement, direction dynamique d'outils pour utilisation d'ordinateur/t√©l√©phone). Localisation visuelle pr√©cise (bo√Ætes englobantes, points, sorties JSON stables). G√©n√©ration de sorties structur√©es (factures, formulaires, tableaux). Le Qwen2.5-VL-3B surpasse m√™me la version 7B de Qwen2-VL.
    supports_tools: true
    supports_vision: true
    supports_reasoning: true # Agentic capabilities
    supports_security: false
    is_fast: true # 65.63 > 50
    tags:
      - Vision
      - Agent
      - Raisonnement
      - Rapide
      - Efficient
      - OCR
      - Localisation Visuelle
      - Edge AI

  - name: Qwen2.5-VL 7B
    editor: Qwen Team
    location: FR üá´üá∑
    parameters: 8.3 # Note: le fichier de config Ollama indique 8.3B pour le 7B
    parameters_string: 7B (8.3B)
    context_size: 128000
    speed: 37 tokens/seconde
    energy_per_million_tokens: 1.46 # (195W / 37 t/s) / 3.6
    licence: Apache 2.0
    description: Mod√®le Vision-Langage performant, surpassant GPT-4o-mini sur certaines t√¢ches.
    details: Qwen2.5-VL est le nouveau mod√®le phare vision-langage de Qwen, marquant une avanc√©e significative par rapport √† Qwen2-VL. Caract√©ristiques cl√©s - Compr√©hension visuelle (objets communs, textes, graphiques, ic√¥nes, mises en page). Capacit√©s d'agent visuel (raisonnement, direction dynamique d'outils pour utilisation d'ordinateur/t√©l√©phone). Localisation visuelle pr√©cise (bo√Ætes englobantes, points, sorties JSON stables). G√©n√©ration de sorties structur√©es (factures, formulaires, tableaux). Le Qwen2.5-VL-7B-Instruct surpasse GPT-4o-mini dans plusieurs t√¢ches et est particuli√®rement performant pour la compr√©hension de documents et de diagrammes.
    supports_tools: true
    supports_vision: true
    supports_reasoning: true # Agentic capabilities
    supports_security: false
    is_fast: false # 37.93 <= 50
    tags:
      - Vision
      - Agent
      - Raisonnement
      - Efficient
      - OCR
      - Localisation Visuelle

  - name: Foundation-Sec-8B # Llama-3.1-FoundationAI-SecurityLLM-base-8B
    editor: Foundation AI ‚Äî Cisco
    location: FR üá´üá∑
    parameters: 8
    parameters_string: 8B
    context_size: 16000
    speed: 22 tokens/seconde
    energy_per_million_tokens: 2.46 # (195W / 22 t/s) / 3.6 -> kWh/Mtoken (Serveur Mac + Cooling)
    licence: Apache 2.0
    description: Mod√®le de langage sp√©cialis√© pour la cybers√©curit√©, optimis√© pour l'efficacit√©.
    details: Mod√®le Foundation-Sec-8B (Llama-3.1-FoundationAI-SecurityLLM-base-8B) bas√© sur Llama-3.1-8B, pr√©-entra√Æn√© sur un corpus cybers√©curit√©. Con√ßu pour la d√©tection de menaces, l'√©valuation de vuln√©rabilit√©s, l'automatisation de la s√©curit√©, etc. Optimis√© pour le d√©ploiement local. Contexte de 16k tokens.
    supports_tools: false # Bas√© sur le mod√®le de base, les capacit√©s agentiques sont probablement ajout√©es en aval.
    supports_vision: false
    supports_reasoning: true # Bas√© sur les benchmarks et cas d'usage.
    supports_security: true
    is_fast: false
    tags:
      - S√©curit√©
      - Compact

  - name: devstral 24B
    editor: Mistral AI & All Hands AI
    location: FR üá´üá∑
    parameters: 24
    parameters_string: 24B
    context_size: 120000
    speed: 53 tokens/seconde
    energy_per_million_tokens: 4.50 # (858W / 53 t/s) / 3.6 -> kWh/Mtoken (Serveur A100 ia06)
    licence: Apache 2.0
    description: Devstral est un LLM agentique pour les t√¢ches d'ing√©nierie logicielle.
    details: Devstral est un LLM agentique pour les t√¢ches d'ing√©nierie logicielle. Il excelle dans l'utilisation d'outils pour explorer les bases de code, modifier plusieurs fichiers et alimenter les agents d'ing√©nierie logicielle. Il est affin√© √† partir de Mistral Small 3.1, disposant ainsi d'une longue fen√™tre contextuelle allant jusqu'√† 128k tokens. 
    supports_tools: true
    supports_vision: false
    supports_reasoning: false
    supports_security: true
    is_fast: true # 53.37 > 50
    tags:
      - Agent
      - Programmation
      - Open-Source
      - Grand Contexte
    use_cases:
      - Exploration et modification de bases de code
      - Agentic
      - Europ√©en
    
# Cas d'usage recommand√©s
use_cases:
  - name: Dialogue multilingue
    description: Chatbots et assistants capables de communiquer dans plusieurs langues avec d√©tection automatique, maintien du contexte sur l'ensemble de la conversation et compr√©hension des sp√©cificit√©s linguistiques
    recommended_models:
      - Llama 3.3
      - Mistral Small 3.1
      - Qwen 2.5 
      - Granite 3.3 

  - name: Analyse de documents longs
    description: Traitement de documents volumineux (>100 pages) avec maintien du contexte sur l'ensemble du texte, extraction d'informations cl√©s, g√©n√©ration de r√©sum√©s pertinents et r√©ponse √† des questions sp√©cifiques sur le contenu
    recommended_models:
      - Gemma 3 
      - DeepSeek-R1 
      - Granite 3.3  # Mis √† jour

  - name: Programmation et d√©veloppement
    description: G√©n√©ration et optimisation de code dans multiples langages, d√©bogage, refactoring, d√©veloppement de fonctionnalit√©s compl√®tes, compr√©hension des impl√©mentations algorithmiques complexes et cr√©ation de tests unitaires
    recommended_models:
      - DeepCoder
      - QwQ
      - DeepSeek-R1 
      - Granite 3.3 # Ajout√© (FIM)
      - Devstral

  - name: Analyse visuelle
    description: Traitement direct d'images et documents visuels sans pr√©-traitement OCR, interpr√©tation de diagrammes techniques, graphiques, tableaux, dessins et photos avec g√©n√©ration d'explications textuelles d√©taill√©es du contenu visuel
    recommended_models:
      - Granite 3.2 Vision
      - Mistral Small 3.1
      - Gemma 3
      - Qwen2.5-VL
    
  - name: S√©curit√© et conformit√©
    description: Applications n√©cessitant des capacit√©s sp√©cifiques en mati√®re de s√©curit√© ; filtrage de contenu sensible, tra√ßabilit√© des raisonnements, v√©rification RGPD/HDS, minimisation des risques, analyse des vuln√©rabilit√©s et respect des r√©glementations sectorielles
    recommended_models:
      - Granite Guardian
      - Granite 3.3
      - Devstral
      - Mistral Small 3.1
      - Foundation-Sec-8B

  - name: D√©ploiements l√©gers et embarqu√©s
    description: Applications n√©cessitant une empreinte minimale en ressources, d√©ploiement sur appareils √† capacit√© limit√©e, inf√©rence en temps r√©el sur CPU standard et int√©gration dans des syst√®mes embarqu√©s ou IoT
    recommended_models:
      - Gemma 3
      - Granite 3.1 MoE
      - Granite Guardian
      - Granite 3.3
