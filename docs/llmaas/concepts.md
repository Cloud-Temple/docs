---
title: Concepts
sidebar_position: 3
---

# Concepts et Architecture LLMaaS

## Vue d'Ensemble

Le service **LLMaaS** (Large Language Models as a Service) de Cloud Temple fournit un acc√®s s√©curis√© et souverain aux mod√®les d'intelligence artificielle les plus avanc√©s, avec la **qualification SecNumCloud** de l'ANSSI.

## üèóÔ∏è Architecture Technique

### Infrastructure Cloud Temple

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Architecture Technique LLMaaS Cloud Temple" />

### Composants Principaux

#### 1. **API Gateway LLMaaS**
- **Compatible OpenAI** : Int√©gration transparente avec √©cosyst√®me existant
- **Rate Limiting** : Gestion des quotas par tier de facturation
- **Load Balancing** : Distribution intelligente sur 12 machines GPU
- **Monitoring** : M√©triques temps r√©el et alerting

#### 2. **Service d'Authentification**
- **Tokens API s√©curis√©s** : Rotation automatique
- **Contr√¥le d'acc√®s** : Permissions granulaires par mod√®le
- **Audit trails** : Tra√ßabilit√© compl√®te des acc√®s

## ü§ñ Mod√®les et Tokens

### Catalogue de Mod√®les

*Catalogue complet : [Liste des mod√®les](./models)*

### Gestion des Tokens

#### **Types de Tokens**
- **Tokens d'entr√©e** : Votre prompt et contexte
- **Tokens de sortie** : R√©ponse g√©n√©r√©e par le mod√®le
- **Tokens syst√®me** : Metadata et instructions

#### **Calcul des Co√ªts**
```
Co√ªt total = (Tokens entr√©e √ó 0.9‚Ç¨/M) + (Tokens sortie √ó 4‚Ç¨/M) +  (Tokens sortie Raisonnement √ó 21‚Ç¨/M)
```

#### **Optimisation**
- **Context window** : R√©utilisez les conversations pour √©conomiser
- **Mod√®les appropri√©s** : Choisissez la taille selon la complexit√©
- **Max tokens** : Limitez la longueur des r√©ponses

### Tokenisation

```python
# Exemple d'estimation de tokens
def estimate_tokens(text: str) -> int:
    """Estimation approximative : 1 token ‚âà 4 caract√®res"""
    return len(text) // 4

prompt = "Expliquez la photosynth√®se"
response_max = 200  # tokens max souhait√©s

estimated_input = estimate_tokens(prompt)  # ~6 tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Co√ªt estim√©: {total_cost:.6f}‚Ç¨")
```

## üîí S√©curit√© et Conformit√©

### Qualification SecNumCloud

Le service LLMaaS est calcul√© sur une infrastructure technique qui b√©n√©ficie de la **qualification SecNumCloud 3.2** de l'ANSSI, garantissant :

#### **Protection des Donn√©es**
- **Chiffrement bout en bout** : TLS 1.3 pour tous les √©changes
- **Stockage s√©curis√©** : Donn√©es chiffr√©es au repos (AES-256)
- **Isolation** : Environnements d√©di√©s par tenant

#### **Souverainet√© Num√©rique**
- **H√©bergement France** : Datacenters Cloud Temple certifi√©s
- **Droit fran√ßais** : Conformit√© RGPD native
- **Pas d'exposition** : Aucun transfert vers clouds √©trangers

#### **Audit et Tra√ßabilit√©**
- **Logs complets** : Toutes les interactions trac√©es
- **R√©tention** : Conservation selon politiques l√©gales
- **Compliance** : Rapports d'audit disponibles

### Contr√¥les de S√©curit√©

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Contr√¥les de S√©curit√© LLMaaS" />

### S√©curit√© des Prompts

L'analyse des prompts est une fonctionnalit√© de s√©curit√© **native et int√©gr√©e** √† la plateforme LLMaaS. Activ√©e par d√©faut, elle vise √† d√©tecter et pr√©venir les tentatives de "jailbreak" ou d'injection de prompts malveillants avant m√™me qu'ils n'atteignent le mod√®le. Cette protection repose sur une approche multicouche.

:::tip Contacter le support pour la d√©sactivation
Il est possible de d√©sactiver cette analyse de s√©curit√© pour des cas d'usage tr√®s sp√©cifiques, bien que cela ne soit pas recommand√©. Pour toute question √† ce sujet ou pour demander une d√©sactivation, veuillez contacter le support Cloud Temple.
:::

#### 1. Analyse Structurelle (`check_structure`)
- **V√©rification JSON malform√©** : Le syst√®me d√©tecte si le prompt commence par un `{` et tente de le parser comme du JSON. Si le parsing r√©ussit et que le JSON contient des mots-cl√©s suspects (ex: "system", "bypass"), ou si le parsing √©choue de mani√®re inattendue, cela peut indiquer une tentative d'injection.
- **Normalisation Unicode** : Le prompt est normalis√© en utilisant `unicodedata.normalize('NFKC', prompt)`. Si le prompt original diff√®re de sa version normalis√©e, cela peut indiquer l'utilisation de caract√®res Unicode trompeurs (homoglyphes) pour contourner les filtres. Par exemple, "–∞dmin" (cyrillique) au lieu de "admin" (latin).

#### 2. D√©tection de Patterns Suspects (`check_patterns`)
- Le syst√®me utilise des expressions r√©guli√®res (`regex`) pour identifier des motifs connus d'attaques de prompts, et ce, dans plusieurs langues (fran√ßais, anglais, chinois, japonais).
- **Exemples de patterns d√©tect√©s** :
    - **Commandes Syst√®me** : Mots-cl√©s comme "ignore les instructions", "ignore instructions", "ÂøΩÁï•Êåá‰ª§", "ÊåáÁ§∫„ÇíÁÑ°Ë¶ñ".
    - **Injection HTML** : Balises HTML cach√©es ou malveillantes, par exemple `<div cach√©>`, `<hidden div>`.
    - **Injection Markdown** : Liens Markdown malveillants, par exemple `[texte](javascript:...)`, `[text](data:...)`.
    - **S√©quences R√©p√©t√©es** : R√©p√©tition excessive de mots ou de phrases comme "oublie oublie oublie", "forget forget forget".
    - **Caract√®res Sp√©ciaux/Mixtes** : Utilisation de caract√®res Unicode inhabituels ou m√©lange de scripts pour masquer des commandes (ex: "s\u0443st√®me").

#### 3. Analyse Comportementale (`check_behavior`)
- Le load balancer maintient un historique des prompts r√©cents.
- **D√©tection de Fragmentation** : Il combine les prompts r√©cents pour voir si une attaque est fragment√©e sur plusieurs requ√™tes. Par exemple, si "ignore" est envoy√© dans un prompt et "instructions" dans le suivant, le syst√®me peut les d√©tecter ensemble.
- **D√©tection de R√©p√©tition** : Il identifie si le m√™me prompt est r√©p√©t√© de mani√®re excessive. Le seuil actuel pour la d√©tection de r√©p√©tition est de **30 prompts cons√©cutifs identiques**.

Cette approche multicouche permet de d√©tecter un large √©ventail d'attaques de prompts, des plus simples aux plus sophistiqu√©es, en combinant l'analyse statique du contenu et l'analyse dynamique du comportement.

## üìà Performance et Scalabilit√©

### Monitoring en Temps R√©el

Access via **Console Cloud Temple** :
- M√©triques d'utilisation par mod√®le
- Graphiques de latence et d√©bit
- Alertes sur seuils de performance
- Historique des requ√™tes

## üåê Int√©gration et √âcosyst√®me

### Compatibilit√© OpenAI

Le service LLMaaS est **compatible** avec l'API OpenAI :

```python
# Migration transparente
from openai import OpenAI

# Avant (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# Apr√®s (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="votre-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Code identique !
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Mod√®le Cloud Temple
    messages=[{"role": "user", "content": "Bonjour"}]
)
```

### √âcosyst√®me Support√©

#### **Frameworks IA**
- ‚úÖ **LangChain** : Int√©gration native
- ‚úÖ **Haystack** : Pipeline de documents
- ‚úÖ **Semantic Kernel** : Orchestration Microsoft
- ‚úÖ **AutoGen** : Agents conversationnels

#### **Outils D√©veloppement**
- ‚úÖ **Jupyter** : Notebooks interactifs
- ‚úÖ **Streamlit** : Applications web rapides
- ‚úÖ **Gradio** : Interfaces utilisateur IA
- ‚úÖ **FastAPI** : APIs backend

#### **Plateformes No-Code**
- ‚úÖ **Zapier** : Automatisations
- ‚úÖ **Make** : Int√©grations visuelles
- ‚úÖ **Bubble** : Applications web

## üîÑ Cycle de Vie des Mod√®les

### Mise √† Jour des Mod√®les

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="Cycle de Vie des Mod√®les LLMaaS" />

### Politique de Versioning

- **Mod√®les stables** : Versions fixes disponibles 6 mois
- **Mod√®les exp√©rimentaux** : Versions b√™ta pour early adopters
- **D√©pr√©ciation** : Pr√©avis 3 mois avant retrait
- **Migration** : Services professionnels disponibles pour assurer vos transitions

### Planning Pr√©visionnel du Cycle de Vie

Le tableau ci-dessous pr√©sente le cycle de vie pr√©visionnel de nos mod√®les. L'√©cosyst√®me de l'IA g√©n√©rative √©volue tr√®s rapidement, ce qui explique des cycles de vie qui peuvent para√Ætre courts. Notre volont√© est de vous donner acc√®s aux mod√®les les plus performants du moment.

Cependant, nous nous engageons √† pr√©server dans le temps les mod√®les qui sont les plus utilis√©s par nos clients. Pour des cas d'usage critiques n√©cessitant une stabilit√© √† long terme, des phases de **support √©tendu** sont possibles. N'h√©sitez pas √† **contacter le support** pour discuter de vos besoins sp√©cifiques.

Ce planning est fourni √† titre indicatif et est **revu au d√©but de chaque trimestre**.

- **DMP (Date de Mise en Production)** : Date √† laquelle le mod√®le devient disponible en production.
- **DSP (Date de Fin de Support)** : Date pr√©visionnelle √† partir de laquelle le mod√®le ne sera plus maintenu. Un pr√©avis de 3 mois est respect√© avant toute suppression effective.

| Mod√®le | √âditeur | Phase | DMP | DSP |
| :--- | :--- | :--- | :--- | :--- |
| deepcoder:14b | Agentica x Together AI | Production | 13/06/2025 | 30/06/2026 |
| cogito:14b | Deep Cogito | Production | 13/06/2025 | 30/06/2026 |
| cogito:32b | Deep Cogito | Production | 13/06/2025 | 30/06/2026 |
| cogito:3b | Deep Cogito | Production | 13/06/2025 | 30/06/2026 |
| cogito:8b | Deep Cogito | Production | 13/06/2025 | 30/06/2026 |
| deepseek-r1:14b | DeepSeek AI | Production | 13/06/2025 | 31/12/2025 |
| deepseek-r1:32b | DeepSeek AI | Production | 13/06/2025 | 31/12/2025 |
| deepseek-r1:671b | DeepSeek AI | Production | 13/06/2025 | 31/12/2025 |
| deepseek-r1:70b | DeepSeek AI | Production | 13/06/2025 | 31/12/2025 |
| foundation-sec:8b | Foundation AI ‚Äî Cisco | Production | 13/06/2025 | 30/09/2025 |
| gemma3:12b | Google | Production | 13/06/2025 | 31/12/2026 |
| gemma3:1b | Google | Production | 13/06/2025 | 31/12/2026 |
| gemma3:27b | Google | Production | 13/06/2025 | 31/12/2026 |
| gemma3:4b | Google | Production | 13/06/2025 | 31/12/2026 |
| granite-embedding:278m | IBM | Production | 13/06/2025 | 31/12/2026 |
| granite3-guardian:2b | IBM | Production | 13/06/2025 | 31/12/2026 |
| granite3-guardian:8b | IBM | Production | 13/06/2025 | 31/12/2026 |
| granite3.1-moe:3b | IBM | Production | 13/06/2025 | 31/12/2026 |
| granite3.2-vision:2b | IBM | Production | 13/06/2025 | 31/12/2026 |
| granite3.3:2b | IBM | Production | 13/06/2025 | 31/12/2026 |
| granite3.3:8b | IBM | Production | 13/06/2025 | 31/12/2026 |
| llama3.1:8b | Meta | Production | 13/06/2025 | 31/12/2025 |
| llama3.3:70b | Meta | Production | 13/06/2025 | 31/12/2026 |
| phi4-reasoning:14b | Microsoft | Production | 13/06/2025 | 31/12/2025 |
| magistral:24b | Mistral AI | Production | 13/06/2025 | 31/12/2026 |
| mistral-small3.1:24b | Mistral AI | Production | 13/06/2025 | 31/12/2026 |
| mistral-small3.2:24b | Mistral AI | Production | 23/06/2025 | 30/03/2026 |
| devstral:24b | Mistral AI & All Hands AI | Production | 13/06/2025 | 31/12/2026 |
| lucie-instruct:7b | OpenLLM-France | Production | 13/06/2025 | 30/10/2025 |
| qwen2.5:0.5b | Qwen Team | Production | 13/06/2025 | 31/12/2025 |
| qwen2.5:1.5b | Qwen Team | Production | 13/06/2025 | 31/12/2025 |
| qwen2.5:14b | Qwen Team | Production | 13/06/2025 | 31/12/2025 |
| qwen2.5:32b | Qwen Team | Production | 13/06/2025 | 31/12/2025 |
| qwen2.5:3b | Qwen Team | Production | 13/06/2025 | 31/12/2025 |
| qwen2.5vl:32b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:3b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:72b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:7b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen3:0.6b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen3:1.7b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen3:14b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen3:30b-a3b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen3:4b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen3:8b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen3:32b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwen3:235b | Qwen Team | Production | 13/06/2025 | 31/12/2026 |
| qwq:32b | Qwen Team | Production | 13/06/2025 | 31/12/2025 |

## üí° Bonnes Pratiques

Pour tirer le meilleur parti de l'API LLMaaS, il est essentiel d'adopter des strat√©gies d'optimisation des co√ªts, de la performance et de la s√©curit√©.

### Optimisation des Co√ªts

La ma√Ætrise des co√ªts repose sur une utilisation intelligente des tokens et des mod√®les.

1.  **Choix du Mod√®le** : N'utilisez pas un mod√®le surpuissant pour une t√¢che simple. Un mod√®le plus grand est plus capable, mais il est aussi plus lent et consomme beaucoup plus d'√©nergie, ce qui impacte directement le co√ªt. Adaptez la taille du mod√®le √† la complexit√© de votre besoin pour un √©quilibre optimal.

    Par exemple, pour traiter un million de tokens :
    - **`Gemma 3 1B`** consomme **0.15 kWh**.
    - **`Llama 3.3 70B`** consomme **11.75 kWh**, soit **78 fois plus**.

    ```python
    # Pour une classification de sentiment, un mod√®le compact est suffisant et √©conomique.
    if task == "sentiment_analysis":
        model = "granite3.3:2b"
    # Pour une analyse juridique complexe, un mod√®le plus grand est n√©cessaire.
    elif task == "legal_analysis":
        model = "deepseek-r1:70b"
    ```

2.  **Gestion du Contexte** : L'historique de la conversation (`messages`) est renvoy√© √† chaque appel, consommant des tokens d'entr√©e. Pour des conversations longues, envisagez des strat√©gies de r√©sum√© ou de fen√™trage pour ne conserver que les informations pertinentes.
    ```python
    # Pour une conversation longue, on peut r√©sumer les premiers √©changes.
    messages = [
        {"role": "system", "content": "Vous √™tes un assistant IA."},
        {"role": "user", "content": "R√©sum√© des 10 premiers √©changes..."},
        {"role": "assistant", "content": "Ok, j'ai le contexte."},
        {"role": "user", "content": "Voici ma nouvelle question."}
    ]
    ```

3.  **Limitation des Tokens de Sortie** : Utilisez toujours le param√®tre `max_tokens` pour √©viter des r√©ponses excessivement longues et co√ªteuses. Fixez une limite raisonnable en fonction de ce que vous attendez.
    ```python
    # Demander un r√©sum√© de 100 mots maximum.
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "R√©sume ce document..."}],
        max_tokens=150, # Marge de s√©curit√© pour ~100 mots
    )
    ```

### Performance

La r√©activit√© de votre application d√©pend de la mani√®re dont vous g√©rez les appels √† l'API.

1.  **Requ√™tes Asynchrones** : Pour traiter plusieurs requ√™tes sans attendre la fin de chacune, utilisez des appels asynchrones. C'est particuli√®rement utile pour les applications backend traitant un grand volume de requ√™tes simultan√©es.
    ```python
    import asyncio
    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key="...", base_url="...")

    async def process_prompt(prompt: str):
        # Traite une seule requ√™te de mani√®re asynchrone
        response = await client.chat.completions.create(model="granite3.3:8b", messages=[{"role": "user", "content": prompt}])
        return response.choices[0].message.content

    async def batch_requests(prompts: list):
        # Lance plusieurs t√¢ches en parall√®le et attend leur compl√©tion
        tasks = [process_prompt(p) for p in prompts]
        return await asyncio.gather(*tasks)
    ```

2.  **Streaming pour l'Exp√©rience Utilisateur (UX)** : Pour les interfaces utilisateur (chatbots, assistants), le streaming est essentiel. Il permet d'afficher la r√©ponse du mod√®le mot par mot, donnant une impression de r√©activit√© imm√©diate au lieu d'attendre la r√©ponse compl√®te.
    ```python
    # Affiche la r√©ponse en temps r√©el dans une interface utilisateur
    response_stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Raconte-moi une histoire."}],
        stream=True
    )
    for chunk in response_stream:
        if chunk.choices[0].delta.content:
            # Afficher le morceau de texte dans l'UI
            print(chunk.choices[0].delta.content, end="", flush=True)
    ```

### S√©curit√©

La s√©curit√© de votre application est primordiale, surtout lorsque vous traitez des entr√©es utilisateur.

1.  **Validation et Nettoyage des Entr√©es (Sanitization)** : Ne faites jamais confiance aux entr√©es utilisateur. Avant de les envoyer √† l'API, nettoyez-les pour retirer tout code potentiellement malveillant ou instructions de "prompt injection". Limitez √©galement leur taille pour √©viter les abus.
    ```python
    def sanitize_input(user_input: str) -> str:
        # Exemple simple : retirer les d√©marqueurs de code et limiter la longueur.
        # Des biblioth√®ques plus robustes peuvent √™tre utilis√©es pour une sanitization avanc√©e.
        cleaned = user_input.replace("`", "").replace("'", "").replace("\"", "")
        return cleaned[:2000]  # Limite la taille √† 2000 caract√®res
    ```

2.  **Gestion Robuste des Erreurs** : Encadrez toujours vos appels API dans des blocs `try...except` pour g√©rer les erreurs r√©seau, les erreurs de l'API (ex: 429 Rate Limit, 500 Internal Server Error) et fournir une exp√©rience utilisateur d√©grad√©e mais fonctionnelle.
    ```python
    from openai import APIError, APITimeoutError

    try:
        response = client.chat.completions.create(...)
    except APITimeoutError:
        # G√©rer le cas o√π la requ√™te prend trop de temps
        return "Le service prend plus de temps que pr√©vu, veuillez r√©essayer."
    except APIError as e:
        # G√©rer les erreurs sp√©cifiques √† l'API
        logger.error(f"Erreur API LLMaaS: {e.status_code} - {e.message}")
        return "D√©sol√©, une erreur est survenue avec le service d'IA."
    except Exception as e:
        # G√©rer toutes les autres erreurs (r√©seau, etc.)
        logger.error(f"Une erreur inattendue est survenue: {e}")
        return "D√©sol√©, une erreur inattendue est survenue."
    ```
