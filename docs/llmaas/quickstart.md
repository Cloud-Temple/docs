---
title: Guide de D√©marrage
sidebar_position: 3
---

# Guide de D√©marrage Rapide

Ce guide vous permet de faire votre premi√®re requ√™te √† l'API LLMaaS en moins de 5 minutes.

## Pr√©requis

- Acc√®s √† la Console Cloud Temple
- Compte avec permissions LLMaaS activ√©es

## √âtape 1 : G√©n√©rer une cl√© API

1. Connectez-vous √† la Console Cloud Temple
2. Acc√©dez aux param√®tres de votre compte
3. G√©n√©rez une nouvelle cl√© API LLMaaS
4. Copiez et sauvegardez la cl√© (elle ne sera affich√©e qu'une fois)

## √âtape 2 : Tester la connexion

V√©rifiez que votre cl√© fonctionne en listant les mod√®les disponibles :

```bash
curl -X GET "https://api.ai.cloud-temple.com/v1/models" \
  -H "Authorization: Bearer VOTRE_CLE_API"
```

Vous devriez recevoir une liste JSON des mod√®les disponibles.

## √âtape 3 : Premi√®re requ√™te

Faites votre premi√®re g√©n√©ration de texte avec un mod√®le rapide :

```bash
curl -X POST "https://api.ai.cloud-temple.com/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer VOTRE_CLE_API" \
  -d '{
    "model": "granite3.3:8b",
    "messages": [
      {
        "role": "user",
        "content": "√âcris un haiku sur la technologie."
      }
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

## √âtape 4 : Tester avec Python

Installez la biblioth√®que requests et testez avec du code Python :

```bash
pip install requests
```

```python
import requests
import json

# Configuration
API_KEY = "VOTRE_CLE_API"
BASE_URL = "https://api.ai.cloud-temple.com/v1"

# Headers
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {API_KEY}"
}

# Requ√™te
payload = {
    "model": "granite3.3:8b",
    "messages": [
        {
            "role": "user",
            "content": "Explique-moi la photosynth√®se en 3 phrases."
        }
    ],
    "max_tokens": 150,
    "temperature": 0.7
}

response = requests.post(
    f"{BASE_URL}/chat/completions",
    headers=headers,
    json=payload
)

if response.status_code == 200:
    result = response.json()
    print(result["choices"][0]["message"]["content"])
else:
    print(f"Erreur: {response.status_code}")
    print(response.text)
```

## Choix du mod√®le

Pour votre premier test, utilisez un de ces mod√®les recommand√©s :

| Mod√®le | Usage | Vitesse | Note |
|--------|--------|---------|------|
| `granite3.3:8b` | Usage g√©n√©ral, √©quilibr√© | Rapide | Recommand√© pour d√©buter |
| `qwen3:14b` | T√¢ches complexes | Moyen | Mode "thinking" visible |
| `gemma3:4b` | Tests rapides, prototypage | Tr√®s rapide | R√©ponses d√©taill√©es |

Consultez le [catalogue complet des mod√®les](./models) pour plus d'options.

## Param√®tres recommand√©s

Pour d√©buter, utilisez ces param√®tres :

```json
{
  "temperature": 0.7,    // Cr√©ativit√© mod√©r√©e
  "max_tokens": 200,     // R√©ponses concises
  "top_p": 1.0,         // Diversit√© standard
  "stream": false       // R√©ponse compl√®te d'un coup
}
```

## Gestion des erreurs courantes

### Erreur 401 - Non autoris√©
```json
{"error": {"message": "Invalid API key", "type": "invalid_request_error"}}
```
**Solution** : V√©rifiez votre cl√© API dans la Console Cloud Temple.

### Erreur 400 - Mod√®le non trouv√©
```json
{"error": {"message": "Model not found", "type": "invalid_request_error"}}
```
**Solution** : Utilisez `/v1/models` pour lister les mod√®les disponibles.

### Erreur 429 - Limite de d√©bit
```json
{"error": {"message": "Rate limit exceeded", "type": "rate_limit_error"}}
```
**Solution** : Attendez quelques secondes et r√©essayez.

## Monitoring de l'usage

Dans la Console Cloud Temple, vous pouvez :
- Voir vos requ√™tes en temps r√©el
- Consulter votre consommation de tokens
- Configurer des alertes de co√ªt
- Analyser les performances par mod√®le

## Aller plus loin : Exemples de Tool Calling et Vision

Cette section fournit des exemples de scripts Python simples et autonomes pour illustrer des fonctionnalit√©s sp√©cifiques de l'API LLMaaS. Chaque exemple est con√ßu pour √™tre ex√©cut√© directement, avec des instructions claires pour la configuration et l'utilisation.

---

## üí° Exemples de Code Autonomes

Cette section fournit des exemples de scripts Python simples et autonomes pour illustrer des fonctionnalit√©s sp√©cifiques de l'API LLMaaS. Chaque exemple est con√ßu pour √™tre ex√©cut√© directement, avec des instructions claires pour la configuration et l'utilisation.

### 1. Exemple Simple de Tool Calling

Le "Tool Calling" (ou appel de fonction) permet √† un mod√®le de langage de demander l'ex√©cution d'une fonction que vous avez d√©finie dans votre code. C'est une fonctionnalit√© puissante pour connecter les LLMs √† des outils externes (API, bases de donn√©es, etc.).

Le flux est le suivant :
1.  L'utilisateur pose une question qui n√©cessite un outil (ex: "quel temps fait-il ?").
2.  Vous envoyez la question et la liste des outils disponibles √† l'API.
3.  Le mod√®le, au lieu de r√©pondre directement, renvoie une requ√™te `tool_calls` demandant d'ex√©cuter une fonction sp√©cifique avec certains arguments.
4.  Votre code ex√©cute la fonction demand√©e.
5.  Vous renvoyez le r√©sultat de la fonction au mod√®le.
6.  Le mod√®le utilise ce r√©sultat pour formuler une r√©ponse finale √† l'utilisateur.

**Structure des fichiers**

Pour cet exemple, cr√©ez un r√©pertoire `simple_tool_calling` avec les fichiers suivants :

-   `test_tool_calling.py`: Le script principal.
-   `requirements.txt`: Les d√©pendances Python.
-   `.env.example`: Un mod√®le pour votre fichier de configuration.

**`requirements.txt`**
```txt
httpx
python-dotenv
```

**`.env.example`**
```env
# URL de base de l'API LLMaaS
API_URL="https://api.ai.cloud-temple.com/v1"

# Votre cl√© API LLMaaS
API_KEY="votre_cle_api_ici"

# Optionnel: Mod√®le par d√©faut √† utiliser pour le test
# Assurez-vous que ce mod√®le est compatible avec le "tool calling"
DEFAULT_MODEL="qwen3:30b-a3b"
```

**Code Source (`test_tool_calling.py`)**

```python
# -*- coding: utf-8 -*-
"""
Exemple simple de Tool Calling avec l'API LLMaaS.

Ce script montre comment d√©finir un outil simple (une calculatrice),
l'envoyer √† un mod√®le compatible, et interpr√©ter la r√©ponse du mod√®le
pour ex√©cuter l'outil et renvoyer le r√©sultat.
"""
import os
import json
import httpx
from dotenv import load_dotenv

# --- Configuration ---
# Charger les variables d'environnement depuis un fichier .env
load_dotenv()

API_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
API_KEY = os.getenv("API_KEY")
# Utiliser un mod√®le connu pour bien g√©rer le tool calling
MODEL = os.getenv("DEFAULT_MODEL", "qwen3:30b-a3b")

# --- D√©finition de l'outil ---

def calculator(expression: str) -> str:
    """
    √âvalue une expression math√©matique simple.
    Exemple: "2 + 2 * 10"
    """
    try:
        # S√©curit√© : ne pas utiliser eval() directement en production sans validation stricte.
        # Pour cet exemple, nous limitons les caract√®res autoris√©s.
        allowed_chars = "0123456789+-*/(). "
        if not all(char in allowed_chars for char in expression):
            return "Erreur: L'expression contient des caract√®res non autoris√©s."
        # eval() est utilis√© ici pour la simplicit√© de l'exemple.
        result = eval(expression)
        return str(result)
    except Exception as e:
        return f"Erreur de calcul: {str(e)}"

# Description de l'outil au format attendu par l'API
TOOLS_AVAILABLE = [
    {
        "type": "function",
        "function": {
            "name": "calculator",
            "description": "√âvalue une expression math√©matique. Par exemple, '2+2*10'.",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "L'expression math√©matique √† √©valuer."
                    }
                },
                "required": ["expression"],
            },
        },
    }
]

# Mapping entre le nom de l'outil et la fonction Python √† appeler
TOOL_FUNCTIONS_MAP = {
    "calculator": calculator
}

# --- Logique principale ---

def run_chat_with_tool_calling():
    """
    Fonction principale qui ex√©cute le sc√©nario de test.
    """
    if not API_KEY:
        print("‚ùå Erreur: La variable d'environnement API_KEY n'est pas d√©finie.")
        print("Veuillez cr√©er un fichier .env ou l'exporter dans votre session.")
        return

    print(f"ü§ñ Mod√®le utilis√© : {MODEL}")
    print("-" * 30)

    # 1. Premier appel √† l'API avec la question de l'utilisateur
    # ---------------------------------------------------------
    print("‚û°Ô∏è √âtape 1: Envoi de la requ√™te initiale au LLM...")

    # L'historique des messages commence avec la question de l'utilisateur
    messages = [
        {"role": "user", "content": "Bonjour, peux-tu calculer 15 + (3 * 5) ?"}
    ]

    payload = {
        "model": MODEL,
        "messages": messages,
        "tools": TOOLS_AVAILABLE,
        "tool_choice": "auto",  # Le mod√®le d√©cide s'il doit utiliser un outil
    }

    try:
        with httpx.Client() as client:
            response = client.post(
                f"{API_URL}/chat/completions",
                headers={"Authorization": f"Bearer {API_KEY}"},
                json=payload,
                timeout=60,
            )
            response.raise_for_status()
            response_data = response.json()

    except httpx.HTTPStatusError as e:
        print(f"‚ùå Erreur API (HTTP Status) lors de l'√©tape 1: {e}")
        print(f"R√©ponse de l'API : {e.response.text}")
        return
    except httpx.RequestError as e:
        print(f"‚ùå Erreur API (Request) lors de l'√©tape 1: {e}")
        return

    # Le message de l'assistant contient la demande d'appel d'outil
    assistant_message = response_data["choices"][0]["message"]
    messages.append(assistant_message)

    # 2. V√©rification et ex√©cution de l'appel d'outil
    # ------------------------------------------------
    print("\n‚úÖ Le LLM a demand√© d'utiliser un outil.")

    if "tool_calls" not in assistant_message:
        print("ü§î Le mod√®le n'a pas demand√© d'utiliser un outil. R√©ponse directe :")
        print(assistant_message.get("content", "Pas de contenu."))
        return

    tool_call = assistant_message["tool_calls"][0]
    function_name = tool_call["function"]["name"]
    function_args_str = tool_call["function"]["arguments"]
    tool_call_id = tool_call["id"]

    print(f"   - Outil √† appeler : {function_name}")
    print(f"   - Arguments       : {function_args_str}")

    if function_name in TOOL_FUNCTIONS_MAP:
        function_to_call = TOOL_FUNCTIONS_MAP[function_name]
        try:
            # Les arguments sont une cha√Æne JSON, il faut les parser
            function_args = json.loads(function_args_str)
            tool_result = function_to_call(**function_args)
            print(f"   - R√©sultat de l'outil : {tool_result}")
        except Exception as e:
            print(f"‚ùå Erreur lors de l'ex√©cution de l'outil: {e}")
            tool_result = f"Erreur: {e}"
    else:
        print(f"‚ùå Outil inconnu : {function_name}")
        tool_result = f"Erreur: Outil '{function_name}' non trouv√©."

    # 3. Second appel √† l'API avec le r√©sultat de l'outil
    # ----------------------------------------------------
    print("\n‚û°Ô∏è √âtape 2: Envoi du r√©sultat de l'outil au LLM...")

    # On ajoute le r√©sultat de l'outil √† l'historique des messages
    messages.append(
        {
            "role": "tool",
            "tool_call_id": tool_call_id,
            "content": tool_result
        }
    )

    # On refait un appel SANS les outils cette fois-ci pour obtenir la r√©ponse finale
    payload_final = {
        "model": MODEL,
        "messages": messages,
    }

    try:
        with httpx.Client() as client:
            response_final = client.post(
                f"{API_URL}/chat/completions",
                headers={"Authorization": f"Bearer {API_KEY}"},
                json=payload_final,
                timeout=60,
            )
            response_final.raise_for_status()
            final_data = response_final.json()

    except (httpx.HTTPStatusError, httpx.RequestError) as e:
        print(f"‚ùå Erreur API lors de l'√©tape 2: {e}")
        return

    final_answer = final_data["choices"][0]["message"]["content"]
    print("\n‚úÖ R√©ponse finale du LLM :")
    print(f"üí¨ \"{final_answer}\"")


if __name__ == "__main__":
    run_chat_with_tool_calling()
```

**Utilisation**

1.  **Installez les d√©pendances :**
    ```bash
    pip install -r tests/llmaas/requirements.txt
    ```
2.  **Configurez votre cl√© API :**
    Copiez `tests/llmaas/.env.example` en `tests/llmaas/.env` et remplacez `"votre_cle_api_ici"` par votre cl√© API LLMaaS.
3.  **Ex√©cutez le script :**
    ```bash
    python tests/llmaas/test_tool_calling.py
    ```

### 2. Exemple Simple de Vision (Multimodal)

Les mod√®les multimodaux peuvent analyser √† la fois du texte et des images. Cet exemple montre comment envoyer une image et une question au mod√®le pour obtenir une description de ce qu'il "voit".

**Structure des fichiers**

Cr√©ez un r√©pertoire `simple_vision` avec les fichiers suivants :

-   `test_vision.py`: Le script principal.
-   `requirements.txt`: Les d√©pendances (incluant `Pillow` pour g√©n√©rer l'image).
-   `.env.example`: Le mod√®le de configuration.
-   `image_example.png`: L'image √† analyser (le script la g√©n√©rera pour vous si elle manque).

**`requirements.txt`**
```txt
httpx
python-dotenv
Pillow
```

**`.env.example`**
```env
# URL de base de l'API LLMaaS
API_URL="https://api.ai.cloud-temple.com/v1"

# Votre cl√© API LLMaaS
API_KEY="votre_cle_api_ici"

# Optionnel: Mod√®le par d√©faut √† utiliser pour le test
# Assurez-vous que ce mod√®le est multimodal (vision)
DEFAULT_MODEL="granite3.2-vision:2b"
```

**Code Source (`test_vision.py`)**

```python
# -*- coding: utf-8 -*-
"""
Exemple simple d'utilisation de l'API Vision de LLMaaS.

Ce script montre comment envoyer une image locale avec une question
√† un mod√®le de vision (multimodal) et afficher sa r√©ponse.
"""
import os
import base64
import httpx
from dotenv import load_dotenv

# --- Configuration ---
# Charger les variables d'environnement depuis un fichier .env
load_dotenv()

API_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
API_KEY = os.getenv("API_KEY")
# Utiliser un mod√®le de vision.
MODEL = os.getenv("DEFAULT_MODEL", "granite3.2-vision:2b")
IMAGE_PATH = "image_example.png" # L'image doit √™tre dans le m√™me r√©pertoire

# --- Fonctions ---

def encode_image_to_base64(image_path: str) -> str:
    """
    Encode une image en base64 pour l'inclure dans la requ√™te API.
    """
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except FileNotFoundError:
        print(f"‚ùå Erreur: Le fichier image '{image_path}' n'a pas √©t√© trouv√©.")
        return ""
    except Exception as e:
        print(f"‚ùå Erreur lors de l'encodage de l'image: {e}")
        return ""

def generate_example_image():
    """
    G√©n√®re une image simple pour le test si elle n'existe pas.
    N√©cessite la biblioth√®que Pillow (PIL).
    """
    try:
        from PIL import Image, ImageDraw
        if not os.path.exists(IMAGE_PATH):
            print(f"üñºÔ∏è  L'image '{IMAGE_PATH}' n'existe pas, g√©n√©ration en cours...")
            # Cr√©e une image simple : un carr√© rouge sur fond blanc
            img = Image.new('RGB', (200, 200), color = 'white')
            draw = ImageDraw.Draw(img)
            draw.rectangle([50, 50, 150, 150], fill='red', outline='black')
            img.save(IMAGE_PATH)
            print("‚úÖ Image d'exemple g√©n√©r√©e.")
    except ImportError:
        print("‚ö†Ô∏è  Avertissement: La biblioth√®que Pillow n'est pas install√©e.")
        print("   Veuillez installer 'Pillow' (`pip install Pillow`) pour g√©n√©rer l'image d'exemple,")
        print(f"   ou placez manuellement un fichier nomm√© '{IMAGE_PATH}' dans ce r√©pertoire.")
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration de l'image: {e}")


# --- Logique principale ---

def run_vision_test():
    """
    Fonction principale qui ex√©cute le sc√©nario de test de vision.
    """
    if not API_KEY:
        print("‚ùå Erreur: La variable d'environnement API_KEY n'est pas d√©finie.")
        return

    # G√©n√©rer l'image d'exemple si n√©cessaire
    generate_example_image()

    # Encoder l'image en base64
    base64_image = encode_image_to_base64(IMAGE_PATH)
    if not base64_image:
        return

    print(f"ü§ñ Mod√®le utilis√© : {MODEL}")
    print(f"üñºÔ∏è Image envoy√©e : {IMAGE_PATH}")
    print("-" * 30)

    # Construction du payload au format multimodal
    payload = {
        "model": MODEL,
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Que vois-tu sur cette image ? D√©cris la forme et la couleur principale."
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "max_tokens": 500 # Limiter la longueur de la description
    }

    print("‚û°Ô∏è Envoi de la requ√™te au LLM de vision...")
    try:
        with httpx.Client() as client:
            response = client.post(
                f"{API_URL}/chat/completions",
                headers={"Authorization": f"Bearer {API_KEY}"},
                json=payload,
                timeout=120, # Les mod√®les de vision peuvent √™tre plus longs
            )
            response.raise_for_status()
            response_data = response.json()

    except httpx.HTTPStatusError as e:
        print(f"‚ùå Erreur API (HTTP Status): {e}")
        print(f"R√©ponse de l'API : {e.response.text}")
        return
    except httpx.RequestError as e:
        print(f"‚ùå Erreur API (Request): {e}")
        return

    final_answer = response_data["choices"][0]["message"]["content"]
    print("\n‚úÖ R√©ponse du mod√®le :")
    print(f"üí¨ \"{final_answer}\"")


if __name__ == "__main__":
    run_vision_test()
```

**Utilisation**

1.  **Installez les d√©pendances :**
    ```bash
    pip install -r tests/llmaas/requirements.txt
    ```
2.  **Configurez votre cl√© API :**
    Copiez `tests/llmaas/.env.example` en `tests/llmaas/.env` et remplacez `"votre_cle_api_ici"` par votre cl√© API LLMaaS.
3.  **Ex√©cutez le script :**
    ```bash
    python tests/llmaas/test_vision.py
    ```
    Le script g√©n√©rera automatiquement une image `image_example.png` si elle n'existe pas.

---
## Prochaines √©tapes

Une fois votre premier test r√©ussi :

1. **Explorez les mod√®les** : Testez diff√©rents mod√®les selon vos besoins
2. **Optimisez les prompts** : Am√©liorez la qualit√© des r√©ponses
3. **Int√©grez dans votre application** : Consultez la [documentation API](./api)
4. **Cas d'usage avanc√©s** : Voir les [tutoriels](./tutorials)

## Support

En cas de probl√®me :
- Consultez la [documentation API compl√®te](./api)
- V√©rifiez le statut du service dans la Console
- Contactez le support via la Console Cloud Temple
