---
title: Tutoriels 
sidebar_position: 6
---

# Tutorials LLMaaS

## Vue d'overview

Ces tutorials avanc√©s couvrent l'int√©gration, l'optimisation et les meilleures pratiques pour exploiter pleinement LLMaaS Cloud Temple en production. Chaque tutorial inclut du code test√© et des m√©triques de performance r√©elles.

## üöÄ Int√©grations LangChain et Frameworks

### 1. Int√©gration de base avec LangChain

Ce premier exemple montre comment int√©grer notre API LLMaaS avec le framework populaire LangChain en cr√©ant un "wrapper" personnalis√©. Un wrapper est une classe qui "enveloppe" notre API pour la rendre compatible avec les m√©canismes internes de LangChain.

#### Le code expliqu√©

Le code ci-dessous d√©finit une classe `CloudTempleLLM` qui h√©rite de la classe de base `LLM` de LangChain. Cela nous permet de d√©finir un comportement sur mesure tout en restant compatible avec l'√©cosyst√®me LangChain (cha√Ænes, agents, etc.).

1. **`CloudTempleLLM(LLM)`** : Notre classe h√©rite de `LLM`, ce qui nous oblige √† impl√©menter certaines m√©thodes, notamment `_call`.
2. **`_call(self, prompt: str, ...)`** : C'est le c≈ìur de notre wrapper. √Ä chaque fois que LangChain aura besoin de faire appel √† notre mod√®le de langage, il invoquera cette m√©thode. √Ä l'int√©rieur, nous formatons une requ√™te HTTP POST standard avec les bons headers (`Authorization`) et le `payload` attendu par notre API `/v1/chat/completions`.
3. **`exemple_langchain_basic()`** : Cette fonction de d√©monstration montre comment utiliser notre wrapper. On l'instancie, on cr√©e un `PromptTemplate` pour structurer notre requ√™te, et on les combine dans une `LLMChain`. Lorsque l'on ex√©cute la cha√Æne (`chain.run(...)`), LangChain appelle en coulisses la m√©thode `_call` que nous avons d√©finie.

Cette approche est utile si vous souhaitez un contr√¥le total sur la mani√®re dont LangChain interagit avec l'API, mais elle est plus verbeuse que l'utilisation du client `ChatOpenAI` (voir [API Reference](./api#langchain)).

```python
# Installation des d√©pendances
# pip install langchain requests pydantic

from langchain.llms.base import LLM
from langchain.schema import LLMResult, Generation
from typing import Optional, List, Any
from pydantic import Field
import requests
import json
import os

# --- Configuration ---
# Il est recommand√© de stocker votre cl√© API dans une variable d'environnement
API_KEY = os.getenv("LLMAAS_API_KEY", "votre-cl√©-api-ici")
BASE_URL = "https://api.ai.cloud-temple.com/v1"

class CloudTempleLLM(LLM):
    """
    Wrapper LangChain personnalis√© pour l'API LLMaaS de Cloud Temple.
    Cette classe permet d'utiliser notre API comme un LLM standard dans LangChain.
    """
    
    api_key: str = Field(default="")
    model_name: str = Field(default="granite3.3:8b")
    temperature: float = Field(default=0.7)
    max_tokens: int = Field(default=1000)
    
    @property
    def _llm_type(self) -> str:
        """Identifiant unique pour notre type de LLM."""
        return "cloud_temple_llmaas"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """
        La m√©thode principale qui effectue l'appel √† l'API LLMaaS.
        LangChain utilise cette m√©thode pour chaque requ√™te au mod√®le.
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        if stop:
            payload["stop"] = stop
        
        # Ex√©cution de la requ√™te POST vers l'API
        response = requests.post(
            f"{BASE_URL}/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        response.raise_for_status()  # L√®ve une exception en cas d'erreur HTTP
        result = response.json()
        
        # Retourne le contenu du message de l'assistant
        return result['choices'][0]['message']['content']

# --- Exemple d'utilisation ---
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

def exemple_langchain_wrapper():
    """D√©montre l'utilisation du wrapper LLM avec une cha√Æne LangChain."""
    
    # 1. Initialisation de notre LLM personnalis√©
    llm = CloudTempleLLM(
        api_key=API_KEY,
        model_name="granite3.3:8b"
    )
    
    # 2. Cr√©ation d'un template de prompt pour structurer les requ√™tes
    template = """
    Tu es un expert en {domaine}. 
    R√©ponds √† cette question de mani√®re d√©taill√©e et professionnelle :
    
    Question: {question}
    
    R√©ponse:
    """
    prompt = PromptTemplate(
        input_variables=["domaine", "question"],
        template=template
    )
    
    # 3. Cr√©ation d'une cha√Æne qui combine le prompt et le LLM
    chain = LLMChain(llm=llm, prompt=prompt)
    
    # 4. Ex√©cution de la cha√Æne avec des variables sp√©cifiques
    result = chain.run(
        domaine="cybers√©curit√©",
        question="Quelles sont les meilleures pratiques pour s√©curiser une API REST ?"
    )
    
    return result

# --- Lancement du test ---
if __name__ == "__main__":
    if API_KEY == "votre-cl√©-api-ici":
        print("Veuillez configurer votre LLMAAS_API_KEY dans vos variables d'environnement.")
    else:
        reponse = exemple_langchain_wrapper()
        print("R√©ponse de l'expert en cybers√©curit√© :\n")
        print(reponse)
```

### 2. RAG (Retrieval-Augmented Generation) avec l'API LLMaaS

Le RAG est une technique puissante qui permet √† un LLM de r√©pondre √† des questions en s'appuyant sur une base de connaissances externe. Ce tutoriel vous guide √† travers la cr√©ation d'un pipeline RAG simple en utilisant notre API pour les embeddings et la g√©n√©ration, et FAISS, une biblioth√®que de similarit√© vectorielle, pour cr√©er un index en m√©moire.

#### Le code expliqu√©

Le pipeline se d√©compose en plusieurs √©tapes logiques :

1. **Configuration** : Nous importons les biblioth√®ques n√©cessaires et chargeons notre cl√© API depuis les variables d'environnement. Nous d√©finissons les mod√®les √† utiliser : `granite-embedding:278m` pour la vectorisation et `granite3.3:8b` pour la g√©n√©ration.
2. **`LLMaaSEmbeddings`** : Comme dans l'exemple pr√©c√©dent, nous avons besoin d'un wrapper pour interagir avec notre API d'embeddings. Cette classe se charge de transformer les morceaux de texte (chunks) en vecteurs num√©riques (embeddings).
3. **`setup_rag_pipeline`** : Cette fonction orchestre la cr√©ation du pipeline.
    * **Chargement des documents** : `DirectoryLoader` charge les fichiers texte de notre base de connaissances.
    * **Division en chunks** : `RecursiveCharacterTextSplitter` d√©coupe les documents en plus petits morceaux. C'est essentiel pour que le mod√®le d'embedding puisse traiter efficacement le texte et pour que la recherche de similarit√© soit pr√©cise.
    * **Vectorisation et Indexation** : `FAISS.from_documents` est une √©tape cl√©. Elle prend les chunks de texte, utilise notre classe `LLMaaSEmbeddings` pour appeler l'API et obtenir les vecteurs correspondants, puis stocke ces vecteurs dans un index FAISS en m√©moire.
    * **Configuration du LLM** : Nous utilisons `ChatOpenAI` qui est nativement compatible avec notre API pour la partie g√©n√©ration de r√©ponse.
    * **Cr√©ation de la cha√Æne `RetrievalQA`** : C'est la cha√Æne LangChain qui lie tous les √©l√©ments. Quand on lui pose une question, elle :
        a. Utilise le `retriever` (bas√© sur notre index FAISS) pour trouver les chunks de texte les plus pertinents.
        b. "Stuff" (fourre) ces chunks dans un prompt avec la question.
        c. Envoie ce prompt enrichi au LLM pour g√©n√©rer une r√©ponse contextuelle.
4. **Ex√©cution** : La fonction `main` simule une utilisation r√©elle en cr√©ant des fichiers de connaissance temporaires, en construisant le pipeline et en posant une question.

```python
import os
import tempfile
import shutil
from pathlib import Path
from dotenv import load_dotenv
from typing import List

# --- Imports LangChain ---
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# --- Configuration ---
# Charge les variables d'environnement (ex: LLMAAS_API_KEY)
load_dotenv()
API_KEY = os.getenv("LLMAAS_API_KEY")
BASE_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
EMBEDDING_MODEL = "granite-embedding:278m"
LLM_MODEL = "granite3.3:8b"

# --- Classe d'Embedding Personnalis√©e ---
class LLMaaSEmbeddings(Embeddings):
    """Classe d'embedding personnalis√©e pour l'API LLMaaS de Cloud Temple."""
    def __init__(self, api_key: str, model_name: str):
        if not api_key:
            raise ValueError("La cl√© API LLMaaS ne peut pas √™tre vide.")
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = BASE_URL
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

    def _embed(self, texts: List[str]) -> List[List[float]]:
        import httpx
        payload = {"input": texts, "model": self.model_name}
        try:
            with httpx.Client(timeout=60.0) as client:
                response = client.post(f"{self.base_url}/embeddings", headers=self.headers, json=payload)
                response.raise_for_status()
                data = response.json()['data']
                data.sort(key=lambda e: e['index'])
                return [item['embedding'] for item in data]
        except httpx.HTTPStatusError as e:
            print(f"Erreur HTTP lors de la g√©n√©ration des embeddings: {e.response.text}")
            raise
        except Exception as e:
            print(f"Une erreur inattendue est survenue lors de la g√©n√©ration des embeddings: {e}")
            raise

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embed(texts)

    def embed_query(self, text: str) -> List[float]:
        # La m√©thode _embed attend une liste, nous encapsulons donc le texte unique.
        return self._embed([text])[0]

# --- Pipeline RAG ---
def setup_rag_pipeline(documents_path: str):
    """Configuration compl√®te du pipeline RAG avec les outils LLMaaS."""
    print("1. Chargement et division des documents...")
    loader = DirectoryLoader(documents_path, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(documents)
    print(f"   -> {len(documents)} document(s) charg√©(s) et divis√©(s) en {len(splits)} chunks.")
    
    print(f"2. Cr√©ation des embeddings via LLMaaS (mod√®le: {EMBEDDING_MODEL})...")
    embeddings = LLMaaSEmbeddings(api_key=API_KEY, model_name=EMBEDDING_MODEL)
    
    print("3. Cr√©ation de l'index vectoriel en m√©moire (FAISS)...")
    vectorstore = FAISS.from_documents(splits, embeddings)
    print("   -> Index FAISS cr√©√© avec succ√®s.")
    
    print(f"4. Configuration du LLM (mod√®le: {LLM_MODEL})...")
    # Correction pour la compatibilit√© Pydantic/LangChain
    from langchain_core.caches import BaseCache
    from langchain_core.callbacks.base import Callbacks
    ChatOpenAI.model_rebuild()
    
    llm = ChatOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=LLM_MODEL,
        temperature=0.3,
        model_kwargs={"max_tokens": 300}
    )
    
    print("5. Cr√©ation de la cha√Æne de Question/R√©ponse (RAG)...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    print("   -> Pipeline RAG pr√™t.")
    return qa_chain

# --- Ex√©cution ---
def main():
    """Fonction principale pour ex√©cuter le pipeline RAG de bout en bout."""
    if not API_KEY:
        print("Erreur: La variable d'environnement LLMAAS_API_KEY n'est pas d√©finie.")
        return

    # Cr√©er des documents de test temporaires
    temp_dir = tempfile.mkdtemp()
    print(f"\nCr√©ation de documents de test dans: {temp_dir}")
    try:
        documents_content = {
            "overview.txt": "Cloud Temple est un fournisseur de cloud souverain fran√ßais qualifi√© SecNumCloud.",
            "pricing.txt": "Les tarifs de l'API LLMaaS sont de 0.9‚Ç¨/million de tokens en entr√©e et 4‚Ç¨/million en sortie."
        }
        for filename, content in documents_content.items():
            with open(Path(temp_dir) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # Configurer et ex√©cuter le pipeline
        rag_chain = setup_rag_pipeline(temp_dir)
        
        print("\n--- Interrogation du Pipeline RAG ---")
        question = "Quel est le tarif des tokens en sortie sur l'API LLMaaS de Cloud Temple ?"
        result = rag_chain({"query": question})
        
        print(f"\nQuestion: {question}")
        print(f"R√©ponse: {result['result']}")
        print("\nSources utilis√©es pour la r√©ponse:")
        for source in result["source_documents"]:
            print(f"- Fichier: {os.path.basename(source.metadata['source'])}")
            print(f"  Contenu: \"{source.page_content}\"")

    finally:
        # Nettoyer le r√©pertoire temporaire
        print(f"\nNettoyage du r√©pertoire temporaire: {temp_dir}")
        shutil.rmtree(temp_dir)

if __name__ == "__main__":
    main()
```

### 3. Int√©gration avec une base de donn√©es vectorielle (Qdrant)

Pour des applications RAG en production, l'utilisation d'une base de donn√©es vectorielle d√©di√©e comme **Qdrant** est recommand√©e. Contrairement √† FAISS qui fonctionne en m√©moire, Qdrant offre la persistance des donn√©es, des capacit√©s de recherche avanc√©es et une meilleure scalabilit√©.

#### Le code expliqu√©

Ce tutoriel adapte le pipeline RAG pr√©c√©dent pour utiliser Qdrant.

1. **Pr√©requis** : La premi√®re √©tape est de lancer une instance de Qdrant. Le moyen le plus simple est d'utiliser Docker.
2. **`setup_qdrant_rag_pipeline`** :
    * **Embeddings et Documents** : La cr√©ation des embeddings et des documents reste identique √† l'exemple pr√©c√©dent.
    * **Connexion √† Qdrant** : Au lieu de cr√©er un index FAISS, nous utilisons `Qdrant.from_documents`. Cette m√©thode LangChain g√®re plusieurs √©tapes :
        a. Elle se connecte √† votre instance Qdrant via l'URL fournie.
        b. Elle cr√©e une nouvelle "collection" (l'√©quivalent d'une table dans une base de donn√©es SQL) si elle n'existe pas.
        c. Elle appelle notre classe `LLMaaSEmbeddings` pour vectoriser les documents.
        d. Elle ins√®re les documents et leurs vecteurs dans la collection Qdrant.
    * **`force_recreate=True`** : Pour ce tutoriel, nous utilisons ce param√®tre pour nous assurer que la collection est vide √† chaque ex√©cution. En production, vous le mettriez √† `False` pour conserver vos donn√©es.
3. **Le reste du pipeline** (configuration du LLM, cr√©ation de la cha√Æne `RetrievalQA`) est identique, ce qui d√©montre la flexibilit√© de LangChain : il suffit de changer la source du `retriever` (le chercheur d'informations) pour passer de FAISS √† Qdrant.

:::info Pr√©requis : Lancer Qdrant
Pour ce tutoriel, vous aurez besoin d'une instance Qdrant. Vous pouvez la lancer facilement avec Docker :

```bash
# 1. T√©l√©charger la derni√®re image de Qdrant
docker pull qdrant/qdrant

# 2. D√©marrer le conteneur Qdrant
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
```
:::

Le code ci-dessous montre comment adapter le pipeline RAG pour utiliser Qdrant comme base de donn√©es vectorielle.

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Qdrant
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List
from langchain_core.embeddings import Embeddings

# (La classe LLMaaSEmbeddings est la m√™me que dans l'exemple pr√©c√©dent,
# nous la r√©utilisons ici. Assurez-vous qu'elle est d√©finie dans votre script.)

# --- Configuration ---
load_dotenv()
API_KEY = os.getenv("LLMAAS_API_KEY")
BASE_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
EMBEDDING_MODEL = "granite-embedding:278m"
LLM_MODEL = "granite3.3:8b"
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_COLLECTION_NAME = "tutorial_collection"

# --- Classe d'Embedding (r√©utilis√©e de l'exemple pr√©c√©dent) ---
class LLMaaSEmbeddings(Embeddings):
    def __init__(self, api_key: str, model_name: str):
        if not api_key: raise ValueError("API Key is required.")
        self.api_key, self.model_name, self.base_url = api_key, model_name, BASE_URL
        self.headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
    def _embed(self, texts: List[str]) -> List[List[float]]:
        import httpx
        payload = {"input": texts, "model": self.model_name}
        with httpx.Client(timeout=60.0) as client:
            r = client.post(f"{self.base_url}/embeddings", headers=self.headers, json=payload)
            r.raise_for_status()
            data = r.json()['data']
            data.sort(key=lambda e: e['index'])
            return [item['embedding'] for item in data]
    def embed_documents(self, texts: List[str]) -> List[List[float]]: return self._embed(texts)
    def embed_query(self, text: str) -> List[float]: return self._embed([text])[0]

def setup_qdrant_rag_pipeline():
    """Configure et retourne un pipeline RAG utilisant Qdrant."""
    print("1. Initialisation du client d'embedding LLMaaS...")
    embeddings = LLMaaSEmbeddings(api_key=API_KEY, model_name=EMBEDDING_MODEL)

    print("2. Pr√©paration des documents...")
    documents_content = [
        "Cloud Temple est un fournisseur de cloud souverain fran√ßais avec la qualification SecNumCloud.",
        "Les tarifs LLMaaS sont de 0.9‚Ç¨ pour l'input et 4‚Ç¨ pour l'output par million de tokens."
    ]
    documents = [Document(page_content=d) for d in documents_content]
    
    print(f"3. Connexion √† Qdrant et peuplement de la collection '{QDRANT_COLLECTION_NAME}'...")
    vectorstore = Qdrant.from_documents(
        documents,
        embeddings,
        url=QDRANT_URL,
        collection_name=QDRANT_COLLECTION_NAME,
        force_recreate=True, # Assure une collection propre pour le tutoriel
    )
    print("   -> Collection cr√©√©e et peupl√©e avec succ√®s.")

    print(f"4. Configuration du LLM ({LLM_MODEL})...")
    llm = ChatOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=LLM_MODEL,
        temperature=0.3
    )

    print("5. Cr√©ation de la cha√Æne RAG...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    print("   -> Pipeline RAG avec Qdrant pr√™t.")
    return qa_chain

# --- Ex√©cution ---
def main_qdrant():
    """Fonction principale pour ex√©cuter le pipeline RAG avec Qdrant."""
    if not API_KEY:
        print("Erreur: La variable d'environnement LLMAAS_API_KEY n'est pas d√©finie.")
        return
        
    try:
        rag_chain = setup_qdrant_rag_pipeline()
        question = "Quels sont les tarifs de l'API LLMaaS de Cloud Temple ?"
        
        print(f"\n--- Interrogation du pipeline ---")
        result = rag_chain({"query": question})

        print(f"\nQuestion: {question}")
        print(f"R√©ponse: {result['result']}")
        print("\nSources utilis√©es pour la r√©ponse:")
        for source in result["source_documents"]:
            print(f"- Contenu: \"{source.page_content}\"")
            
    except Exception as e:
        print(f"\nUne erreur est survenue: {e}")
        print("Veuillez vous assurer que le conteneur Qdrant est bien en cours d'ex√©cution.")

if __name__ == "__main__":
    main_qdrant()
```

### 4. Agents LangChain avec Outils

Un agent est un LLM qui ne se contente pas de r√©pondre √† une question, mais qui peut utiliser un ensemble d'**outils** (fonctions, API, etc.) pour construire une r√©ponse plus complexe. Il peut raisonner, d√©composer un probl√®me, choisir un outil, l'ex√©cuter, observer le r√©sultat, et r√©p√©ter ce cycle jusqu'√† obtenir une r√©ponse finale.

#### Le code expliqu√©

Cet exemple construit un agent simple capable d'utiliser deux outils : un pour interroger une API (simul√©e) de Cloud Temple et un autre pour faire des calculs.

1. **D√©finition des Outils** : Les classes `CloudTempleAPITool` et `CalculatorTool` h√©ritent de `BaseTool`. Chaque outil a :
    * Un `name` : un nom simple et descriptif.
    * Une `description` : **cruciale**, c'est ce que le LLM lit pour d√©cider quel outil utiliser. Elle doit √™tre tr√®s claire sur ce que fait l'outil et quand l'utiliser.
    * Une m√©thode `_run` : le code qui est r√©ellement ex√©cut√© lorsque l'agent choisit cet outil.
2. **`create_agent_with_tools`** :
    * **Initialisation du LLM** : Nous utilisons notre wrapper `CloudTempleLLM` d√©fini dans le premier tutoriel.
    * **Liste des outils** : Nous fournissons √† l'agent la liste des outils qu'il a le droit d'utiliser.
    * **Prompt de l'agent** : Le prompt est tr√®s sp√©cifique. Il s'agit d'un "prompt de raisonnement" qui instruit le LLM sur la mani√®re de penser (`Thought`), de choisir une action (`Action`), de fournir une entr√©e √† cette action (`Action Input`), et d'observer le r√©sultat (`Observation`). C'est le m√©canisme central du framework ReAct (Reasoning and Acting) utilis√© ici.
    * **Cr√©ation de l'agent** : `create_react_agent` assemble le LLM, les outils et le prompt pour cr√©er l'agent.
    * **`AgentExecutor`** : C'est le moteur qui fait tourner l'agent en boucle jusqu'√† ce qu'il produise une `Final Answer`. Le param√®tre `verbose=True` est tr√®s utile pour voir le "dialogue int√©rieur" de l'agent (ses pens√©es, ses actions, etc.).

```python
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain.tools import BaseTool
from langchain.prompts import PromptTemplate
import requests
import json
import os

# (La classe CloudTempleLLM est la m√™me que dans le premier exemple)

# --- D√©finition des Outils ---

class CloudTempleAPITool(BaseTool):
    """Un outil qui simule un appel √† une API interne pour obtenir des informations sur les services."""
    name = "cloud_temple_api_checker"
    description = "Utile pour obtenir des informations sur les services, produits et offres de Cloud Temple."

    def _run(self, query: str) -> str:
        # Dans un cas r√©el, ceci appellerait une v√©ritable API.
        print(f"--- Outil CloudTempleAPITool appel√© avec la requ√™te: '{query}' ---")
        if "service" in query.lower():
            return "Cloud Temple propose les services suivants : IaaS, PaaS, LLMaaS, S√©curit√© Manag√©e."
        return "Information non trouv√©e."

    async def _arun(self, query: str) -> str:
        # Impl√©mentation asynchrone non n√©cessaire pour cet exemple.
        raise NotImplementedError("L'outil API ne supporte pas l'ex√©cution asynchrone.")

class SimpleCalculatorTool(BaseTool):
    """Un outil simple pour effectuer des calculs math√©matiques."""
    name = "simple_calculator"
    description = "Utile pour effectuer des calculs math√©matiques simples. Prend une expression valide en Python."

    def _run(self, expression: str) -> str:
        print(f"--- Outil SimpleCalculatorTool appel√© avec l'expression: '{expression}' ---")
        try:
            # ATTENTION: eval() est dangereux en production. C'est uniquement pour la d√©mo.
            return str(eval(expression))
        except Exception as e:
            return f"Erreur de calcul: {e}"

    async def _arun(self, expression: str) -> str:
        raise NotImplementedError("L'outil Calculatrice ne supporte pas l'ex√©cution asynchrone.")

# --- Cr√©ation de l'Agent ---

def create_agent():
    """Configure et retourne un agent LangChain avec les outils d√©finis."""
    print("1. Initialisation du LLM pour l'agent...")
    llm = CloudTempleLLM(api_key=os.getenv("LLMAAS_API_KEY", "votre-cl√©-api-ici"))

    tools = [CloudTempleAPITool(), SimpleCalculatorTool()]
    
    # Le template de prompt est crucial : il guide le LLM dans son raisonnement.
    template = """
    R√©ponds aux questions suivantes du mieux que tu peux. Tu as acc√®s aux outils suivants :

    {tools}

    Utilise le format suivant :

    Question: la question √† laquelle tu dois r√©pondre
    Thought: tu dois toujours r√©fl√©chir √† ce que tu vas faire
    Action: l'action √† prendre, doit √™tre l'un de [{tool_names}]
    Action Input: l'entr√©e de l'action
    Observation: le r√©sultat de l'action
    ... (cette s√©quence Thought/Action/Action Input/Observation peut se r√©p√©ter)
    Thought: Je connais maintenant la r√©ponse finale.
    Final Answer: la r√©ponse finale √† la question d'origine

    Commence !

    Question: {input}
    Thought:{agent_scratchpad}
    """
    
    prompt = PromptTemplate.from_template(template)
    
    print("2. Cr√©ation de l'agent avec le framework ReAct...")
    agent = create_react_agent(llm, tools, prompt)

    # L'AgentExecutor est responsable de l'ex√©cution des cycles de l'agent.
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    print("   -> Agent pr√™t.")
    return agent_executor

# --- Ex√©cution ---

def run_agent():
    """Ex√©cute l'agent avec diff√©rentes questions pour tester ses capacit√©s."""
    if os.getenv("LLMAAS_API_KEY") is None:
        print("Veuillez configurer votre LLMAAS_API_KEY.")
        return
        
    agent_executor = create_agent()
    
    print("\n--- Test 1 : Question n√©cessitant un outil d'information ---")
    question1 = "Quels sont les services offerts par Cloud Temple ?"
    response1 = agent_executor.invoke({"input": question1})
    print(f"\nR√©ponse finale de l'agent: {response1['output']}")
    
    print("\n--- Test 2 : Question n√©cessitant un calcul ---")
    question2 = "Quel est le r√©sultat de 125 * 8 + 50 ?"
    response2 = agent_executor.invoke({"input": question2})
    print(f"\nR√©ponse finale de l'agent: {response2['output']}")

if __name__ == "__main__":
    run_agent()
```

### 5. Int√©gration OpenAI SDK

**Migration transparente depuis OpenAI**

```python
from openai import OpenAI

# Configuration pour Cloud Temple LLMaaS
def setup_cloud_temple_client():
    """Configuration client OpenAI pour Cloud Temple"""
    
    client = OpenAI(
        api_key="your-cloud-temple-api-key",
        base_url="https://api.ai.cloud-temple.com/v1"
    )
    
    return client

def test_openai_compatibility():
    """Test de compatibilit√© avec SDK OpenAI"""
    
    client = setup_cloud_temple_client()
    
    # Chat completion standard
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[
            {"role": "system", "content": "Tu es un assistant IA professionnel."},
            {"role": "user", "content": "Explique-moi l'architecture cloud native."}
        ],
        max_tokens=300,
        temperature=0.7
    )
    
    print(f"R√©ponse: {response.choices[0].message.content}")
    
    # Streaming
    stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[
            {"role": "user", "content": "√âcris un po√®me sur l'IA."}
        ],
        stream=True,
        max_tokens=200
    )
    
    print("Stream:")
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
    print()

# Test de compatibilit√©
test_openai_compatibility()
```

### 5. Int√©gration Semantic Kernel (Microsoft)

[Semantic Kernel](https://learn.microsoft.com/fr-fr/semantic-kernel/overview/) est un SDK open-source de Microsoft qui permet d'int√©grer des LLMs dans des applications .NET, Python, et Java. Bien qu'il soit optimis√© pour les services Azure OpenAI, sa flexibilit√© permet de l'utiliser avec n'importe quelle API compatible OpenAI, y compris la n√¥tre.

#### Le code expliqu√©

Cet exemple ne n√©cessite pas le SDK Semantic Kernel complet. Il d√©montre comment le **concept de "fonction s√©mantique"** peut √™tre impl√©ment√© par un simple appel √† notre API. Une fonction s√©mantique est essentiellement un prompt structur√© envoy√© √† un LLM pour accomplir une t√¢che sp√©cifique.

1. **`semantic_kernel_simple()`** : Cette fonction simule une "fonction de r√©sum√©".
2. **Prompt Structur√©** : Nous utilisons un message `system` pour donner un r√¥le au LLM ("Tu es un expert en r√©sum√©.") et un message `user` contenant le texte √† r√©sumer. C'est le c≈ìur du concept de fonction s√©mantique.
3. **Appel API Direct** : Un simple appel `requests.post` √† notre endpoint `/v1/chat/completions` suffit pour ex√©cuter la fonction.

Cet exemple illustre qu'il n'est pas toujours n√©cessaire d'utiliser un framework lourd. Pour des t√¢ches simples et bien d√©finies, un appel direct √† l'API LLMaaS est souvent la solution la plus efficace et la plus performante.

```python
import requests
import os
from dotenv import load_dotenv

def semantic_kernel_simulation():
    """
    Simule une "fonction s√©mantique" de r√©sum√© en appelant directement l'API LLMaaS.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Veuillez d√©finir la variable d'environnement LLMAAS_API_KEY.")
        return

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    text_to_summarize = """
    L'intelligence artificielle (IA) transforme de nombreux secteurs industriels en automatisant les t√¢ches, 
    en optimisant les processus et en permettant des analyses pr√©dictives avanc√©es. 
    Cloud Temple, avec son offre LLMaaS souveraine et certifi√©e SecNumCloud, permet aux entreprises 
    d'int√©grer ces capacit√©s d'IA tout en garantissant la s√©curit√© et la confidentialit√© de leurs donn√©es.
    """
    
    # Le prompt combine une instruction (r√¥le syst√®me) et des donn√©es (r√¥le utilisateur)
    payload = {
        "model": "granite3.3:8b",
        "messages": [
            {"role": "system", "content": "Tu es un assistant expert en synth√®se de documents techniques."},
            {"role": "user", "content": f"R√©sume le texte suivant en une seule phrase concise: {text_to_summarize}"}
        ],
        "max_tokens": 100,
        "temperature": 0.5
    }
    
    try:
        response = requests.post(
            "https://api.ai.cloud-temple.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        summary = result['choices'][0]['message']['content']
        
        print("Texte original:\n", text_to_summarize)
        print("\nR√©sum√© g√©n√©r√©:\n", summary)
        return summary
        
    except requests.exceptions.RequestException as e:
        print(f"Une erreur d'API est survenue: {e}")

if __name__ == "__main__":
    semantic_kernel_simulation()
```

### 6. Framework Haystack

[Haystack](https://haystack.deepset.ai/) est un autre framework open-source puissant pour construire des applications de recherche s√©mantique, de RAG et d'agents. Comme pour Semantic Kernel, notre API peut √™tre int√©gr√©e directement.

#### Le code expliqu√©

Cet exemple simule un "pipeline" Haystack de base pour la recherche de r√©ponses dans un contexte donn√© (Question Answering).

1. **`process_with_context`** : Cette fonction repr√©sente le c≈ìur d'un pipeline de QA. Elle prend un `contexte` (par exemple, un paragraphe de document) et une `question`.
2. **Prompt Contextuel** : Le prompt est soigneusement structur√© pour inclure √† la fois le contexte et la question. C'est une technique fondamentale en RAG : on fournit au LLM les informations pertinentes pour qu'il puisse formuler une r√©ponse factuelle.
3. **Appel API** : Encore une fois, un simple appel `requests.post` √† notre API suffit. Le LLM re√ßoit le contexte et la question, et sa t√¢che est de synth√©tiser une r√©ponse bas√©e *uniquement* sur les informations fournies.

Cet exemple illustre la flexibilit√© de l'API LLMaaS, qui peut servir de brique de base pour la g√©n√©ration de texte dans n'importe quel framework, m√™me ceux pour lesquels il n'existe pas d'int√©gration officielle.

```python
import requests
import os
from dotenv import load_dotenv

def haystack_simulation():
    """
    Simule un pipeline de Question-Answering de type Haystack
    en utilisant un appel direct √† l'API LLMaaS.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Veuillez d√©finir la variable d'environnement LLMAAS_API_KEY.")
        return

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # Le contexte est l'information que le LLM est autoris√© √† utiliser.
    context = """
    Un cloud souverain est une infrastructure de cloud computing qui est enti√®rement contenue 
    dans les fronti√®res d'un pays sp√©cifique et soumise √† ses lois. 
    Les principaux avantages sont la garantie de la r√©sidence des donn√©es, la conformit√© avec 
    les r√©glementations locales (comme le RGPD en Europe), et une protection accrue contre 
    l'acc√®s par des entit√©s √©trang√®res en vertu de lois extraterritoriales comme le CLOUD Act am√©ricain.
    """
    
    question = "Quels sont les avantages d'un cloud souverain ?"
    
    # Le prompt guide le LLM pour qu'il base sa r√©ponse sur le contexte fourni.
    prompt = f"""
    En te basant uniquement sur le contexte suivant, r√©ponds √† la question.
    
    Contexte:
    ---
    {context}
    ---
    
    Question: {question}
    """
    
    payload = {
        "model": "granite3.3:8b",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 200,
        "temperature": 0.2 # Temp√©rature basse pour une r√©ponse factuelle
    }
    
    try:
        response = requests.post(
            "https://api.ai.cloud-temple.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        answer = result['choices'][0]['message']['content']
        
        print(f"Question: {question}")
        print("\nR√©ponse g√©n√©r√©e:\n", answer)
        return answer
        
    except requests.exceptions.RequestException as e:
        print(f"Une erreur d'API est survenue: {e}")

if __name__ == "__main__":
    haystack_simulation()
```

### 7. Int√©gration LlamaIndex

[LlamaIndex](https://www.llamaindex.ai/) est un framework sp√©cialis√© dans la construction d'applications RAG. Il offre des composants de haut niveau pour l'ingestion de donn√©es, l'indexation et l'interrogation. Notre API, √©tant compatible avec l'interface OpenAI, s'int√®gre tr√®s facilement.

#### Le code expliqu√©

Cet exemple montre comment configurer LlamaIndex pour utiliser l'API LLMaaS pour la g√©n√©ration de texte, tout en utilisant un mod√®le d'embedding local pour la vectorisation.

1. **`setup_and_run_llamaindex`** : Cette fonction unique orchestre l'ensemble du processus.
    * **Configuration du LLM** : LlamaIndex fournit une classe `OpenAILike` qui permet de se connecter √† n'importe quelle API respectant le format OpenAI. Il suffit de lui fournir notre `api_base` et une `api_key`. C'est la m√©thode la plus simple pour rendre notre LLM compatible.
    * **Configuration des Embeddings** : Pour cet exemple, nous utilisons un mod√®le d'embedding local (`HuggingFaceEmbedding`). Cela montre la flexibilit√© de LlamaIndex, qui permet de mixer les composants. Vous pourriez tout aussi bien utiliser la classe `LLMaaSEmbeddings` des exemples pr√©c√©dents pour utiliser notre API d'embedding.
    * **`Settings`** : L'objet `Settings` de LlamaIndex est un moyen pratique de configurer les composants par d√©faut (LLM, mod√®le d'embedding, taille des chunks, etc.) qui seront utilis√©s par les autres objets LlamaIndex.
    * **Ingestion des donn√©es** : `SimpleDirectoryReader` charge les documents d'un dossier.
    * **Cr√©ation de l'index** : `VectorStoreIndex.from_documents` est la m√©thode de haut niveau de LlamaIndex. Elle g√®re automatiquement le d√©coupage en chunks, la vectorisation des chunks (en utilisant le `embed_model` configur√© dans `Settings`), et la cr√©ation de l'index en m√©moire.
    * **Moteur de requ√™te** : `.as_query_engine()` cr√©e une interface simple pour poser des questions √† notre index. Lorsque vous appelez `.query()`, le moteur vectorise votre question, trouve les documents les plus pertinents dans l'index, et les envoie au LLM (configur√© dans `Settings`) avec la question pour g√©n√©rer une r√©ponse.

```python
# D√©pendances:
# pip install llama-index llama-index-llms-openai-like llama-index-embeddings-huggingface

import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import shutil

def setup_and_run_llamaindex():
    """
    Configure et ex√©cute un pipeline RAG simple avec LlamaIndex et l'API LLMaaS.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Veuillez d√©finir la variable d'environnement LLMAAS_API_KEY.")
        return

    # 1. Configuration du LLM pour utiliser l'API LLMaaS via l'interface OpenAILike
    print("1. Configuration du LLM pour pointer vers l'API LLMaaS...")
    llm = OpenAILike(
        api_key=api_key,
        api_base="https://api.ai.cloud-temple.com/v1",
        model="granite3.3:8b",
        is_chat_model=True,
        # Il est parfois n√©cessaire d'ajouter des param√®tres de contexte pour certains mod√®les
        # context_window=3900, 
    )

    # 2. Configuration du mod√®le d'embedding (local dans cet exemple pour la simplicit√©)
    print("2. Configuration du mod√®le d'embedding local...")
    embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # 3. Application des configurations globales via l'objet Settings de LlamaIndex
    Settings.llm = llm
    Settings.embed_model = embed_model
    print("   -> LLM et mod√®le d'embedding configur√©s.")

    # 4. Cr√©ation d'une base de connaissances simple dans un r√©pertoire temporaire
    print("4. Cr√©ation et chargement d'une base de connaissances temporaire...")
    temp_dir = "temp_llama_data"
    os.makedirs(temp_dir, exist_ok=True)
    knowledge_file = os.path.join(temp_dir, "knowledge.txt")
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write("L'offre LLMaaS de Cloud Temple est une solution d'IA g√©n√©rative souveraine, "
                "enti√®rement op√©r√©e en France et qualifi√©e SecNumCloud par l'ANSSI.")
    
    documents = SimpleDirectoryReader(temp_dir).load_data()
    print(f"   -> {len(documents)} document(s) charg√©(s).")

    # 5. Cr√©ation de l'index vectoriel. LlamaIndex g√®re le chunking et l'embedding.
    print("5. Cr√©ation de l'index vectoriel...")
    index = VectorStoreIndex.from_documents(documents)
    print("   -> Index cr√©√©.")

    # 6. Cr√©ation du moteur de requ√™te et interrogation de la base de connaissances
    print("6. Cr√©ation du moteur de requ√™te et interrogation...")
    query_engine = index.as_query_engine()
    question = "Quelles sont les garanties de souverainet√© de l'offre LLMaaS ?"
    response = query_engine.query(question)
    
    print(f"\nQuestion: {question}")
    print(f"R√©ponse: {response}")

    # Nettoyage du r√©pertoire temporaire
    shutil.rmtree(temp_dir)
    print(f"\nR√©pertoire temporaire '{temp_dir}' supprim√©.")

if __name__ == "__main__":
    setup_and_run_llamaindex()
```

### 8. Configuration de l'extension CLINE pour VSCode

Ce tutoriel vous guide pour configurer l'extension CLINE dans Visual Studio Code afin d'utiliser les mod√®les de langage de Cloud Temple directement depuis votre √©diteur.

#### √âtapes de configuration

1. **Ouvrir les param√®tres de CLINE** : Dans VSCode, ouvrez les param√®tres de l'extension CLINE.
2. **Cr√©er un nouveau mod√®le** : Ajoutez une nouvelle configuration de mod√®le.
3. **Remplir les champs** : Configurez les champs comme suit, en vous basant sur l'image ci-dessous.

    ![Configuration de CLINE pour LLMaaS](./images/cline_configuration.png)

    * **API Provider**: S√©lectionnez `OpenAI Compatible`.
    * **Base URL**: Entrez l'endpoint de l'API LLMaaS de Cloud Temple : `https://api.ai.cloud-temple.com/v1`.
    * **OpenAI Compatible API Key**: Collez la cl√© d'API que vous avez g√©n√©r√©e depuis la console Cloud Temple.
    
    :::tip G√©n√©ration de la cl√© API
    Pour g√©n√©rer votre cl√© API, rendez-vous dans la console Cloud Temple, section **LLMaaS** > **Cl√©s API**, puis cliquez sur **"Cr√©er une cl√© API"**.
    
    ![Cr√©ation d'une cl√© API depuis la console](./images/console_create_api_key.png)
    :::
    
    * **Model ID**: Sp√©cifiez le mod√®le que vous souhaitez utiliser, par exemple `qwen3-coder:30b`. Vous pouvez trouver la liste des mod√®les disponibles dans la section [Mod√®les](./models.md).
    * **Model Configuration**:
        * **Supports Images**: Cochez cette case si le mod√®le supporte les images.
        * **Supports browser use**: Cochez cette case.
        * **Context Window Size**: Indiquez la taille de la fen√™tre de contexte du mod√®le (ex: `128000`).
        * **Max Output Tokens**: Laissez √† `-1` pour une sortie non limit√©e par d√©faut.
        * **Temperature**: R√©glez la temp√©rature selon vos besoins (ex: `0`).

Vous pouvez maintenant s√©lectionner un mod√®le dans CLINE et l'utiliser pour g√©n√©rer du code, r√©pondre √† des questions, etc.

---

## üí° Exemples Avanc√©s

Vous trouverez dans le r√©pertoire GitHub ci-dessous une collection d'exemples de code et de scripts d√©montrant les diff√©rentes fonctionnalit√©s et cas d'utilisation de l'offre LLM as a Service (LLMaaS) de Cloud Temple :

[Cloud-Temple/product-llmaas-how-to](https://github.com/Cloud-Temple/product-llmaas-how-to/tree/main)

Vous y trouverez des guides pratiques pour :
- __Extraction d'Informations et Analyse de Texte :__ Capacit√© √† analyser des documents pour en extraire des donn√©es structur√©es telles que des entit√©s, des √©v√©nements, des relations et des attributs, en s'appuyant sur des ontologies sp√©cifiques √† des domaines (ex: juridique, RH, IT).

- __Interaction Conversationnelle et Chatbots :__ D√©veloppement d'agents conversationnels capables de dialoguer, de maintenir un historique d'√©change, d'utiliser des instructions syst√®me (prompts syst√®me) et d'invoquer des outils externes.

- __Transcription Audio (Speech-to-Text) :__ Conversion de contenu audio en texte, y compris pour des fichiers volumineux, gr√¢ce √† des techniques de d√©coupage, de normalisation et de traitement par lots.

- __Traduction de Texte :__ Traduction de documents d'une langue √† une autre, en g√©rant le contexte sur plusieurs segments pour am√©liorer la coh√©rence.

- __Gestion et √âvaluation des Mod√®les :__ Listage des mod√®les de langage disponibles via l'API, consultation de leurs sp√©cifications et ex√©cution de tests pour comparer leurs performances.

- __Streaming de R√©ponses en Temps R√©el :__ D√©monstration de la capacit√© √† recevoir et afficher les r√©ponses des mod√®les de mani√®re progressive (token par token), essentielle pour les applications interactives.
- __Pipeline RAG avec Base de Connaissances en M√©moire :__ D√©monstrateur RAG p√©dagogique pour illustrer le fonctionnement du Retrieval-Augmented Generation. Utilise l'API LLMaaS pour l'embedding et la g√©n√©ration, avec stockage des vecteurs en m√©moire (FAISS) pour une compr√©hension claire du processus.
- __Pipeline RAG avec Base de Donn√©es Vectorielle (Qdrant) :__ D√©monstrateur RAG complet et conteneuris√© utilisant Qdrant comme base de donn√©es vectorielle. L'API LLMaaS est utilis√©e pour l'embedding des documents et la g√©n√©ration de r√©ponses augment√©es.
- __OCR & Analyse de Documents (DeepSeek-OCR) :__ Guide complet et outil de d√©monstration pour convertir des images et PDF en Markdown structur√©, extraire des tableaux et transcrire des formules math√©matiques. Voir la [documentation d√©di√©e](./ocr).
