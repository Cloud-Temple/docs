---
title: Tutoriels 
sidebar_position: 6
---

# Tutorials LLMaaS

## Vue d'overview

Ces tutorials avancÃ©s couvrent l'intÃ©gration, l'optimisation et les meilleures pratiques pour exploiter pleinement LLMaaS Cloud Temple en production. Chaque tutorial inclut du code testÃ© et des mÃ©triques de performance rÃ©elles.

## ðŸš€ IntÃ©grations LangChain et Frameworks

### 1. IntÃ©gration de base avec LangChain

Ce premier exemple montre comment intÃ©grer notre API LLMaaS avec le framework populaire LangChain en crÃ©ant un "wrapper" personnalisÃ©. Un wrapper est une classe qui "enveloppe" notre API pour la rendre compatible avec les mÃ©canismes internes de LangChain.

#### Le code expliquÃ©

Le code ci-dessous dÃ©finit une classe `CloudTempleLLM` qui hÃ©rite de la classe de base `LLM` de LangChain. Cela nous permet de dÃ©finir un comportement sur mesure tout en restant compatible avec l'Ã©cosystÃ¨me LangChain (chaÃ®nes, agents, etc.).

1.  **`CloudTempleLLM(LLM)`** : Notre classe hÃ©rite de `LLM`, ce qui nous oblige Ã  implÃ©menter certaines mÃ©thodes, notamment `_call`.
2.  **`_call(self, prompt: str, ...)`** : C'est le cÅ“ur de notre wrapper. Ã€ chaque fois que LangChain aura besoin de faire appel Ã  notre modÃ¨le de langage, il invoquera cette mÃ©thode. Ã€ l'intÃ©rieur, nous formatons une requÃªte HTTP POST standard avec les bons headers (`Authorization`) et le `payload` attendu par notre API `/v1/chat/completions`.
3.  **`exemple_langchain_basic()`** : Cette fonction de dÃ©monstration montre comment utiliser notre wrapper. On l'instancie, on crÃ©e un `PromptTemplate` pour structurer notre requÃªte, et on les combine dans une `LLMChain`. Lorsque l'on exÃ©cute la chaÃ®ne (`chain.run(...)`), LangChain appelle en coulisses la mÃ©thode `_call` que nous avons dÃ©finie.

Cette approche est utile si vous souhaitez un contrÃ´le total sur la maniÃ¨re dont LangChain interagit avec l'API, mais elle est plus verbeuse que l'utilisation du client `ChatOpenAI` (voir [API Reference](./api#langchain)).

```python
# Installation des dÃ©pendances
# pip install langchain requests pydantic

from langchain.llms.base import LLM
from langchain.schema import LLMResult, Generation
from typing import Optional, List, Any
from pydantic import Field
import requests
import json
import os

# --- Configuration ---
# Il est recommandÃ© de stocker votre clÃ© API dans une variable d'environnement
API_KEY = os.getenv("LLMAAS_API_KEY", "votre-clÃ©-api-ici")
BASE_URL = "https://api.ai.cloud-temple.com/v1"

class CloudTempleLLM(LLM):
    """
    Wrapper LangChain personnalisÃ© pour l'API LLMaaS de Cloud Temple.
    Cette classe permet d'utiliser notre API comme un LLM standard dans LangChain.
    """
    
    api_key: str = Field()
    model_name: str = Field(default="granite3.3:8b")
    temperature: float = Field(default=0.7)
    max_tokens: int = Field(default=1000)
    
    @property
    def _llm_type(self) -> str:
        """Identifiant unique pour notre type de LLM."""
        return "cloud_temple_llmaas"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """
        La mÃ©thode principale qui effectue l'appel Ã  l'API LLMaaS.
        LangChain utilise cette mÃ©thode pour chaque requÃªte au modÃ¨le.
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        if stop:
            payload["stop"] = stop
        
        # ExÃ©cution de la requÃªte POST vers l'API
        response = requests.post(
            f"{BASE_URL}/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        response.raise_for_status()  # LÃ¨ve une exception en cas d'erreur HTTP
        result = response.json()
        
        # Retourne le contenu du message de l'assistant
        return result['choices'][0]['message']['content']

# --- Exemple d'utilisation ---
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

def exemple_langchain_wrapper():
    """DÃ©montre l'utilisation du wrapper LLM avec une chaÃ®ne LangChain."""
    
    # 1. Initialisation de notre LLM personnalisÃ©
    llm = CloudTempleLLM(
        api_key=API_KEY,
        model_name="granite3.3:8b"
    )
    
    # 2. CrÃ©ation d'un template de prompt pour structurer les requÃªtes
    template = """
    Tu es un expert en {domaine}. 
    RÃ©ponds Ã  cette question de maniÃ¨re dÃ©taillÃ©e et professionnelle :
    
    Question: {question}
    
    RÃ©ponse:
    """
    prompt = PromptTemplate(
        input_variables=["domaine", "question"],
        template=template
    )
    
    # 3. CrÃ©ation d'une chaÃ®ne qui combine le prompt et le LLM
    chain = LLMChain(llm=llm, prompt=prompt)
    
    # 4. ExÃ©cution de la chaÃ®ne avec des variables spÃ©cifiques
    result = chain.run(
        domaine="cybersÃ©curitÃ©",
        question="Quelles sont les meilleures pratiques pour sÃ©curiser une API REST ?"
    )
    
    return result

# --- Lancement du test ---
if __name__ == "__main__":
    if API_KEY == "votre-clÃ©-api-ici":
        print("Veuillez configurer votre LLMAAS_API_KEY dans vos variables d'environnement.")
    else:
        reponse = exemple_langchain_wrapper()
        print("RÃ©ponse de l'expert en cybersÃ©curitÃ© :\n")
        print(reponse)
```

### 2. RAG (Retrieval-Augmented Generation) avec l'API LLMaaS

Le RAG est une technique puissante qui permet Ã  un LLM de rÃ©pondre Ã  des questions en s'appuyant sur une base de connaissances externe. Ce tutoriel vous guide Ã  travers la crÃ©ation d'un pipeline RAG simple en utilisant notre API pour les embeddings et la gÃ©nÃ©ration, et FAISS, une bibliothÃ¨que de similaritÃ© vectorielle, pour crÃ©er un index en mÃ©moire.

#### Le code expliquÃ©

Le pipeline se dÃ©compose en plusieurs Ã©tapes logiques :

1.  **Configuration** : Nous importons les bibliothÃ¨ques nÃ©cessaires et chargeons notre clÃ© API depuis les variables d'environnement. Nous dÃ©finissons les modÃ¨les Ã  utiliser : `granite-embedding:278m` pour la vectorisation et `granite3.3:8b` pour la gÃ©nÃ©ration.
2.  **`LLMaaSEmbeddings`** : Comme dans l'exemple prÃ©cÃ©dent, nous avons besoin d'un wrapper pour interagir avec notre API d'embeddings. Cette classe se charge de transformer les morceaux de texte (chunks) en vecteurs numÃ©riques (embeddings).
3.  **`setup_rag_pipeline`** : Cette fonction orchestre la crÃ©ation du pipeline.
    *   **Chargement des documents** : `DirectoryLoader` charge les fichiers texte de notre base de connaissances.
    *   **Division en chunks** : `RecursiveCharacterTextSplitter` dÃ©coupe les documents en plus petits morceaux. C'est essentiel pour que le modÃ¨le d'embedding puisse traiter efficacement le texte et pour que la recherche de similaritÃ© soit prÃ©cise.
    *   **Vectorisation et Indexation** : `FAISS.from_documents` est une Ã©tape clÃ©. Elle prend les chunks de texte, utilise notre classe `LLMaaSEmbeddings` pour appeler l'API et obtenir les vecteurs correspondants, puis stocke ces vecteurs dans un index FAISS en mÃ©moire.
    *   **Configuration du LLM** : Nous utilisons `ChatOpenAI` qui est nativement compatible avec notre API pour la partie gÃ©nÃ©ration de rÃ©ponse.
    *   **CrÃ©ation de la chaÃ®ne `RetrievalQA`** : C'est la chaÃ®ne LangChain qui lie tous les Ã©lÃ©ments. Quand on lui pose une question, elle :
        a. Utilise le `retriever` (basÃ© sur notre index FAISS) pour trouver les chunks de texte les plus pertinents.
        b. "Stuff" (fourre) ces chunks dans un prompt avec la question.
        c. Envoie ce prompt enrichi au LLM pour gÃ©nÃ©rer une rÃ©ponse contextuelle.
4.  **ExÃ©cution** : La fonction `main` simule une utilisation rÃ©elle en crÃ©ant des fichiers de connaissance temporaires, en construisant le pipeline et en posant une question.

```python
import os
import tempfile
import shutil
from pathlib import Path
from dotenv import load_dotenv
from typing import List

# --- Imports LangChain ---
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# --- Configuration ---
# Charge les variables d'environnement (ex: LLMAAS_API_KEY)
load_dotenv()
API_KEY = os.getenv("LLMAAS_API_KEY")
BASE_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
EMBEDDING_MODEL = "granite-embedding:278m"
LLM_MODEL = "granite3.3:8b"

# --- Classe d'Embedding PersonnalisÃ©e ---
class LLMaaSEmbeddings(Embeddings):
    """Classe d'embedding personnalisÃ©e pour l'API LLMaaS de Cloud Temple."""
    def __init__(self, api_key: str, model_name: str):
        if not api_key:
            raise ValueError("La clÃ© API LLMaaS ne peut pas Ãªtre vide.")
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = BASE_URL
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

    def _embed(self, texts: List[str]) -> List[List[float]]:
        import httpx
        payload = {"input": texts, "model": self.model_name}
        try:
            with httpx.Client(timeout=60.0) as client:
                response = client.post(f"{self.base_url}/embeddings", headers=self.headers, json=payload)
                response.raise_for_status()
                data = response.json()['data']
                data.sort(key=lambda e: e['index'])
                return [item['embedding'] for item in data]
        except httpx.HTTPStatusError as e:
            print(f"Erreur HTTP lors de la gÃ©nÃ©ration des embeddings: {e.response.text}")
            raise
        except Exception as e:
            print(f"Une erreur inattendue est survenue lors de la gÃ©nÃ©ration des embeddings: {e}")
            raise

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embed(texts)

    def embed_query(self, text: str) -> List[float]:
        # La mÃ©thode _embed attend une liste, nous encapsulons donc le texte unique.
        return self._embed([text])[0]

# --- Pipeline RAG ---
def setup_rag_pipeline(documents_path: str):
    """Configuration complÃ¨te du pipeline RAG avec les outils LLMaaS."""
    print("1. Chargement et division des documents...")
    loader = DirectoryLoader(documents_path, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(documents)
    print(f"   -> {len(documents)} document(s) chargÃ©(s) et divisÃ©(s) en {len(splits)} chunks.")
    
    print(f"2. CrÃ©ation des embeddings via LLMaaS (modÃ¨le: {EMBEDDING_MODEL})...")
    embeddings = LLMaaSEmbeddings(api_key=API_KEY, model_name=EMBEDDING_MODEL)
    
    print("3. CrÃ©ation de l'index vectoriel en mÃ©moire (FAISS)...")
    vectorstore = FAISS.from_documents(splits, embeddings)
    print("   -> Index FAISS crÃ©Ã© avec succÃ¨s.")
    
    print(f"4. Configuration du LLM (modÃ¨le: {LLM_MODEL})...")
    # Correction pour la compatibilitÃ© Pydantic/LangChain
    from langchain_core.caches import BaseCache
    from langchain_core.callbacks.base import Callbacks
    ChatOpenAI.model_rebuild()
    
    llm = ChatOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=LLM_MODEL,
        temperature=0.3,
        model_kwargs={"max_tokens": 300}
    )
    
    print("5. CrÃ©ation de la chaÃ®ne de Question/RÃ©ponse (RAG)...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    print("   -> Pipeline RAG prÃªt.")
    return qa_chain

# --- ExÃ©cution ---
def main():
    """Fonction principale pour exÃ©cuter le pipeline RAG de bout en bout."""
    if not API_KEY:
        print("Erreur: La variable d'environnement LLMAAS_API_KEY n'est pas dÃ©finie.")
        return

    # CrÃ©er des documents de test temporaires
    temp_dir = tempfile.mkdtemp()
    print(f"\nCrÃ©ation de documents de test dans: {temp_dir}")
    try:
        documents_content = {
            "overview.txt": "Cloud Temple est un fournisseur de cloud souverain franÃ§ais qualifiÃ© SecNumCloud.",
            "pricing.txt": "Les tarifs de l'API LLMaaS sont de 0.9â‚¬/million de tokens en entrÃ©e et 4â‚¬/million en sortie."
        }
        for filename, content in documents_content.items():
            with open(Path(temp_dir) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # Configurer et exÃ©cuter le pipeline
        rag_chain = setup_rag_pipeline(temp_dir)
        
        print("\n--- Interrogation du Pipeline RAG ---")
        question = "Quel est le tarif des tokens en sortie sur l'API LLMaaS de Cloud Temple ?"
        result = rag_chain({"query": question})
        
        print(f"\nQuestion: {question}")
        print(f"RÃ©ponse: {result['result']}")
        print("\nSources utilisÃ©es pour la rÃ©ponse:")
        for source in result["source_documents"]:
            print(f"- Fichier: {os.path.basename(source.metadata['source'])}")
            print(f"  Contenu: \"{source.page_content}\"")

    finally:
        # Nettoyer le rÃ©pertoire temporaire
        print(f"\nNettoyage du rÃ©pertoire temporaire: {temp_dir}")
        shutil.rmtree(temp_dir)

if __name__ == "__main__":
    main()
```

### 3. IntÃ©gration avec une base de donnÃ©es vectorielle (Qdrant)

Pour des applications RAG en production, l'utilisation d'une base de donnÃ©es vectorielle dÃ©diÃ©e comme **Qdrant** est recommandÃ©e. Contrairement Ã  FAISS qui fonctionne en mÃ©moire, Qdrant offre la persistance des donnÃ©es, des capacitÃ©s de recherche avancÃ©es et une meilleure scalabilitÃ©.

#### Le code expliquÃ©

Ce tutoriel adapte le pipeline RAG prÃ©cÃ©dent pour utiliser Qdrant.

1.  **PrÃ©requis** : La premiÃ¨re Ã©tape est de lancer une instance de Qdrant. Le moyen le plus simple est d'utiliser Docker.
2.  **`setup_qdrant_rag_pipeline`** :
    *   **Embeddings et Documents** : La crÃ©ation des embeddings et des documents reste identique Ã  l'exemple prÃ©cÃ©dent.
    *   **Connexion Ã  Qdrant** : Au lieu de crÃ©er un index FAISS, nous utilisons `Qdrant.from_documents`. Cette mÃ©thode LangChain gÃ¨re plusieurs Ã©tapes :
        a. Elle se connecte Ã  votre instance Qdrant via l'URL fournie.
        b. Elle crÃ©e une nouvelle "collection" (l'Ã©quivalent d'une table dans une base de donnÃ©es SQL) si elle n'existe pas.
        c. Elle appelle notre classe `LLMaaSEmbeddings` pour vectoriser les documents.
        d. Elle insÃ¨re les documents et leurs vecteurs dans la collection Qdrant.
    *   **`force_recreate=True`** : Pour ce tutoriel, nous utilisons ce paramÃ¨tre pour nous assurer que la collection est vide Ã  chaque exÃ©cution. En production, vous le mettriez Ã  `False` pour conserver vos donnÃ©es.
3.  **Le reste du pipeline** (configuration du LLM, crÃ©ation de la chaÃ®ne `RetrievalQA`) est identique, ce qui dÃ©montre la flexibilitÃ© de LangChain : il suffit de changer la source du `retriever` (le chercheur d'informations) pour passer de FAISS Ã  Qdrant.

:::info PrÃ©requis : Lancer Qdrant
Pour ce tutoriel, vous aurez besoin d'une instance Qdrant. Vous pouvez la lancer facilement avec Docker :

```bash
# 1. TÃ©lÃ©charger la derniÃ¨re image de Qdrant
docker pull qdrant/qdrant

# 2. DÃ©marrer le conteneur Qdrant
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
```
:::

Le code ci-dessous montre comment adapter le pipeline RAG pour utiliser Qdrant comme base de donnÃ©es vectorielle.

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Qdrant
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List
from langchain_core.embeddings import Embeddings

# (La classe LLMaaSEmbeddings est la mÃªme que dans l'exemple prÃ©cÃ©dent,
# nous la rÃ©utilisons ici. Assurez-vous qu'elle est dÃ©finie dans votre script.)

# --- Configuration ---
load_dotenv()
API_KEY = os.getenv("LLMAAS_API_KEY")
BASE_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
EMBEDDING_MODEL = "granite-embedding:278m"
LLM_MODEL = "granite3.3:8b"
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_COLLECTION_NAME = "tutorial_collection"

# --- Classe d'Embedding (rÃ©utilisÃ©e de l'exemple prÃ©cÃ©dent) ---
class LLMaaSEmbeddings(Embeddings):
    def __init__(self, api_key: str, model_name: str):
        if not api_key: raise ValueError("API Key is required.")
        self.api_key, self.model_name, self.base_url = api_key, model_name, BASE_URL
        self.headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
    def _embed(self, texts: List[str]) -> List[List[float]]:
        import httpx
        payload = {"input": texts, "model": self.model_name}
        with httpx.Client(timeout=60.0) as client:
            r = client.post(f"{self.base_url}/embeddings", headers=self.headers, json=payload)
            r.raise_for_status()
            data = r.json()['data']
            data.sort(key=lambda e: e['index'])
            return [item['embedding'] for item in data]
    def embed_documents(self, texts: List[str]) -> List[List[float]]: return self._embed(texts)
    def embed_query(self, text: str) -> List[float]: return self._embed([text])[0]

def setup_qdrant_rag_pipeline():
    """Configure et retourne un pipeline RAG utilisant Qdrant."""
    print("1. Initialisation du client d'embedding LLMaaS...")
    embeddings = LLMaaSEmbeddings(api_key=API_KEY, model_name=EMBEDDING_MODEL)

    print("2. PrÃ©paration des documents...")
    documents_content = [
        "Cloud Temple est un fournisseur de cloud souverain franÃ§ais avec la qualification SecNumCloud.",
        "Les tarifs LLMaaS sont de 0.9â‚¬ pour l'input et 4â‚¬ pour l'output par million de tokens."
    ]
    documents = [Document(page_content=d) for d in documents_content]
    
    print(f"3. Connexion Ã  Qdrant et peuplement de la collection '{QDRANT_COLLECTION_NAME}'...")
    vectorstore = Qdrant.from_documents(
        documents,
        embeddings,
        url=QDRANT_URL,
        collection_name=QDRANT_COLLECTION_NAME,
        force_recreate=True, # Assure une collection propre pour le tutoriel
    )
    print("   -> Collection crÃ©Ã©e et peuplÃ©e avec succÃ¨s.")

    print(f"4. Configuration du LLM ({LLM_MODEL})...")
    llm = ChatOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=LLM_MODEL,
        temperature=0.3
    )

    print("5. CrÃ©ation de la chaÃ®ne RAG...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    print("   -> Pipeline RAG avec Qdrant prÃªt.")
    return qa_chain

# --- ExÃ©cution ---
def main_qdrant():
    """Fonction principale pour exÃ©cuter le pipeline RAG avec Qdrant."""
    if not API_KEY:
        print("Erreur: La variable d'environnement LLMAAS_API_KEY n'est pas dÃ©finie.")
        return
        
    try:
        rag_chain = setup_qdrant_rag_pipeline()
        question = "Quels sont les tarifs de l'API LLMaaS de Cloud Temple ?"
        
        print(f"\n--- Interrogation du pipeline ---")
        result = rag_chain({"query": question})

        print(f"\nQuestion: {question}")
        print(f"RÃ©ponse: {result['result']}")
        print("\nSources utilisÃ©es pour la rÃ©ponse:")
        for source in result["source_documents"]:
            print(f"- Contenu: \"{source.page_content}\"")
            
    except Exception as e:
        print(f"\nUne erreur est survenue: {e}")
        print("Veuillez vous assurer que le conteneur Qdrant est bien en cours d'exÃ©cution.")

if __name__ == "__main__":
    main_qdrant()
```

### 4. Agents LangChain avec Outils

Un agent est un LLM qui ne se contente pas de rÃ©pondre Ã  une question, mais qui peut utiliser un ensemble d'**outils** (fonctions, API, etc.) pour construire une rÃ©ponse plus complexe. Il peut raisonner, dÃ©composer un problÃ¨me, choisir un outil, l'exÃ©cuter, observer le rÃ©sultat, et rÃ©pÃ©ter ce cycle jusqu'Ã  obtenir une rÃ©ponse finale.

#### Le code expliquÃ©

Cet exemple construit un agent simple capable d'utiliser deux outils : un pour interroger une API (simulÃ©e) de Cloud Temple et un autre pour faire des calculs.

1.  **DÃ©finition des Outils** : Les classes `CloudTempleAPITool` et `CalculatorTool` hÃ©ritent de `BaseTool`. Chaque outil a :
    *   Un `name` : un nom simple et descriptif.
    *   Une `description` : **cruciale**, c'est ce que le LLM lit pour dÃ©cider quel outil utiliser. Elle doit Ãªtre trÃ¨s claire sur ce que fait l'outil et quand l'utiliser.
    *   Une mÃ©thode `_run` : le code qui est rÃ©ellement exÃ©cutÃ© lorsque l'agent choisit cet outil.
2.  **`create_agent_with_tools`** :
    *   **Initialisation du LLM** : Nous utilisons notre wrapper `CloudTempleLLM` dÃ©fini dans le premier tutoriel.
    *   **Liste des outils** : Nous fournissons Ã  l'agent la liste des outils qu'il a le droit d'utiliser.
    *   **Prompt de l'agent** : Le prompt est trÃ¨s spÃ©cifique. Il s'agit d'un "prompt de raisonnement" qui instruit le LLM sur la maniÃ¨re de penser (`Thought`), de choisir une action (`Action`), de fournir une entrÃ©e Ã  cette action (`Action Input`), et d'observer le rÃ©sultat (`Observation`). C'est le mÃ©canisme central du framework ReAct (Reasoning and Acting) utilisÃ© ici.
    *   **CrÃ©ation de l'agent** : `create_react_agent` assemble le LLM, les outils et le prompt pour crÃ©er l'agent.
    *   **`AgentExecutor`** : C'est le moteur qui fait tourner l'agent en boucle jusqu'Ã  ce qu'il produise une `Final Answer`. Le paramÃ¨tre `verbose=True` est trÃ¨s utile pour voir le "dialogue intÃ©rieur" de l'agent (ses pensÃ©es, ses actions, etc.).

```python
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain.tools import BaseTool
from langchain.prompts import PromptTemplate
import requests
import json
import os

# (La classe CloudTempleLLM est la mÃªme que dans le premier exemple)

# --- DÃ©finition des Outils ---

class CloudTempleAPITool(BaseTool):
    """Un outil qui simule un appel Ã  une API interne pour obtenir des informations sur les services."""
    name = "cloud_temple_api_checker"
    description = "Utile pour obtenir des informations sur les services, produits et offres de Cloud Temple."

    def _run(self, query: str) -> str:
        # Dans un cas rÃ©el, ceci appellerait une vÃ©ritable API.
        print(f"--- Outil CloudTempleAPITool appelÃ© avec la requÃªte: '{query}' ---")
        if "service" in query.lower():
            return "Cloud Temple propose les services suivants : IaaS, PaaS, LLMaaS, SÃ©curitÃ© ManagÃ©e."
        return "Information non trouvÃ©e."

    async def _arun(self, query: str) -> str:
        # ImplÃ©mentation asynchrone non nÃ©cessaire pour cet exemple.
        raise NotImplementedError("L'outil API ne supporte pas l'exÃ©cution asynchrone.")

class SimpleCalculatorTool(BaseTool):
    """Un outil simple pour effectuer des calculs mathÃ©matiques."""
    name = "simple_calculator"
    description = "Utile pour effectuer des calculs mathÃ©matiques simples. Prend une expression valide en Python."

    def _run(self, expression: str) -> str:
        print(f"--- Outil SimpleCalculatorTool appelÃ© avec l'expression: '{expression}' ---")
        try:
            # ATTENTION: eval() est dangereux en production. C'est uniquement pour la dÃ©mo.
            return str(eval(expression))
        except Exception as e:
            return f"Erreur de calcul: {e}"

    async def _arun(self, expression: str) -> str:
        raise NotImplementedError("L'outil Calculatrice ne supporte pas l'exÃ©cution asynchrone.")

# --- CrÃ©ation de l'Agent ---

def create_agent():
    """Configure et retourne un agent LangChain avec les outils dÃ©finis."""
    print("1. Initialisation du LLM pour l'agent...")
    llm = CloudTempleLLM(api_key=os.getenv("LLMAAS_API_KEY", "votre-clÃ©-api-ici"))

    tools = [CloudTempleAPITool(), SimpleCalculatorTool()]
    
    # Le template de prompt est crucial : il guide le LLM dans son raisonnement.
    template = """
    RÃ©ponds aux questions suivantes du mieux que tu peux. Tu as accÃ¨s aux outils suivants :

    {tools}

    Utilise le format suivant :

    Question: la question Ã  laquelle tu dois rÃ©pondre
    Thought: tu dois toujours rÃ©flÃ©chir Ã  ce que tu vas faire
    Action: l'action Ã  prendre, doit Ãªtre l'un de [{tool_names}]
    Action Input: l'entrÃ©e de l'action
    Observation: le rÃ©sultat de l'action
    ... (cette sÃ©quence Thought/Action/Action Input/Observation peut se rÃ©pÃ©ter)
    Thought: Je connais maintenant la rÃ©ponse finale.
    Final Answer: la rÃ©ponse finale Ã  la question d'origine

    Commence !

    Question: {input}
    Thought:{agent_scratchpad}
    """
    
    prompt = PromptTemplate.from_template(template)
    
    print("2. CrÃ©ation de l'agent avec le framework ReAct...")
    agent = create_react_agent(llm, tools, prompt)

    # L'AgentExecutor est responsable de l'exÃ©cution des cycles de l'agent.
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    print("   -> Agent prÃªt.")
    return agent_executor

# --- ExÃ©cution ---

def run_agent():
    """ExÃ©cute l'agent avec diffÃ©rentes questions pour tester ses capacitÃ©s."""
    if os.getenv("LLMAAS_API_KEY") is None:
        print("Veuillez configurer votre LLMAAS_API_KEY.")
        return
        
    agent_executor = create_agent()
    
    print("\n--- Test 1 : Question nÃ©cessitant un outil d'information ---")
    question1 = "Quels sont les services offerts par Cloud Temple ?"
    response1 = agent_executor.invoke({"input": question1})
    print(f"\nRÃ©ponse finale de l'agent: {response1['output']}")
    
    print("\n--- Test 2 : Question nÃ©cessitant un calcul ---")
    question2 = "Quel est le rÃ©sultat de 125 * 8 + 50 ?"
    response2 = agent_executor.invoke({"input": question2})
    print(f"\nRÃ©ponse finale de l'agent: {response2['output']}")

if __name__ == "__main__":
    run_agent()
```

### 5. IntÃ©gration OpenAI SDK

**Migration transparente depuis OpenAI**

```python
from openai import OpenAI

# Configuration pour Cloud Temple LLMaaS
def setup_cloud_temple_client():
    """Configuration client OpenAI pour Cloud Temple"""
    
    client = OpenAI(
        api_key="your-cloud-temple-api-key",
        base_url="https://api.ai.cloud-temple.com/v1"
    )
    
    return client

def test_openai_compatibility():
    """Test de compatibilitÃ© avec SDK OpenAI"""
    
    client = setup_cloud_temple_client()
    
    # Chat completion standard
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[
            {"role": "system", "content": "Tu es un assistant IA professionnel."},
            {"role": "user", "content": "Explique-moi l'architecture cloud native."}
        ],
        max_tokens=300,
        temperature=0.7
    )
    
    print(f"RÃ©ponse: {response.choices[0].message.content}")
    
    # Streaming
    stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[
            {"role": "user", "content": "Ã‰cris un poÃ¨me sur l'IA."}
        ],
        stream=True,
        max_tokens=200
    )
    
    print("Stream:")
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
    print()

# Test de compatibilitÃ©
test_openai_compatibility()
```

### 5. IntÃ©gration Semantic Kernel (Microsoft)

[Semantic Kernel](https://learn.microsoft.com/fr-fr/semantic-kernel/overview/) est un SDK open-source de Microsoft qui permet d'intÃ©grer des LLMs dans des applications .NET, Python, et Java. Bien qu'il soit optimisÃ© pour les services Azure OpenAI, sa flexibilitÃ© permet de l'utiliser avec n'importe quelle API compatible OpenAI, y compris la nÃ´tre.

#### Le code expliquÃ©

Cet exemple ne nÃ©cessite pas le SDK Semantic Kernel complet. Il dÃ©montre comment le **concept de "fonction sÃ©mantique"** peut Ãªtre implÃ©mentÃ© par un simple appel Ã  notre API. Une fonction sÃ©mantique est essentiellement un prompt structurÃ© envoyÃ© Ã  un LLM pour accomplir une tÃ¢che spÃ©cifique.

1.  **`semantic_kernel_simple()`** : Cette fonction simule une "fonction de rÃ©sumÃ©".
2.  **Prompt StructurÃ©** : Nous utilisons un message `system` pour donner un rÃ´le au LLM ("Tu es un expert en rÃ©sumÃ©.") et un message `user` contenant le texte Ã  rÃ©sumer. C'est le cÅ“ur du concept de fonction sÃ©mantique.
3.  **Appel API Direct** : Un simple appel `requests.post` Ã  notre endpoint `/v1/chat/completions` suffit pour exÃ©cuter la fonction.

Cet exemple illustre qu'il n'est pas toujours nÃ©cessaire d'utiliser un framework lourd. Pour des tÃ¢ches simples et bien dÃ©finies, un appel direct Ã  l'API LLMaaS est souvent la solution la plus efficace et la plus performante.

```python
import requests
import os
from dotenv import load_dotenv

def semantic_kernel_simulation():
    """
    Simule une "fonction sÃ©mantique" de rÃ©sumÃ© en appelant directement l'API LLMaaS.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Veuillez dÃ©finir la variable d'environnement LLMAAS_API_KEY.")
        return

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    text_to_summarize = """
    L'intelligence artificielle (IA) transforme de nombreux secteurs industriels en automatisant les tÃ¢ches, 
    en optimisant les processus et en permettant des analyses prÃ©dictives avancÃ©es. 
    Cloud Temple, avec son offre LLMaaS souveraine et certifiÃ©e SecNumCloud, permet aux entreprises 
    d'intÃ©grer ces capacitÃ©s d'IA tout en garantissant la sÃ©curitÃ© et la confidentialitÃ© de leurs donnÃ©es.
    """
    
    # Le prompt combine une instruction (rÃ´le systÃ¨me) et des donnÃ©es (rÃ´le utilisateur)
    payload = {
        "model": "granite3.3:8b",
        "messages": [
            {"role": "system", "content": "Tu es un assistant expert en synthÃ¨se de documents techniques."},
            {"role": "user", "content": f"RÃ©sume le texte suivant en une seule phrase concise: {text_to_summarize}"}
        ],
        "max_tokens": 100,
        "temperature": 0.5
    }
    
    try:
        response = requests.post(
            "https://api.ai.cloud-temple.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        summary = result['choices'][0]['message']['content']
        
        print("Texte original:\n", text_to_summarize)
        print("\nRÃ©sumÃ© gÃ©nÃ©rÃ©:\n", summary)
        return summary
        
    except requests.exceptions.RequestException as e:
        print(f"Une erreur d'API est survenue: {e}")

if __name__ == "__main__":
    semantic_kernel_simulation()
```

### 6. Framework Haystack

[Haystack](https://haystack.deepset.ai/) est un autre framework open-source puissant pour construire des applications de recherche sÃ©mantique, de RAG et d'agents. Comme pour Semantic Kernel, notre API peut Ãªtre intÃ©grÃ©e directement.

#### Le code expliquÃ©

Cet exemple simule un "pipeline" Haystack de base pour la recherche de rÃ©ponses dans un contexte donnÃ© (Question Answering).

1.  **`process_with_context`** : Cette fonction reprÃ©sente le cÅ“ur d'un pipeline de QA. Elle prend un `contexte` (par exemple, un paragraphe de document) et une `question`.
2.  **Prompt Contextuel** : Le prompt est soigneusement structurÃ© pour inclure Ã  la fois le contexte et la question. C'est une technique fondamentale en RAG : on fournit au LLM les informations pertinentes pour qu'il puisse formuler une rÃ©ponse factuelle.
3.  **Appel API** : Encore une fois, un simple appel `requests.post` Ã  notre API suffit. Le LLM reÃ§oit le contexte et la question, et sa tÃ¢che est de synthÃ©tiser une rÃ©ponse basÃ©e *uniquement* sur les informations fournies.

Cet exemple illustre la flexibilitÃ© de l'API LLMaaS, qui peut servir de brique de base pour la gÃ©nÃ©ration de texte dans n'importe quel framework, mÃªme ceux pour lesquels il n'existe pas d'intÃ©gration officielle.

```python
import requests
import os
from dotenv import load_dotenv

def haystack_simulation():
    """
    Simule un pipeline de Question-Answering de type Haystack
    en utilisant un appel direct Ã  l'API LLMaaS.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Veuillez dÃ©finir la variable d'environnement LLMAAS_API_KEY.")
        return

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # Le contexte est l'information que le LLM est autorisÃ© Ã  utiliser.
    context = """
    Un cloud souverain est une infrastructure de cloud computing qui est entiÃ¨rement contenue 
    dans les frontiÃ¨res d'un pays spÃ©cifique et soumise Ã  ses lois. 
    Les principaux avantages sont la garantie de la rÃ©sidence des donnÃ©es, la conformitÃ© avec 
    les rÃ©glementations locales (comme le RGPD en Europe), et une protection accrue contre 
    l'accÃ¨s par des entitÃ©s Ã©trangÃ¨res en vertu de lois extraterritoriales comme le CLOUD Act amÃ©ricain.
    """
    
    question = "Quels sont les avantages d'un cloud souverain ?"
    
    # Le prompt guide le LLM pour qu'il base sa rÃ©ponse sur le contexte fourni.
    prompt = f"""
    En te basant uniquement sur le contexte suivant, rÃ©ponds Ã  la question.
    
    Contexte:
    ---
    {context}
    ---
    
    Question: {question}
    """
    
    payload = {
        "model": "granite3.3:8b",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 200,
        "temperature": 0.2 # TempÃ©rature basse pour une rÃ©ponse factuelle
    }
    
    try:
        response = requests.post(
            "https://api.ai.cloud-temple.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        answer = result['choices'][0]['message']['content']
        
        print(f"Question: {question}")
        print("\nRÃ©ponse gÃ©nÃ©rÃ©e:\n", answer)
        return answer
        
    except requests.exceptions.RequestException as e:
        print(f"Une erreur d'API est survenue: {e}")

if __name__ == "__main__":
    haystack_simulation()
```

### 7. IntÃ©gration LlamaIndex

[LlamaIndex](https://www.llamaindex.ai/) est un framework spÃ©cialisÃ© dans la construction d'applications RAG. Il offre des composants de haut niveau pour l'ingestion de donnÃ©es, l'indexation et l'interrogation. Notre API, Ã©tant compatible avec l'interface OpenAI, s'intÃ¨gre trÃ¨s facilement.

#### Le code expliquÃ©

Cet exemple montre comment configurer LlamaIndex pour utiliser l'API LLMaaS pour la gÃ©nÃ©ration de texte, tout en utilisant un modÃ¨le d'embedding local pour la vectorisation.

1.  **`setup_and_run_llamaindex`** : Cette fonction unique orchestre l'ensemble du processus.
    *   **Configuration du LLM** : LlamaIndex fournit une classe `OpenAILike` qui permet de se connecter Ã  n'importe quelle API respectant le format OpenAI. Il suffit de lui fournir notre `api_base` et une `api_key`. C'est la mÃ©thode la plus simple pour rendre notre LLM compatible.
    *   **Configuration des Embeddings** : Pour cet exemple, nous utilisons un modÃ¨le d'embedding local (`HuggingFaceEmbedding`). Cela montre la flexibilitÃ© de LlamaIndex, qui permet de mixer les composants. Vous pourriez tout aussi bien utiliser la classe `LLMaaSEmbeddings` des exemples prÃ©cÃ©dents pour utiliser notre API d'embedding.
    *   **`Settings`** : L'objet `Settings` de LlamaIndex est un moyen pratique de configurer les composants par dÃ©faut (LLM, modÃ¨le d'embedding, taille des chunks, etc.) qui seront utilisÃ©s par les autres objets LlamaIndex.
    *   **Ingestion des donnÃ©es** : `SimpleDirectoryReader` charge les documents d'un dossier.
    *   **CrÃ©ation de l'index** : `VectorStoreIndex.from_documents` est la mÃ©thode de haut niveau de LlamaIndex. Elle gÃ¨re automatiquement le dÃ©coupage en chunks, la vectorisation des chunks (en utilisant le `embed_model` configurÃ© dans `Settings`), et la crÃ©ation de l'index en mÃ©moire.
    *   **Moteur de requÃªte** : `.as_query_engine()` crÃ©e une interface simple pour poser des questions Ã  notre index. Lorsque vous appelez `.query()`, le moteur vectorise votre question, trouve les documents les plus pertinents dans l'index, et les envoie au LLM (configurÃ© dans `Settings`) avec la question pour gÃ©nÃ©rer une rÃ©ponse.

```python
# DÃ©pendances:
# pip install llama-index llama-index-llms-openai-like llama-index-embeddings-huggingface

import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import shutil

def setup_and_run_llamaindex():
    """
    Configure et exÃ©cute un pipeline RAG simple avec LlamaIndex et l'API LLMaaS.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Veuillez dÃ©finir la variable d'environnement LLMAAS_API_KEY.")
        return

    # 1. Configuration du LLM pour utiliser l'API LLMaaS via l'interface OpenAILike
    print("1. Configuration du LLM pour pointer vers l'API LLMaaS...")
    llm = OpenAILike(
        api_key=api_key,
        api_base="https://api.ai.cloud-temple.com/v1",
        model="granite3.3:8b",
        is_chat_model=True,
        # Il est parfois nÃ©cessaire d'ajouter des paramÃ¨tres de contexte pour certains modÃ¨les
        # context_window=3900, 
    )

    # 2. Configuration du modÃ¨le d'embedding (local dans cet exemple pour la simplicitÃ©)
    print("2. Configuration du modÃ¨le d'embedding local...")
    embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # 3. Application des configurations globales via l'objet Settings de LlamaIndex
    Settings.llm = llm
    Settings.embed_model = embed_model
    print("   -> LLM et modÃ¨le d'embedding configurÃ©s.")

    # 4. CrÃ©ation d'une base de connaissances simple dans un rÃ©pertoire temporaire
    print("4. CrÃ©ation et chargement d'une base de connaissances temporaire...")
    temp_dir = "temp_llama_data"
    os.makedirs(temp_dir, exist_ok=True)
    knowledge_file = os.path.join(temp_dir, "knowledge.txt")
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write("L'offre LLMaaS de Cloud Temple est une solution d'IA gÃ©nÃ©rative souveraine, "
                "entiÃ¨rement opÃ©rÃ©e en France et qualifiÃ©e SecNumCloud par l'ANSSI.")
    
    documents = SimpleDirectoryReader(temp_dir).load_data()
    print(f"   -> {len(documents)} document(s) chargÃ©(s).")

    # 5. CrÃ©ation de l'index vectoriel. LlamaIndex gÃ¨re le chunking et l'embedding.
    print("5. CrÃ©ation de l'index vectoriel...")
    index = VectorStoreIndex.from_documents(documents)
    print("   -> Index crÃ©Ã©.")

    # 6. CrÃ©ation du moteur de requÃªte et interrogation de la base de connaissances
    print("6. CrÃ©ation du moteur de requÃªte et interrogation...")
    query_engine = index.as_query_engine()
    question = "Quelles sont les garanties de souverainetÃ© de l'offre LLMaaS ?"
    response = query_engine.query(question)
    
    print(f"\nQuestion: {question}")
    print(f"RÃ©ponse: {response}")

    # Nettoyage du rÃ©pertoire temporaire
    shutil.rmtree(temp_dir)
    print(f"\nRÃ©pertoire temporaire '{temp_dir}' supprimÃ©.")

if __name__ == "__main__":
    setup_and_run_llamaindex()
```

---

## ðŸ’¡ Exemples AvancÃ©s

Vous trouverez dans lerÃ©pertoire GitHub du produit une collection d'exemples de code et de scripts dÃ©montrant les diffÃ©rentes fonctionnalitÃ©s et cas d'utilisation de l'offre LLM as a Service (LLMaaS) de Cloud Temple :

[Cloud-Temple/product-llmaas-how-to](https://github.com/Cloud-Temple/product-llmaas-how-to/tree/main)

Vous y trouverez des guides pratiques pour :
- __Extraction d'Informations et Analyse de Texte :__ CapacitÃ© Ã  analyser des documents pour en extraire des donnÃ©es structurÃ©es telles que des entitÃ©s, des Ã©vÃ©nements, des relations et des attributs, en s'appuyant sur des ontologies spÃ©cifiques Ã  des domaines (ex: juridique, RH, IT).

- __Interaction Conversationnelle et Chatbots :__ DÃ©veloppement d'agents conversationnels capables de dialoguer, de maintenir un historique d'Ã©change, d'utiliser des instructions systÃ¨me (prompts systÃ¨me) et d'invoquer des outils externes.

- __Transcription Audio (Speech-to-Text) :__ Conversion de contenu audio en texte, y compris pour des fichiers volumineux, grÃ¢ce Ã  des techniques de dÃ©coupage, de normalisation et de traitement par lots.

- __Traduction de Texte :__ Traduction de documents d'une langue Ã  une autre, en gÃ©rant le contexte sur plusieurs segments pour amÃ©liorer la cohÃ©rence.

- __Gestion et Ã‰valuation des ModÃ¨les :__ Listage des modÃ¨les de langage disponibles via l'API, consultation de leurs spÃ©cifications et exÃ©cution de tests pour comparer leurs performances.

- __Streaming de RÃ©ponses en Temps RÃ©el :__ DÃ©monstration de la capacitÃ© Ã  recevoir et afficher les rÃ©ponses des modÃ¨les de maniÃ¨re progressive (token par token), essentielle pour les applications interactives.
