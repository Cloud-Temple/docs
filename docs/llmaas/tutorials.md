---
title: Tutorials LLMaaS
sidebar_position: 6
---

# Tutorials LLMaaS

## Vue d'ensemble

Ces tutorials avanc√©s couvrent l'int√©gration, l'optimisation et les meilleures pratiques pour exploiter pleinement LLMaaS Cloud Temple en production. Chaque tutorial inclut du code test√© et des m√©triques de performance r√©elles.

## üöÄ Int√©grations LangChain et Frameworks

### 1. Int√©gration LangChain

**Configuration LangChain avec LLMaaS**

```python
# Installation des d√©pendances
# pip install langchain openai requests

from langchain.llms.base import LLM
from langchain.schema import LLMResult, Generation
from typing import Optional, List, Any
import requests
import json

class CloudTempleLLM(LLM):
    """LangChain wrapper pour LLMaaS Cloud Temple"""
    
    api_key: str
    model_name: str = "granite3.3:8b"
    base_url: str = "https://api.ai.cloud-temple.com/v1"
    temperature: float = 0.7
    max_tokens: int = 1000
    
    def __init__(self, api_key: str, **kwargs):
        super().__init__(**kwargs)
        self.api_key = api_key
    
    @property
    def _llm_type(self) -> str:
        return "cloud_temple_llmaas"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        if stop:
            payload["stop"] = stop
        
        response = requests.post(
            f"{self.base_url}/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        response.raise_for_status()
        result = response.json()
        
        return result['choices'][0]['message']['content']
    
    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> LLMResult:
        generations = []
        
        for prompt in prompts:
            text = self._call(prompt, stop)
            generations.append([Generation(text=text)])
        
        return LLMResult(generations=generations)

# Utilisation avec LangChain
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

def exemple_langchain_basic():
    # Initialisation LLM Cloud Temple
    llm = CloudTempleLLM(
        api_key="your-api-key",
        model_name="granite3.3:8b",
        temperature=0.7
    )
    
    # Template de prompt
    template = """
    Tu es un expert en {domaine}. 
    R√©ponds √† cette question de mani√®re d√©taill√©e et professionnelle:
    
    Question: {question}
    
    R√©ponse:
    """
    
    prompt = PromptTemplate(
        input_variables=["domaine", "question"],
        template=template
    )
    
    # Cr√©ation de la cha√Æne
    chain = LLMChain(llm=llm, prompt=prompt)
    
    # Ex√©cution
    result = chain.run(
        domaine="cybers√©curit√©",
        question="Quelles sont les meilleures pratiques pour s√©curiser une API REST ?"
    )
    
    return result

# Test de l'int√©gration
if __name__ == "__main__":
    reponse = exemple_langchain_basic()
    print(f"R√©ponse LangChain: {reponse}")
```

### 2. RAG (Retrieval-Augmented Generation) avec LangChain

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA

def setup_rag_pipeline():
    """Configuration compl√®te pipeline RAG avec LLMaaS"""
    
    # 1. Chargement des documents
    loader = TextLoader("documents/knowledge_base.txt")
    documents = loader.load()
    
    # 2. Division en chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    splits = text_splitter.split_documents(documents)
    
    # 3. Cr√©ation des embeddings
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    
    # 4. Index vectoriel
    vectorstore = FAISS.from_documents(splits, embeddings)
    
    # 5. LLM Cloud Temple
    llm = CloudTempleLLM(
        api_key="your-api-key",
        model_name="granite3.3:8b",
        temperature=0.3  # Plus pr√©cis pour RAG
    )
    
    # 6. Chain RAG
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )
    
    return qa_chain

def query_rag(qa_chain, question: str):
    """Interrogation du syst√®me RAG"""
    result = qa_chain({"query": question})
    
    print(f"Question: {question}")
    print(f"R√©ponse: {result['result']}")
    print(f"Sources: {len(result['source_documents'])} documents")
    
    for i, doc in enumerate(result['source_documents']):
        print(f"Source {i+1}: {doc.page_content[:200]}...")
    
    return result

# Exemple d'utilisation
rag_pipeline = setup_rag_pipeline()
query_rag(rag_pipeline, "Comment configurer la s√©curit√© d'une API ?")
```

### 3. Agents LangChain avec Outils

```python
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain.tools import BaseTool
from langchain.prompts import PromptTemplate
import requests
import json

class CloudTempleAPITool(BaseTool):
    """Outil pour interroger l'API Cloud Temple"""
    
    name = "cloud_temple_api"
    description = "Outil pour r√©cup√©rer des informations sur les services Cloud Temple"
    
    def _run(self, query: str) -> str:
        # Simulation d'appel API Cloud Temple
        api_url = "https://api.cloud-temple.com/v1/services"
        response = requests.get(api_url, params={"query": query})
        
        if response.status_code == 200:
            return f"Informations Cloud Temple: {response.json()}"
        else:
            return "Erreur lors de la r√©cup√©ration des donn√©es"
    
    def _arun(self, query: str) -> str:
        raise NotImplementedError("Async not implemented")

class CalculatorTool(BaseTool):
    """Outil de calcul simple"""
    
    name = "calculator"
    description = "Outil pour effectuer des calculs math√©matiques simples"
    
    def _run(self, expression: str) -> str:
        try:
            result = eval(expression)  # Attention: seulement pour d√©mo
            return f"R√©sultat: {result}"
        except Exception as e:
            return f"Erreur de calcul: {str(e)}"
    
    def _arun(self, expression: str) -> str:
        raise NotImplementedError("Async not implemented")

def create_agent_with_tools():
    """Cr√©ation d'un agent LangChain avec outils"""
    
    # LLM Cloud Temple
    llm = CloudTempleLLM(
        api_key="your-api-key",
        model_name="granite3.3:8b",
        temperature=0.7
    )
    
    # Outils disponibles
    tools = [
        CloudTempleAPITool(),
        CalculatorTool(),
    ]
    
    # Template de prompt pour l'agent
    prompt_template = """Tu es un assistant IA avec acc√®s √† des outils sp√©cialis√©s.
    
    Tu as acc√®s aux outils suivants:
    {tools}
    
    Utilise le format suivant:
    
    Question: la question d'entr√©e que tu dois r√©pondre
    Thought: tu devrais toujours r√©fl√©chir √† ce que tu vas faire
    Action: l'action √† prendre, doit √™tre une des [{tool_names}]
    Action Input: l'entr√©e de l'action
    Observation: le r√©sultat de l'action
    ... (cette s√©quence Thought/Action/Action Input/Observation peut se r√©p√©ter N fois)
    Thought: Je connais maintenant la r√©ponse finale
    Final Answer: la r√©ponse finale √† la question originale
    
    Commence !
    
    Question: {input}
    Thought: {agent_scratchpad}"""
    
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["input", "agent_scratchpad", "tools", "tool_names"]
    )
    
    # Cr√©ation de l'agent
    agent = create_react_agent(llm, tools, prompt)
    
    # Ex√©cuteur d'agent
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        max_iterations=3
    )
    
    return agent_executor

# Utilisation de l'agent
def test_agent():
    agent = create_agent_with_tools()
    
    # Test avec calcul
    result1 = agent.invoke({
        "input": "Calcule le co√ªt mensuel pour 1 million de tokens avec LLMaaS √† 4‚Ç¨/million"
    })
    print(f"R√©sultat 1: {result1}")
    
    # Test avec information
    result2 = agent.invoke({
        "input": "Quels sont les services disponibles chez Cloud Temple ?"
    })
    print(f"R√©sultat 2: {result2}")

test_agent()
```

### 4. Int√©gration OpenAI SDK

**Migration transparente depuis OpenAI**

```python
from openai import OpenAI

# Configuration pour Cloud Temple LLMaaS
def setup_cloud_temple_client():
    """Configuration client OpenAI pour Cloud Temple"""
    
    client = OpenAI(
        api_key="your-cloud-temple-api-key",
        base_url="https://api.ai.cloud-temple.com/v1"
    )
    
    return client

def test_openai_compatibility():
    """Test de compatibilit√© avec SDK OpenAI"""
    
    client = setup_cloud_temple_client()
    
    # Chat completion standard
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[
            {"role": "system", "content": "Tu es un assistant IA professionnel."},
            {"role": "user", "content": "Explique-moi l'architecture cloud native."}
        ],
        max_tokens=300,
        temperature=0.7
    )
    
    print(f"R√©ponse: {response.choices[0].message.content}")
    
    # Streaming
    stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[
            {"role": "user", "content": "√âcris un po√®me sur l'IA."}
        ],
        stream=True,
        max_tokens=200
    )
    
    print("Stream:")
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
    print()

# Test de compatibilit√©
test_openai_compatibility()
```

### 5. Int√©gration Semantic Kernel (Microsoft)

```python
# pip install semantic-kernel

import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion

def setup_semantic_kernel():
    """Configuration Semantic Kernel avec Cloud Temple"""
    
    # Cr√©ation du kernel
    kernel = sk.Kernel()
    
    # Service LLM Cloud Temple (compatible OpenAI)
    service_id = "cloud_temple_llm"
    kernel.add_service(
        OpenAIChatCompletion(
            service_id=service_id,
            api_key="your-api-key",
            base_url="https://api.ai.cloud-temple.com/v1",
            ai_model_id="granite3.3:8b"
        ),
    )
    
    return kernel

def create_semantic_function():
    """Cr√©ation d'une fonction s√©mantique"""
    
    kernel = setup_semantic_kernel()
    
    # Fonction de r√©sum√©
    summarize_function = kernel.create_function_from_prompt(
        prompt="""
        R√©sume le texte suivant en fran√ßais, en gardant les points essentiels:
        
        {{$input}}
        
        R√©sum√©:
        """,
        function_name="Summarize",
        description="R√©sume un texte long"
    )
    
    return kernel, summarize_function

def test_semantic_kernel():
    """Test Semantic Kernel avec Cloud Temple"""
    
    kernel, summarize_func = create_semantic_function()
    
    # Test de r√©sum√©
    long_text = """
    L'intelligence artificielle (IA) est une technologie en rapide √©volution qui transforme
    de nombreux secteurs. Cloud Temple propose LLMaaS, un service s√©curis√© et souverain
    permettant d'acc√©der aux mod√®les les plus avanc√©s tout en respectant la r√©glementation
    fran√ßaise. Avec la qualification SecNumCloud, les entreprises peuvent d√©ployer des
    solutions IA en toute confiance pour leurs donn√©es sensibles.
    """
    
    result = kernel.invoke(summarize_func, input=long_text)
    print(f"R√©sum√©: {result}")

test_semantic_kernel()
```

### 6. Framework Haystack

```python
# pip install haystack-ai

from haystack import Pipeline
from haystack.components.generators import OpenAIGenerator
from haystack.components.builders import PromptBuilder

def setup_haystack_pipeline():
    """Configuration pipeline Haystack avec Cloud Temple"""
    
    # G√©n√©rateur compatible OpenAI
    generator = OpenAIGenerator(
        api_key="your-api-key",
        api_base_url="https://api.ai.cloud-temple.com/v1",
        model="granite3.3:8b"
    )
    
    # Template de prompt
    prompt_builder = PromptBuilder(
        template="""
        Contexte: {{context}}
        
        Question: {{question}}
        
        R√©ponds de mani√®re pr√©cise et professionnelle:
        """
    )
    
    # Pipeline
    pipeline = Pipeline()
    pipeline.add_component("prompt_builder", prompt_builder)
    pipeline.add_component("llm", generator)
    
    # Connexions
    pipeline.connect("prompt_builder", "llm")
    
    return pipeline

def test_haystack():
    """Test pipeline Haystack"""
    
    pipeline = setup_haystack_pipeline()
    
    result = pipeline.run({
        "prompt_builder": {
            "context": "Cloud Temple est un fournisseur cloud souverain fran√ßais.",
            "question": "Quels sont les avantages d'un cloud souverain ?"
        }
    })
    
    print(f"R√©ponse Haystack: {result['llm']['replies'][0]}")

test_haystack()
```

### 7. Int√©gration LlamaIndex

```python
# pip install llama-index

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai_like import OpenAILike
from llama_index.core import Settings

def setup_llamaindex():
    """Configuration LlamaIndex avec Cloud Temple"""
    
    # Configuration LLM
    llm = OpenAILike(
        api_key="your-api-key",
        api_base="https://api.ai.cloud-temple.com/v1",
        model="granite3.3:8b",
        is_chat_model=True
    )
    
    # Configuration globale
    Settings.llm = llm
    
    return llm

def create_knowledge_base():
    """Cr√©ation d'une base de connaissances"""
    
    llm = setup_llamaindex()
    
    # Chargement des documents
    documents = SimpleDirectoryReader("data/").load_data()
    
    # Cr√©ation de l'index
    index = VectorStoreIndex.from_documents(documents)
    
    # Moteur de requ√™te
    query_engine = index.as_query_engine()
    
    return query_engine

def test_llamaindex():
    """Test LlamaIndex avec Cloud Temple"""
    
    query_engine = create_knowledge_base()
    
    response = query_engine.query(
        "Quelles sont les fonctionnalit√©s principales de Cloud Temple LLMaaS ?"
    )
    
    print(f"R√©ponse LlamaIndex: {response}")

test_llamaindex()
```

---

## üîß Optimisations de Performance

*Section √† venir - prochaine √©tape*

## üìä Monitoring et M√©triques

*Section √† venir - prochaine √©tape*

## üõ°Ô∏è Best Practices de S√©curit√©

*Section √† venir - prochaine √©tape*

## üí° Exemples Avanc√©s

*Section √† venir - prochaine √©tape*

---

**Prochaines √©tapes :** Section suivante selon vos besoins - optimisations, monitoring, s√©curit√© ou exemples avanc√©s.
