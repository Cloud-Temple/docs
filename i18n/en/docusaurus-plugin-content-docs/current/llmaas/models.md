

---
title: Catalog of LLMaaS Models
sidebar_position: 2
---



# LLM Models Catalog as a Service



## Overview

Cloud Temple LLMaaS offers **51 large language models** carefully selected and optimized to meet the most stringent **SecNumCloud** requirements. Our catalog covers the entire spectrum, from ultra-efficient micro-models to extremely large models.



### Global Statistics

| Metric | Value |
|--------|-------|
| **Total number of models** | 51 models |
| **Minimum context** | 8 192 tokens |
| **Maximum context** | 262 144 tokens |
| **Compliance** | SecNumCloud ✅ HDS ✅ Sovereignty ✅ C5 ❌ |
| **Location** | 100% France 🇫🇷 |



### Pricing

| Usage Type | Price |
|------------|-------|
| **Input Tokens** | 0.9€ / million of tokens |
| **Output Tokens** | 4€ / million of tokens |
| **Advanced Reasoning** | 21€ / million of tokens |



## Large Models



### gpt-oss:120b  
**OpenAI • 120B parameters • Context: 120,000 tokens**  

Leading-edge open-weight language model from OpenAI, offering strong performance with a flexible Apache 2.0 license.  

**Technical specifications:**  
- **Speed**: 38 tokens/second tokens/second  
- **Consumption**: 3.51 kWh/million tokens  
- **License**: Apache 2.0  
- **Location**: FR 🇫🇷  

**Capabilities:**  
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security  

**Tags:** `MoE` `Agent` `Reasoning` `Open-Source` `Very Large`  

**Use cases:**  
- Advanced conversational agents with complex reasoning and tool integration.  
- Applications requiring full transparency of the reasoning process (chain-of-thought).  
- Business scenarios requiring a permissive license (Apache 2.0).  
- Fine-tuning for specialized tasks requiring a powerful base model.



### llama3.3:70b
**Meta • 70B parameters • Context: 120,000 tokens**

State-of-the-art multilingual model developed by Meta, designed to excel in natural dialogue, complex reasoning, and nuanced instruction understanding.

**Technical specifications:**
- **Speed**: 15 tokens/second tokens/second
- **Consumption**: 8.89 kWh/million tokens
- **License**: LLAMA 3.3 Community Licence
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Agent` `Dialogue` `Multilingual`

**Use cases:**
- Multilingual chatbots supporting 8 languages simultaneously
- Execution of chained complex instructions (prompt chaining)
- Processing a 60K token dialogue window for conversational history
- Analysis of large legal or technical documents (>100 pages)
- Generation of structured texts with fidelity to stylistic instructions



### qwen3:235b
**Qwen Team • 235B parameters • Context: 60,000 tokens**

A very large next-generation Qwen3 model offering extended capabilities for the most complex tasks.

**Technical Specifications:**
- **Speed**: 17 tokens/second ⚡
- **Consumption**: 7.84 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Multilingual` `Very Large`

**Use Cases:**
- Highly advanced conversational agents with large context and tool integration (MCP)
- Solving extremely complex problems (math, code)
- Analysis and generation of very large and technical documents
- Multilingual applications (>100 languages) requiring high-fidelity understanding and generation



### deepseek-r1:671b
**DeepSeek AI • 671B parameters • Context: 16,000 tokens**

Extremely large model from DeepSeek AI, designed for the pinnacle of reasoning and generation.

**Technical specifications:**
- **Speed**: 12 tokens/second tokens/second
- **Consumption**: 11.11 kWh/million tokens
- **License**: MIT License
- **Location**: FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Extremely Large`

**Use Cases:**
- Peak reasoning tasks
- High-quality text generation
- AI research and development

---



### gemma3:27b
**Google • 27B parameters • Context: 120,000 tokens**

Revolutionary model from Google offering an optimal balance between power and efficiency, with an exceptional performance/cost ratio for demanding professional applications.

**Technical specifications:**
- **Speed**: 20 tokens/second
- **Consumption**: 6.67 kWh/million tokens
- **License**: Google Gemma Terms of Use
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Vision` `Agent` `Large context`

**Use cases:**
- Document analysis with extended context up to 120K tokens (about 400 pages)
- Semantic indexing and search in large document databases
- Processing of images and text simultaneously through multimodal capabilities
- Structured data extraction from PDFs and scanned documents
- Integration with external tools via the API function calling



### qwen3-coder:30b
**Qwen Team • 30B parameters • Context: 250,000 tokens**

MoE model optimized for software engineering tasks, with a very long context.

**Technical Specifications:**
- **Speed** : 80 tokens/second ⚡
- **Consumption** : 3.3 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Programming` `Long Context` `MoE`

**Use Cases:**
- Software engineering agents for exploring and modifying codebases
- Complex code generation with repository-scale understanding
- Reasoning tasks on extended contexts
- Code improvement via reinforcement learning



### qwen3-2507-think:30b-a3b
**Qwen Team • 30B parameters • Context: 120,000 tokens**

Advanced model from the Qwen3 family, optimized for deep reasoning and extended contexts.

**Technical specifications:**
- **Speed**: 80 tokens/second ⚡
- **Consumption**: 3.3 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Large Context`

**Use cases:**
- Analyzing large documents with complex reasoning.
- Conversational agents with extended conversation history.
- Q&A tasks on large text corpora.
- Integration with external tools via function calling on large contexts.



### qwen3-2507:30b-a3b
**Qwen Team • 30B parameters • Context: 250,000 tokens**

Improved version of the non-thinking mode of Qwen3-30B, with enhanced general capabilities, knowledge coverage, and user alignment.

**Technical specifications:**
- **Speed**: 90 tokens/second tokens/second ⚡
- **Consumption**: 2.16 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Agent` `Large Context` `MoE` `Multilingual`

**Use cases:**
- Complex tasks requiring precise instruction following and logical reasoning.
- Multilingual applications with extensive knowledge coverage.
- High-quality text generation for open-ended and subjective tasks.
- Analysis of very large documents thanks to the 250k token context.



### qwen3:30b-a3b
**Qwen Team • 30B parameters • Context: 32,000 tokens**

Latest generation of Qwen models, offering significant improvements in training data, architecture, and optimization.

**Technical specifications:**
- **Speed**: 50 tokens/second
- **Consumption**: 3.89 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Programming` `Multilingual` `MoE`

**Use cases:**
- Complex reasoning tasks and code generation.
- Multilingual applications requiring broad language coverage.
- Scenarios requiring a good balance between performance and resource efficiency through the MoE architecture.



### deepseek-r1:70b
**DeepSeek AI • 70B parameters • Context: 32,000 tokens**

70B Model from DeepSeek AI

**Technical Specifications:**
- **Speed**: 21 tokens/second
- **Consumption**: 12.56 kWh/million tokens
- **License**: MIT License
- **Location**: FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Large`

**Use Cases:**
- Top-tier reasoning tasks
- High-quality text generation
- AI research and development

---



### qwen2.5vl:32b
**Qwen Team • 32B parameters • Context: 120,000 tokens**

Most powerful version of the Qwen2.5-VL series, offering cutting-edge visual understanding and agent capabilities.

**Technical specifications:**
- **Speed**: 18 tokens/second tokens/second
- **Consumption**: 7.41 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Vision` `Agent` `Reasoning` `OCR` `Visual Localization` `Large`

**Use cases:**
- Analysis of very complex documents and diagrams
- Autonomous visual agents for navigation and interaction with GUIs
- High-precision object localization and text recognition tasks
- Generation of rich and detailed descriptions from complex images



### qwen2.5vl:72b
**Qwen Team • 72B parameters • Context: 128,000 tokens**

Most powerful version of the Qwen2.5-VL series, offering cutting-edge visual understanding and agent capabilities for the most demanding tasks.

**Technical Specifications:**
- **Speed** : 15 tokens/second tokens/second
- **Consumption** : 8.89 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Vision` `Agent` `Reasoning` `OCR` `Visual Localization` `Very Large`

**Use Cases:**
- Analysis of very complex documents and diagrams
- Autonomous visual agents for navigation and interaction with GUIs
- Object localization and high-precision text recognition tasks
- Generation of rich, detailed descriptions from very complex images



## Specialized Models



### embeddinggemma:300m
**Google • 300M parameters • Context: 2,048 tokens**

State-of-the-art embedding model from Google, optimized for its size, ideal for search and semantic retrieval tasks.

**Technical specifications:**
- **License** : Google Gemma Terms of Use
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Embedding` `Compact` `Semantic` `Efficient` `Multilingual`

**Use cases:**
- Information search and retrieval (Retrieval)
- Document classification and clustering
- Semantic similarity search
- Deployment on resource-limited devices (mobile, laptop)



### gpt-oss:20b
**OpenAI • 20B parameters • Context: 120,000 tokens**

Open-weight language model from OpenAI, optimized for efficiency and deployment on consumer hardware.

**Technical specifications:**
- **Speed** : 57 tokens/second ⚡
- **Power consumption** : 2.34 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `MoE` `Agent` `Reasoning` `Open-Source` `Compact` `Fast`

**Use cases:**
- Deployments on resource-limited devices (edge devices) or low-cost servers.
- Applications requiring fast inference with strong reasoning capabilities.
- Agent use cases with function calls, web navigation, and code execution.
- Fine-tuning for specialized tasks on consumer hardware.



### qwen3:14b
**Qwen Team • 14B parameters • Context: 32,000 tokens**

New generation dense model Qwen3 (14B), offering performance equivalent to Qwen2.5 32B with better efficiency.

**Technical specifications:**
- **Speed** : 40 tokens/second ⚡
- **Consumption** : 3.33 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Fast` `Multilingual`

**Use cases:**
- General tasks requiring performance and large context
- Creative and technical content generation
- Data analysis and complex reasoning
- Integration with external tools via function calling



### gemma3:12b
**Google • 12B parameters • Context: 120,000 tokens**

Intermediate version of the Gemma 3 model offering an excellent balance between performance and efficiency.

**Technical specifications:**
- **Speed** : 56 tokens/second ⚡
- **Consumption** : 4.71 kWh/million tokens
- **License** : Google Gemma Terms of Use
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ✅ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Vision` `Fast` `Large Context`

**Use cases:**
- Multimodal applications with moderate resource constraints
- Document processing with standard context (up to 100 pages)
- Combined text content generation and image analysis
- Deployments on standard GPUs without specialized infrastructure
- Advanced chatbots with integrated visual and text capabilities



### gemma3:4b
**Google • 4B parameters • Context: 120,000 tokens**

Compact model from Google offering excellent performance in a lightweight and cost-effective format.

**Technical specifications:**
- **Speed** : 57 tokens/second ⚡
- **Consumption** : 0.58 kWh/million tokens 🌱
- **License** : Google Gemma Terms of Use
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ✅ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Vision` `Fast` `Compact` `Large Context` `Efficient`

**Use cases:**
- Embedded applications and edge computing with image processing
- Multimodal chatbots with low latency
- Large-scale deployments with visual and text capabilities
- Mobile applications with image and text analysis
- Processing of simple to medium visual queries with high performance



### gemma3:1b
**Google • 1B parameters • Context: 32,000 tokens**

Ultra-light micro-model designed for deployments on devices with very limited resources.

**Technical specifications:**
- **Speed** : 112 tokens/second ⚡
- **Consumption** : 0.15 kWh/million tokens 🌱
- **License** : Google Gemma Terms of Use
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Ultra-compact` `Embedded` `Efficient` `Fast`

**Use Cases:**
- Deployment on IoT devices and embedded systems with API integration
- Applications requiring local inference on CPU with function calls
- Basic text tasks with instant response time and function calling
- Compact assistants for consumer applications with external service integration
- Intelligent control systems integrating multiple APIs/services



### lucie-instruct:7b
**OpenLLM-France • 7B parameters • Context: 32 000 tokens**

Multilingual open-source causal model (7B), fine-tuned from Lucie-7B. Optimized for French.

**Technical specifications:**
- **Speed** : 4 tokens/second
- **Consumption** : 8.33 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags :** `French` `Open-Source` `Efficient`



### mistral-small3.1:24b
**Mistral AI • 24B parameters • Context: 120,000 tokens**

Compact and reactive model from Mistral AI, specifically designed to provide smooth and relevant conversational assistance with optimal response speed.

**Technical specifications:**
- **Speed** : 35 tokens/second tokens/second
- **Consumption** : 3.72 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Vision` `Agent` `Security`

**Use cases:**
- Conversational applications
- Virtual assistants combining image and text analysis (26 tokens/s)
- Technical support chatbots with access to technical documentation
- Content creation/editing tools with immediate response (blogs, emails)
- Deployment on standard infrastructure (24B parameters)



### mistral-small3.2:24b
**Mistral AI • 24B parameters • Context: 120,000 tokens**

Minor update to Mistral Small 3.1, improving instruction following, function calling robustness, and reducing repetition errors.

**Technical Specifications:**
- **Speed** : 35 tokens/second tokens/second
- **Consumption** : 3.72 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Vision` `Agent` `Security` `Instruction Following`

**Use Cases:**
- Conversational agents with improved instruction following
- Robust integration with external tools via function calling
- Applications requiring high reliability to avoid repetitions
- Same use cases as Mistral Small 3.1 with improved performance



### deepcoder:14b  
**Agentica x Together AI • 14B parameters • Context: 32,000 tokens**  

Open-source AI model (14B) by Together AI & Agentica, a credible alternative to proprietary models for code generation.  

**Technical specifications:**  
- **Speed**: 64 tokens/second ⚡  
- **Consumption**: 4.12 kWh/million tokens  
- **License**: Apache 2.0  
- **Location**: FR 🇫🇷  

**Capabilities:**  
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security  

**Tags:** `Programming` `Reasoning` `Open-Source` `Mathematics` `Fast`  

**Use cases:**  
- Code generation in over 15 languages with performance optimization  
- Debugging and refactoring of existing codebases with impact analysis  
- Implementation of complex algorithms (graphs, trees, heuristics)  
- Automated unit test creation with code coverage > 80%  
- Code transposition between languages/frameworks (e.g., Python to JavaScript)



### granite3.2-vision:2b
**IBM • 2B parameters • Context: 16,384 tokens**

Revolutionary compact model from IBM specialized in computer vision, capable of analyzing and understanding visual documents directly without requiring intermediate OCR technologies.

**Technical specifications:**
- **Speed**: 48 tokens/second
- **Consumption**: 0.69 kWh/million tokens 🌱
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Vision` `Security` `Compact` `Efficient`

**Use cases:**
- Extract structured data from invoices and forms without OCR
- Direct analysis of tables and charts with trend interpretation
- Reading and interpreting technical diagrams (electrical, mechanical)
- Processing handwritten documents with high recognition rate
- Lightweight computer vision (2B parameters) with high speed (50 tokens/s)



### granite3.3:8b
**IBM • 8B parameters • Context: 60,000 tokens**

Granite 8B model fine-tuned by IBM for improved reasoning and instruction-following capabilities, with a 128k tokens context.

**Technical specifications:**
- **Speed**: 30 tokens/second
- **Consumption**: 1.11 kWh/million tokens 🌱
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ✅ Security

**Tags:** `Agent` `Reasoning` `Security` `Efficient`

**Use cases:**
- General instruction-following tasks (classification, extraction, Q&A)
- Multilingual AI assistants (12 languages)
- Processing of very long documents (128k tokens) for summary, Q&A, etc.
- Code generation/completion with Fill-in-the-Middle
- Integration with external tools via function calling
- Structured reasoning with the "Thinking" mode



### granite3.3:2b
**IBM • 2B parameters • Context: 120 000 tokens**

Granite 2B model fine-tuned by IBM, optimized for reasoning and instruction following, with a context of 128k tokens.

**Technical specifications:**
- **Speed** : 45 tokens/second tokens/second
- **Consumption** : 0.74 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ✅ Security

**Tags:** `Agent` `Reasoning` `Security` `Efficient`

**Use cases:**
- Lightweight deployments with large context (128k tokens)
- General instruction-following tasks on limited resources
- Compact multilingual AI assistants
- Processing of long documents on less powerful devices
- FIM code generation/completion on standard workstations



### magistral:24b
**Mistral AI • 24B parameters • Context: 40,000 tokens**

Mistral AI's first reasoning model, excelling in domain-specific reasoning, transparent and multilingual.

**Technical specifications:**
- **Speed** : 25 tokens/second tokens/second
- **Consumption** : 5.33 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ✅ Security

**Tags:** `Reasoning` `Multilingual`

**Use Cases:**
- Strategic and business operations (risk modeling)
- Regulated industries (legal, finance) with traceable reasoning
- Software engineering (project planning, architecture)
- Content creation and communication (creative writing, storytelling)



### granite3.1-moe:3b
**IBM • 3B parameters • Context: 32,000 tokens**

Innovative IBM model using the Mixture-of-Experts (MoE) architecture to deliver exceptional performance while drastically optimizing computational resource usage.

**Technical specifications:**
- **Speed** : 74 tokens/second ⚡
- **Consumption** : 0.45 kWh per million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Agent` `Security` `Fast` `MoE` `Efficiency` `Efficient`

**Use cases:**
- General-purpose applications with optimized inference cost (42 tokens/second)
- Document processing in CPU environments with limited RAM usage
- Specialized analyses with dynamic activation of relevant model components
- High-density deployments with low energy consumption per inference
- Parallel processing of multiple query types with MoE specialization



### cogito:14b
**Deep Cogito • 14B parameters • Context: 32,000 tokens**

Deep Cogito model specifically designed to excel in deep reasoning tasks and nuanced contextual understanding, ideal for advanced analytical applications.

**Technical specifications:**
- **Speed** : 60 tokens/second ⚡
- **Consumption** : 4.4 kWh/million tokens
- **License** : LLAMA 3.2 Community Licence
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Comprehension` `Analysis` `Fast`

**Use cases:**
- Semantic analysis of texts with identification of implicit implications
- Structured causal reasoning with identification of cause-effect relationships
- Synthesis of complex documents with extraction of key information
- Precise question-answering systems on specialized document corpora
- Argumentative analysis with evaluation of the soundness of reasoning



### cogito:32b
**Deep Cogito • 32B parameters • Context: 32,000 tokens**

Advanced version of the Cogito model offering significantly enhanced reasoning and analysis capabilities, designed for the most demanding analytical AI applications.

**Technical specifications:**
- **Speed** : 32 tokens/second tokens/second
- **Consumption** : 8.25 kWh/million tokens
- **License** : LLAMA 3.2 Community Licence
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Comprehension` `Analysis`

**Use cases:**
- Multi-factorial scenario analysis with probabilistic result evaluation
- Scientific problem resolution with formal step demonstration
- High-stakes applications requiring precision and result verifiability
- Expert systems in specialized fields (legal, medical, technical)
- Multi-step reasoning analysis with complete conclusion explainability



### qwen3:32b
**Qwen Team • 32B parameters • Context: 40,000 tokens**

Powerful model of the new generation Qwen3, offering advanced capabilities in reasoning, code, and agentics, with an extended context.

**Technical specifications:**
- **Speed** : 18 tokens/second tokens/second
- **Consumption** : 7.41 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Multilingual` `Large Context`

**Use Cases:**
- Advanced conversational agents with large context and tool integration (MCP)
- Complex problem solving (math, code) with "Thinking" mode
- Analysis and generation of large documents
- Multilingual applications (>100 languages) requiring deep understanding



### qwq:32b
**Qwen Team • 32B parameters • Context: 32,000 tokens**

32-billion-parameter model enhanced by reinforcement learning (RL) to excel in reasoning, coding, mathematics, and agent tasks.

**Technical specifications:**
- **Speed**: 35 tokens/second tokens/second
- **Consumption**: 7.54 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Coding` `Mathematics`

**Use cases:**
- Complex problem-solving requiring reasoning and tool usage
- Code generation and execution with result verification
- Advanced mathematical tasks with accuracy verification
- Agent applications capable of interacting with the environment
- Enhanced instruction following and alignment with human preferences



### deepseek-r1:14b
**DeepSeek AI • 14B parameters • Context: 32,000 tokens**

Compact and efficient version of the DeepSeek-R1 model, offering an excellent balance between performance and lightweight for deployments requiring flexibility and responsiveness.

**Technical specifications:**
- **Speed** : 62 tokens/second ⚡
- **Consumption** : 4.26 kWh/million tokens
- **License** : MIT License
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Compact` `Versatile` `Fast`

**Use cases:**
- General-purpose applications with rapid inference needs (44 tokens/s)
- Deployments on standard servers without specialized GPU (14B parameters)
- Text processing with contextual analysis and fast response times
- Edge computing deployment with optimized local inference
- Rapid AI application prototyping with short iteration times



### deepseek-r1:32b
**DeepSeek AI • 32B parameters • Context: 32,000 tokens**

Intermediate version of the DeepSeek-R1 model offering a strategic balance between the advanced capabilities of the 70B version and the efficiency of the 14B version, for optimal versatility and performance.

**Technical specifications:**
- **Speed** : 33 tokens/second
- **Consumption** : 7.99 kWh/million tokens
- **License** : MIT license
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Versatile`

**Use cases:**
- Applications requiring a good power/cost balance (32B parameters)
- Professional text processing with semantic subtlety analysis
- Automated generation of structured reports from raw data
- Applications combining data analysis and content generation
- Specialized assistants for technical fields (legal, medical, technical)



### cogito:3b
**Deep Cogito • 3B parameters • Context: 32,000 tokens**

Compact version of the Cogito model, optimized for reasoning on devices with limited resources.

**Technical specifications:**
- **Speed**: 55 tokens/second ⚡
- **Consumption**: 0.61 kWh/million tokens 🌱
- **License**: LLAMA 3.2 Community License
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Compact` `Embedded` `Efficient` `Fast`



### granite-embedding:278m
**IBM • 278M parameters • Context: 512 tokens**

IBM's ultra-light embedding model for semantic search and classification.

**Technical specifications:**
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags :** `Embedding` `Compact` `Semantic` `Efficient`



### granite3-guardian:2b
**IBM • 2B parameters • Context: 8 192 tokens**

Compact IBM model specialized in security and compliance, detecting risks and inappropriate content.

**Technical specifications:**
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ✅ Security

**Tags :** `Security` `Compliance` `Compact` `Filtering` `Efficient`



### granite3-guardian:8b
**IBM • 8B parameters • Context: 32,000 tokens**

IBM model specialized in security and compliance, offering advanced risk detection capabilities.

**Technical specifications:**
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ✅ Security

**Tags :** `Security` `Compliance` `Filtering`



### qwen2.5:0.5b
**Qwen Team • 0.5B parameters • Context: 32,000 tokens**

Ultra-lightweight micro-model from the Qwen 2.5 family, designed for maximum efficiency on constrained devices.

**Technical Specifications:**
- **Speed**: 162 tokens/second ⚡
- **Consumption**: 0.1 kWh/million tokens 🌱
- **License**: MIT license
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Ultra-compact` `Fast` `Embedded` `Efficient`



### qwen2.5:1.5b
**Qwen Team • 1.5B parameters • Context: 32,000 tokens**

Very compact model from the Qwen 2.5 family, offering a good performance/size balance for lightweight deployments.

**Technical specifications:**
- **Speed** : 102 tokens/second ⚡
- **Consumption** : 0.33 kWh per million tokens 🌱
- **License** : MIT license
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Compact` `Fast` `Embedded` `Efficient`



### qwen2.5:14b
**Qwen Team • 14B parameters • Context: 32,000 tokens**

A versatile mid-sized model from the Qwen 2.5 family, offering a good balance between performance and resources.

**Technical specifications:**
- **Speed** : 61 tokens/second ⚡
- **Consumption** : 4.33 kWh/million tokens
- **License** : MIT License
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Versatile` `Multilingual` `Fast`



### qwen2.5:32b
**Qwen Team • 32B parameters • Context: 32,000 tokens**

Powerful model from the Qwen 2.5 family, offering advanced capabilities in understanding and generation.

**Technical specifications:**
- **Speed**: 31 tokens/second
- **Consumption**: 8.51 kWh/million tokens
- **License**: MIT License
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Versatile` `Multilingual` `Reasoning`



### qwen2.5:3b
**Qwen Team • 3B parameters • Context: 32,000 tokens**

Compact and efficient model from the Qwen 2.5 family, suitable for general tasks on limited resources.

**Technical specifications:**
- **Speed** : 64 tokens/second ⚡
- **Consumption** : 0.52 kWh/million tokens 🌱
- **License** : MIT License
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Compact` `Fast` `Versatile` `Efficient`



### qwen3:0.6b
**Qwen Team • 0.6B parameters • Context: 32,000 tokens**

Compact and efficient model from the Qwen3 family, suitable for general tasks on limited resources.

**Technical specifications:**
- **Speed** : 112 tokens/second ⚡
- **Consumption** : 0.15 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Compact` `Fast` `Versatile` `Efficient`



### qwen3:1.7b
**Qwen Team • 1.7B parameters • Context: 32,000 tokens**

Very compact model from the Qwen3 family, offering a good performance/size balance for lightweight deployments.

**Technical specifications:**
- **Speed** : 88 tokens/second ⚡
- **Consumption** : 0.38 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Compact` `Fast` `Embedded` `Efficient`



### qwen3:4b
**Qwen Team • 4B parameters • Context: 32,000 tokens**

Compact model from the Qwen3 family offering excellent performance in a lightweight and cost-effective format.

**Technical specifications:**
- **Speed** : 49 tokens/second tokens/second
- **Consumption** : 0.68 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Compact` `Efficient`



### qwen3-2507-think:4b
**Qwen Team • 4B parameters • Context: 250,000 tokens**

Qwen3-4B model optimized for reasoning, with improved performance on logical tasks, mathematics, science, and code, and an extended context of 250K tokens.

**Technical specifications:**
- **Speed**: 70 tokens/second ⚡
- **Consumption**: 1.9 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Large Context` `Compact` `Fast`

**Use cases:**
- Very complex reasoning tasks (logic, mathematics, science, code).
- Conversational agents with very long conversation history (256k tokens).
- Analysis of very large documents with deep reasoning.
- Integration with external tools via function calling on very large contexts.



### qwen3-2507:4b
**Qwen Team • 4B parameters • Context: 250,000 tokens**

Updated version of the non-thinking mode of Qwen3-4B, with significant improvements in general capabilities, expanded knowledge coverage, and better alignment with user preferences.

**Technical specifications:**
- **Speed**: 70 tokens/second ⚡
- **Power Consumption**: 1.9 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Agent` `Large Context` `Compact` `Fast` `Multilingual`

**Use cases:**
- General tasks requiring precise instruction following and logical reasoning.
- Multilingual applications with extensive knowledge coverage.
- High-quality text generation for open and subjective tasks.
- Analysis of very large documents thanks to the 256k token context.



### qwen3:8b
**Qwen Team • 8B parameters • Context: 32,000 tokens**

Qwen3 8B model offering a good balance between performance and efficiency for general tasks.

**Technical specifications:**
- **Speed**: 33 tokens/second
- **Consumption**: 1.01 kWh/million tokens 🌱
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Agent` `Multilingual` `Efficient`



### qwen2.5vl:3b
**Qwen Team • 3.8B parameters • Context: 128,000 tokens**

Compact Vision-Language model, high-performance solution for edge AI.

**Technical Specifications:**
- **Speed** : 65 tokens/second ⚡
- **Consumption** : 0.51 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Vision` `Agent` `Reasoning` `Fast` `Efficient` `OCR` `Visual Localization` `Edge AI`



### qwen2.5vl:7b
**Qwen Team • 7B (8.3B) parameters • Context: 128,000 tokens**

High-performance Vision-Language model, outperforming GPT-4o-mini on certain tasks.

**Technical Specifications:**
- **Speed** : 35 tokens/second tokens/second
- **Consumption** : 0.95 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Vision` `Agent` `Reasoning` `Efficient` `OCR` `Visual Localization`



### hf.co/roadus/Foundation-Sec-8B-Q4_K_M-GGUF:Q4_K_M
**Foundation AI — Cisco • 8B parameters • Context: 16,384 tokens**

Specialized language model for cybersecurity, optimized for efficiency.

**Technical specifications:**
- **Speed** : 21 tokens/second
- **Power consumption** : 1.59 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ✅ Security

**Tags:** `Security` `Compact`



### devstral:24b
**Mistral AI & All Hands AI • 24B parameters • Context: 120,000 tokens**

Devstral is an agentic LLM for software engineering tasks.

**Technical specifications:**
- **Speed**: 45 tokens/second
- **Consumption**: 5.86 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Agent` `Programming` `Open-Source` `Large Context`

**Use cases:**
- Exploration and modification of codebases
- Agentic
- European



### cogito:8b
**Deep Cogito • 8B parameters • Context: 32,000 tokens**

Intermediate-sized model from the Cogito family, offering a good balance between reasoning capabilities and efficiency.

**Technical specifications:**
- **Speed** : 30 tokens/second
- **Consumption** : 1.11 kWh/million tokens 🌱
- **License** : LLAMA 3.2 Community Licence
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags :** `Agent` `Reasoning` `Versatile` `Efficient`



### llama3.1:8b
**Meta • 8B parameters • Context: 32,000 tokens**

Base model of the Llama 3.1 family, offering strong performance for its size.

**Technical specifications:**
- **Speed** : 31 tokens/second tokens/second
- **Consumption** : 1.08 kWh/million tokens 🌱
- **License** : LLAMA 3.1 Community Licence
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Versatile` `Efficient`



### phi4-reasoning:14b
**Microsoft • 14B parameters • Context: 32,000 tokens**

Microsoft's Phi family model, specialized in complex reasoning and mathematics.

**Technical specifications:**
- **Speed** : 71 tokens/second ⚡
- **Consumption** : 3.71 kWh/million tokens
- **License** : MIT License
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Mathematics` `Programming` `Fast`



## Recommended Use Cases



### Multilingual Dialogue
Chatbots and assistants capable of communicating in multiple languages with automatic detection, context maintenance throughout the conversation, and understanding of linguistic specifics

**Recommended Models:**
- Llama 3.3
- Mistral Small 3.2
- Qwen 3
- Granite 3.3



### Long Document Analysis
Processing of large documents (>100 pages) with maintaining context throughout the text, extracting key information, generating relevant summaries, and answering specific questions about the content

**Recommended Models:**
- Gemma 3
- Qwen3
- Granite 3.3



### Programming and Development
Code generation and optimization in multiple languages, debugging, refactoring, development of full features, understanding of complex algorithmic implementations, and unit test creation

**Recommended models:**
- DeepCoder
- QwQ
- Qwen3 coder
- Granite 3.3
- Devstral



### Visual Analysis
Direct processing of images and visual documents without OCR pre-processing, interpretation of technical diagrams, graphs, tables, drawings, and photos with generation of detailed textual explanations of the visual content

**Recommended models:**
- Granite 3.2 Vision
- Mistral Small 3.2
- Gemma 3
- Qwen2.5-VL



### Security and Compliance
Applications requiring specific security capabilities; sensitive content filtering, reasoning traceability, RGPD/HDS compliance verification, risk minimization, vulnerability analysis, and adherence to sectoral regulations

**Recommended models:**
- Granite Guardian
- Granite 3.3
- Devstral
- Mistral Small 3.1
- Magistral 24b
- Foundation-Sec-8B



### Lightweight and Embedded Deployments
Applications requiring minimal resource footprint, deployment on devices with limited capacity, real-time inference on standard CPUs, and integration into embedded systems or IoT

**Recommended Models:**
- Gemma 3
- Granite 3.1 MoE
- Granite Guardian
- Granite 3.3