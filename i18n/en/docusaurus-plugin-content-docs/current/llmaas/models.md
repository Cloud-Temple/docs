---
title: Catalog of LLMaaS Models
sidebar_position: 2
---

# LLM as a Service Model Catalog

## Overview

Cloud Temple LLMaaS offers **45 carefully selected and optimized large language models** to meet the strictest **SecNumCloud** requirements. Our catalog covers the entire spectrum, from ultra-efficient micro-models to extremely large models.

### Global Statistics

| Metric | Value |
|----------|--------|
| **Total number of models** | 45 models |
| **Minimum context** | 8 192 tokens |
| **Maximum context** | 128 000 tokens |
| **Compliance** | SecNumCloud ✅ HDS ✅ Sovereignty ✅ C5 ❌ |
| **Location** | 100% France 🇫🇷 |

### Pricing

| Usage Type | Price |
|------------|-------|
| **Input Tokens** | 0.9€ / million tokens |
| **Output Tokens** | 4€ / million tokens |
| **Advanced Reasoning** | 21€ / million tokens |

## Large Models

### Llama 3.3 70B
**Meta • 70B parameters • Context: 60,000 tokens**

State-of-the-art multilingual model developed by Meta, designed to excel in natural dialogue, complex reasoning, and nuanced understanding of instructions.

**Technical specifications:**
- **Speed** : 26 tokens/second
- **Consumption** : 11.75 kWh/million tokens
- **License** : LLAMA 3.3 Community Licence
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Agent` `Dialogue` `Multilingual`

**Use cases:**
- Multilingual chatbots supporting 8 languages simultaneously
- Execution of chained complex instructions (prompt chaining)
- Processing a 60K token dialogue window for conversational history
- Analysis of large legal or technical documents (>100 pages)
- Generation of structured texts with fidelity to stylistic instructions

### Qwen3 235B
**Qwen Team • 235B parameters • Context: 60,000 tokens**

A very large-scale model from the new generation Qwen3, offering extended capabilities for the most complex tasks.

**Technical specifications:**
- **Speed** : 17 tokens/second
- **Power consumption** : 7.84 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Multilingual` `Very Large`

**Use cases:**
- Highly advanced conversational agents with large context and tool integration (MCP)
- Solving extremely complex problems (maths, code)
- Analyzing and generating very large and technical documents
- Multilingual applications (>100 languages) requiring high-fidelity understanding and generation

### DeepSeek-R1 671B
**DeepSeek AI • 671B parameters • Context: 16,000 tokens**

Extremely large model from DeepSeek AI, designed for the pinnacle of reasoning and generation.

**Technical Specifications:**
- **Speed**: 12 tokens/second
- **Consumption**: 11.11 kWh/million tokens
- **License**: MIT License
- **Location**: FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Extremely Large`

**Use Cases:**
- Cutting-edge reasoning tasks
- High-quality text generation
- AI research and development

---

### Gemma 3 27B  
**Google • 27B parameters • Context: 120,000 tokens**  

Revolutionary model from Google offering an optimal balance between power and efficiency, with an exceptional performance/cost ratio for demanding professional applications.  

**Technical specifications:**  
- **Speed:** 20 tokens/second  
- **Consumption:** 6.67 kWh/million tokens  
- **License:** Google Gemma Terms of Use  
- **Location:** FR 🇫🇷  

**Capabilities:**  
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ❌ Security  

**Tags:** `Vision` `Agent` `Large context`  

**Use cases:**  
- Document analysis with extended context up to 120K tokens (approximately 400 pages)  
- Semantic indexing and search in large document databases  
- Simultaneous image and text processing through multimodal capabilities  
- Structured data extraction from PDFs and scanned documents  
- Integration with external tools via API function calling

### Qwen3 30B-A3B FP8
**Qwen Team • 30B-A3B parameters • Context: 32,000 tokens**

Next-generation FP8 MoE (3B activated) model with hybrid thinking modes and advanced agent capabilities.

**Technical Specifications:**
- **Speed**: 106 tokens/second ⚡
- **Consumption**: 2.88 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `MoE` `Agent` `Reasoning` `Fast` `Multilingual`

**Use Cases:**
- Advanced conversational agents with tool integration (MCP)
- Complex problem solving (math, code) with "Thinking" mode
- Multilingual applications (>100 languages)
- Scenarios requiring cost/performance balance (MoE) on VLLM
- Engaging multi-turn dialogue and precise instruction following

### DeepSeek-R1 70B
**DeepSeek AI • 70B parameters • Context: 32,000 tokens**

70B model from DeepSeek AI

**Technical specifications:**
- **Speed**: 21 tokens/second tokens/second
- **Consumption**: 12.56 kWh/million tokens
- **License**: MIT License
- **Location**: FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Large`

**Use cases:**
- Top-tier reasoning tasks
- High-quality text generation
- AI research and development

---

### Qwen2.5-VL 32B
**Qwen Team • 32B parameters • Context: 120 000 tokens**

Most powerful version of the Qwen2.5-VL series, offering cutting-edge visual understanding and agent capabilities.

**Technical Specifications:**
- **Speed**: 18 tokens/second
- **Consumption**: 7.41 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Vision` `Agent` `Reasoning` `OCR` `Visual Localization` `Large`

**Use Cases:**
- Analysis of highly complex documents and diagrams
- Autonomous visual agents for navigation and interaction with GUIs
- High-precision object localization and text recognition tasks
- Generation of rich and detailed descriptions from complex images

### Qwen2.5-VL 72B
**Qwen Team • 72B parameters • Context: 128,000 tokens**

Most powerful version of the Qwen2.5-VL series, offering cutting-edge visual understanding and agent capabilities for the most demanding tasks.

**Technical specifications:**
- **Speed**: 15 tokens/second
- **Consumption**: 8.89 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Vision` `Agent` `Reasoning` `OCR` `Visual Localization` `Very Large`

**Use cases:**
- Analysis of highly complex documents and diagrams
- Autonomous visual agents for navigation and interaction with GUIs
- Object localization and high-precision text recognition tasks
- Generation of rich and detailed descriptions from highly complex images

## Specialized Models

### Qwen3 14B
**Qwen Team • 14B parameters • Context: 32,000 tokens**

Next-generation dense model Qwen3 (14B), offering performance equivalent to Qwen2.5 32B with better efficiency.

**Technical specifications:**
- **Speed**: 68 tokens/second ⚡
- **Consumption**: 3.88 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Rapid` `Multilingual`

**Use cases:**
- General tasks requiring performance and large context
- Creative and technical content generation
- Data analysis and complex reasoning
- Integration with external tools via function calling

### Gemma 3 12B  
**Google • 12B parameters • Context: 120 000 tokens**  

Intermediate version of the Gemma 3 model offering an excellent balance between performance and efficiency.  

**Technical specifications:**  
- **Speed** : 56 tokens/second ⚡  
- **Consumption** : 4.71 kWh/million tokens  
- **License** : Google Gemma Terms of Use  
- **Location** : FR 🇫🇷  

**Capabilities:**  
❌ Tools/Agent • ✅ Vision • ❌ Reasoning • ❌ Security  

**Tags:** `Vision` `Fast` `Large Context`  

**Use Cases:**  
- Multimodal applications with moderate resource constraints  
- Document processing with standard context (up to 100 pages)  
- Combined text content generation and image analysis  
- Deployments on standard GPUs without specialized infrastructure  
- Advanced chatbots with integrated visual and text capabilities

### Gemma 3 4B  
**Google • 4B parameters • Context: 120,000 tokens**  

Compact model from Google offering excellent performance in a lightweight and cost-effective format.  

**Technical specifications:**  
- **Speed** : 57 tokens/second ⚡  
- **Consumption** : 0.58 kWh/million tokens 🌱  
- **License** : Google Gemma Terms of Use  
- **Location** : FR 🇫🇷  

**Capabilities:**  
❌ Tools/Agent • ✅ Vision • ❌ Reasoning • ❌ Security  

**Tags:** `Vision` `Fast` `Compact` `Large Context` `Efficient`  

**Use cases:**  
- Embedded applications and edge computing with image processing  
- Multimodal chatbots requiring low latency  
- Large-scale deployments with visual and text capabilities  
- Mobile applications with image and text analysis  
- Processing of simple to medium visual queries with high performance

### Gemma 3 1B
**Google • 1B parameters • Context: 32,000 tokens**

Ultra-light micro-model designed for deployments on devices with very limited resources.

**Technical specifications:**
- **Speed**: 112 tokens/second ⚡
- **Consumption**: 0.15 kWh/million tokens 🌱
- **License**: Google Gemma Terms of Use
- **Location**: FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Ultra-compact` `Embedded` `Efficient` `Fast`

**Use cases:**
- Deployment on IoT devices and embedded systems with API integration
- Applications requiring local inference on CPU with function calls
- Basic text tasks with instant response time and function calling
- Compact assistants for consumer applications with external service integration
- Intelligent control systems integrating multiple APIs/services

### Lucie-7B-Instruct
**OpenLLM-France • 7B parameters • Context: 32,000 tokens**

Causal multilingual open-source model (7B), fine-tuned from Lucie-7B. Optimized for French.

**Technical specifications:**
- **Speed** : 4 tokens/sec tokens/sec
- **Consumption** : 8.33 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `French` `Open-Source` `Efficient`

### Mistral Small 3.1
**Mistral AI • 24B parameters • Context: 120,000 tokens**

Compact and reactive model from Mistral AI, specifically designed to provide smooth and relevant conversational assistance with optimal response speed.

**Technical specifications:**
- **Speed** : 35 tokens/second
- **Consumption** : 3.72 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Vision` `Agent` `Security`

**Use cases:**
- Conversational applications
- Virtual assistants combining image and text analysis (26 tokens/s)
- Technical support chatbots with access to technical documentation
- Content creation/editing tools with immediate response (blogs, emails)
- Deployment on standard infrastructure (24B parameters)

### Mistral Small 3.2
**Mistral AI • 24B parameters • Context: 120,000 tokens**

Minor update of Mistral Small 3.1, improving instruction following, function calling robustness, and reducing repetition errors.

**Technical specifications:**
- **Speed** : 35 tokens/second tokens/second
- **Consumption** : 3.72 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Vision` `Agent` `Security` `Instruction Following`

**Use cases:**
- Conversational agents with improved instruction following
- Robust integration with external tools via function calling
- Applications requiring high reliability to avoid repetitions
- Use cases identical to Mistral Small 3.1 with improved performance

---

### Mistral Small 3.2
**Mistral AI • 24B parameters • Context: 120,000 tokens**

Minor update of Mistral Small 3.1, improving instruction following, function calling robustness, and reducing repetition errors.

**Technical specifications:**
- **Speed** : 50 tokens/second
- **Consumption** : 5.28 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Vision` `Agent` `Security` `Instruction Following`

**Use cases:**
- Conversational agents with improved instruction following
- Robust integration with external tools via function calling
- Applications requiring high reliability to avoid repetitions
- Use cases identical to Mistral Small 3.1 with improved performance

### DeepCoder
**Agentica x Together AI • 14B parameters • Context: 32,000 tokens**

Open source AI model (14B) by Together AI & Agentica, a credible alternative to proprietary models for code generation.

**Technical specifications:**
- **Speed** : 64 tokens/second ⚡
- **Consumption** : 4.12 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Programming` `Reasoning` `Open-Source` `Mathematics` `Fast`

**Use cases:**
- Code generation in over 15 languages with performance optimization
- Debugging and refactoring of existing codebases with impact analysis
- Implementation of complex algorithms (graphs, trees, heuristics)
- Automated creation of unit tests with code coverage > 80%
- Code transposition between languages/frameworks (e.g., Python to JavaScript)

### Granite 3.2 Vision
**IBM • 2B parameters • Context: 16 384 tokens**

Revolutionary compact model from IBM specialized in computer vision, capable of analyzing and understanding visual documents directly without requiring intermediate OCR technologies.

**Technical specifications:**
- **Speed**: 48 tokens/second
- **Consumption**: 0.69 kWh/million tokens 🌱
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Vision` `Security` `Compact` `Efficient`

**Use cases:**
- Structured data extraction from invoices and forms without OCR
- Direct analysis of tables and charts with trend interpretation
- Reading and interpreting technical diagrams (electrical, mechanical)
- Processing handwritten documents with high recognition rates
- Lightweight computer vision (2B parameters) with high speed (50 tokens/s)

### Granite 3.3 8B
**IBM • 8B Parameters • Context: 60,000 tokens**

Granite 8B model fine-tuned by IBM for improved reasoning and instruction following, with a 128k token context.

**Technical Specifications:**
- **Speed** : 30 tokens/second
- **Consumption** : 1.11 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ✅ Security

**Tags:** `Agent` `Reasoning` `Security` `Efficient`

**Use Cases:**
- General instruction-following tasks (classification, extraction, Q&A)
- Multilingual AI assistants (12 languages)
- Processing of very long documents (128k tokens) for summary, Q&A, etc.
- Code generation/completion with Fill-in-the-Middle
- Integration with external tools via function calling
- Structured reasoning with the "Thinking" mode

### Granite 3.3 2B
**IBM • 2B parameters • Context: 120,000 tokens**

Granite 2B model fine-tuned by IBM, optimized for reasoning and instruction following, with a 128k token context.

**Technical specifications:**
- **Speed**: 45 tokens/second tokens/second
- **Consumption**: 0.74 kWh/million tokens 🌱
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ✅ Security

**Tags:** `Agent` `Reasoning` `Security` `Efficient`

**Use cases:**
- Lightweight deployments with large context (128k tokens)
- General instruction-following tasks on limited resources
- Compact multilingual AI assistants
- Processing of long documents on less powerful devices
- FIM code generation/completion on standard workstations

### Magistral 24B
**Mistral AI • 24B parameters • Context: 40,000 tokens**

The first reasoning model from Mistral AI, excelling in domain-specific reasoning, transparent and multilingual.

**Technical specifications:**
- **Speed** : 25 tokens/second tokens/second
- **Consumption** : 5.33 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ✅ Security

**Tags:** `Reasoning` `Multilingual`

**Use cases:**
- Strategy and business operations (risk modeling)
- Regulated industries (legal, finance) with traceable reasoning
- Software engineering (project planning, architecture)
- Content creation and communication (creative writing, storytelling)

### Granite 3.1 MoE
**IBM • 3B parameters • Context: 32,000 tokens**

Innovative IBM model using the Mixture-of-Experts (MoE) architecture to deliver exceptional performance while drastically optimizing computational resource usage.

**Technical specifications:**
- **Speed** : 74 tokens/second tokens/second ⚡
- **Consumption** : 0.45 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Agent` `Security` `Fast` `MoE` `Efficiency` `Efficient`

**Use cases:**
- General-purpose applications with optimized inference cost (42 tokens/second)
- Document processing in CPU environments with limited RAM usage
- Specialized analyses with dynamic activation of relevant parts of the model
- High-density deployments with low energy consumption per inference
- Parallel processing of multiple query types with MoE specialization

### cogito:14b
**Deep Cogito • 14B parameters • Context: 32,000 tokens**

Deep Cogito model specifically designed to excel in deep reasoning tasks and nuanced contextual understanding, ideal for sophisticated analytical applications.

**Technical specifications:**
- **Speed** : 60 tokens/second ⚡
- **Consumption** : 4.4 kWh/million tokens
- **License** : LLAMA 3.2 Community Licence
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Comprehension` `Analysis` `Fast`

**Use cases:**
- Semantic analysis of texts with identification of implicit implications
- Structured causal reasoning with identification of cause-effect relationships
- Synthesis of complex documents with extraction of key information
- Precise question-answering systems on specialized document corpora
- Argumentative analysis with evaluation of the soundness of reasoning

### Cogito 32B
**Deep Cogito • 32B parameters • Context: 32,000 tokens**

Advanced version of the Cogito model offering significantly enhanced reasoning and analysis capabilities, designed for the most demanding applications in analytical artificial intelligence.

**Technical specifications:**
- **Speed**: 32 tokens/second
- **Consumption**: 8.25 kWh/million tokens
- **License**: LLAMA 3.2 Community Licence
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Comprehension` `Analysis`

**Use cases:**
- Multi-factorial scenario analysis with probabilistic result evaluation
- Scientific problem resolution with formal step demonstration
- High-criticality applications requiring precision and result verifiability
- Expert systems in specialized domains (legal, medical, technical)
- Multi-step reasoning analysis with complete conclusion explainability

### Qwen3 32B
**Qwen Team • 32B parameters • Context: 40,000 tokens**

Powerful next-generation Qwen3 model offering advanced capabilities in reasoning, code, and agentics with an extended context.

**Technical specifications:**
- **Speed**: 18 tokens/second tokens/second
- **Consumption**: 7.41 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Multilingual` `Large Context`

**Use cases:**
- Advanced conversational agents with large context and tool integration (MCP)
- Complex problem solving (math, code) with "Thinking" mode
- Analysis and generation of large documents
- Multilingual applications (>100 languages) requiring deep understanding

### QwQ-32B
**Qwen Team • 32B parameters • Context: 32,000 tokens**

32 billion parameter model enhanced by reinforcement learning (RL) to excel in reasoning, coding, mathematics, and agent tasks.

**Technical Specifications:**
- **Speed**: 35 tokens/second tokens/second
- **Consumption**: 7.54 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Coding` `Mathematics`

**Use Cases:**
- Complex problem resolution requiring reasoning and tool usage
- Code generation and execution with result verification
- Advanced math tasks with accuracy verification
- Agent applications capable of interacting with the environment
- Enhanced instruction following and human preference alignment

### DeepSeek-R1 14B
**DeepSeek AI • 14B parameters • Context: 32,000 tokens**

Compact and efficient version of the DeepSeek-R1 model, offering an excellent balance between performance and lightweight for deployments requiring flexibility and responsiveness.

**Technical specifications:**
- **Speed**: 62 tokens/second ⚡
- **Consumption**: 4.26 kWh/million tokens
- **License**: MIT license
- **Location**: FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Compact` `Versatile` `Fast`

**Use cases:**
- General-purpose applications with needs for fast inference (44 tokens/s)
- Deployments on standard servers without specialized GPU (14B parameters)
- Text processing with contextual analysis and fast response times
- Deployment on edge computing with optimized local inference
- Rapid prototyping of AI applications with short iteration times

### DeepSeek-R1 32B  
**DeepSeek AI • 32B parameters • Context: 32,000 tokens**  

Intermediate version of the DeepSeek-R1 model offering a strategic balance between the advanced capabilities of the 70B version and the efficiency of the 14B version, for optimal versatility and performance.  

**Technical specifications:**  
- **Speed**: 33 tokens/second  
- **Consumption**: 7.99 kWh/million tokens  
- **License**: MIT License  
- **Location**: FR 🇫🇷  

**Capabilities:**  
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security  

**Tags:** `Reasoning` `Versatile`  

**Use cases:**  
- Applications requiring a good power/cost balance (32B parameters)  
- Professional text processing with semantic nuance analysis  
- Automated generation of structured reports from raw data  
- Applications combining data analysis and content generation  
- Specialized assistants for technical fields (legal, medical, technical)

### Cogito 3B
**Deep Cogito • 3B parameters • Context: 32,000 tokens**

Compact version of the Cogito model, optimized for reasoning on devices with limited resources.

**Technical specifications:**
- **Speed** : 55 tokens/second ⚡
- **Consumption** : 0.61 kWh/million tokens 🌱
- **License** : LLAMA 3.2 Community Licence
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Compact` `Embedded` `Efficient` `Fast`

### Granite Embedding  
**IBM • 278M parameters • Context: 512 tokens**  

Ultra-lightweight IBM embedding model for semantic search and classification.  

**Technical specifications:**  
- **License** : Apache 2.0  
- **Location** : FR 🇫🇷  

**Capabilities:**  
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security  

**Tags:** `Embedding` `Compact` `Semantic` `Efficient`

### Granite 3 Guardian 2B
**IBM • 2B parameters • Context: 8,192 tokens**

Compact IBM model specialized in security and compliance, detecting risks and inappropriate content.

**Technical specifications:**
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ✅ Security

**Tags :** `Security` `Compliance` `Compact` `Filtering` `Efficient`

### Granite 3 Guardian 8B
**IBM • 8B parameters • Context: 32,000 tokens**

IBM model specialized in security and compliance, offering advanced risk detection capabilities.

**Technical specifications:**
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ✅ Security

**Tags :** `Security` `Compliance` `Filtering`

### Qwen 2.5 0.5B
**Qwen Team • 0.5B parameters • Context: 32,000 tokens**

Ultra-lightweight micro-model from the Qwen 2.5 family, designed for maximum efficiency on constrained devices.

**Technical specifications:**
- **Speed** : 162 tokens/second ⚡
- **Consumption** : 0.1 kWh/million tokens 🌱
- **License** : MIT License
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Ultra-compact` `Fast` `Embedded` `Efficient`

### Qwen 2.5 1.5B
**Qwen Team • 1.5B parameters • Context: 32,000 tokens**

Very compact model from the Qwen 2.5 family, offering a good performance/size balance for lightweight deployments.

**Technical specifications:**
- **Speed**: 102 tokens/second ⚡
- **Consumption**: 0.33 kWh/million tokens 🌱
- **License**: MIT License
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Compact` `Fast` `Embedded` `Efficient`

### Qwen 2.5 14B
**Qwen Team • 14B parameters • Context: 32,000 tokens**

Medium-sized versatile model from the Qwen 2.5 family, with a good performance/resource balance.

**Technical specifications:**
- **Speed**: 61 tokens/second tokens/second ⚡
- **Consumption**: 4.33 kWh/million tokens
- **License**: MIT License
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Versatile` `Multilingual` `Fast`

### Qwen 2.5 32B
**Qwen Team • 32B parameters • Context: 32,000 tokens**

Powerful model from the Qwen 2.5 family, offering advanced capabilities in understanding and generation.

**Technical specifications:**
- **Speed** : 31 tokens/second tokens/second
- **Consumption** : 8.51 kWh/million tokens
- **License** : MIT License
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Versatile` `Multilingual` `Reasoning`

### Qwen 2.5 3B
**Qwen Team • 3B parameters • Context: 32,000 tokens**

Compact and efficient model from the Qwen 2.5 family, suitable for general tasks on limited resources.

**Technical specifications:**
- **Speed**: 64 tokens/second tokens/second ⚡
- **Consumption**: 0.52 kWh/million tokens 🌱
- **License**: MIT license
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Compact` `Fast` `Versatile` `Efficient`

### Qwen3 0.6b
**Qwen Team • 0.6B parameters • Context: 32,000 tokens**

Compact and efficient model from the Qwen3 family, suitable for general tasks on limited resources.

**Technical specifications:**
- **Speed** : 112 tokens/second ⚡
- **Consumption** : 0.15 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Compact` `Fast` `Versatile` `Efficient`

### Qwen3 1.7b
**Qwen Team • 1.7B parameters • Context: 32,000 tokens**

Very compact model from the Qwen3 family, offering a good performance/size balance for lightweight deployments.

**Technical specifications:**
- **Speed**: 88 tokens/second ⚡
- **Consumption**: 0.38 kWh/million tokens 🌱
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Compact` `Fast` `Embedded` `Efficient`

### Qwen3 4b
**Qwen Team • 4B parameters • Context: 32,000 tokens**

Compact model from the Qwen3 family offering excellent performance in a lightweight and cost-effective format.

**Technical specifications:**
- **Speed**: 49 tokens/second
- **Consumption**: 0.68 kWh/million tokens 🌱
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags:** `Compact` `Efficient`

### Qwen3 8b  
**Qwen Team • 8B parameters • Context: 32,000 tokens**  

Qwen3 8B model offering a good balance between performance and efficiency for general tasks.  

**Technical specifications:**  
- **Speed**: 33 tokens/second tokens/second  
- **Consumption**: 1.01 kWh/million tokens 🌱  
- **License**: Apache 2.0  
- **Location**: FR 🇫🇷  

**Capabilities:**  
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security  

**Tags:** `Reasoning` `Agent` `Multilingual` `Efficient`

### Qwen2.5-VL 3B
**Qwen Team • 3.8B parameters • Context: 128,000 tokens**

Compact Vision-Langage model, efficient solution for edge AI.

**Technical Specifications:**
- **Speed**: 65 tokens/second ⚡
- **Consumption**: 0.51 kWh/million tokens 🌱
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Vision` `Agent` `Reasoning` `Fast` `Efficient` `OCR` `Visual Localization` `Edge AI`

### Qwen2.5-VL 7B
**Qwen Team • 7B (8.3B) parameters • Context: 128,000 tokens**

High-performance Vision-Language model that outperforms GPT-4o-mini on certain tasks.

**Technical Specifications:**
- **Speed** : 35 tokens/second tokens/second
- **Consumption** : 0.95 kWh/million tokens 🌱
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ✅ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Vision` `Agent` `Reasoning` `Efficient` `OCR` `Visual Localization`

### Foundation-Sec-8B
**Foundation AI — Cisco • 8B parameters • Context: 16,384 tokens**

Specialized language model for cybersecurity, optimized for efficiency.

**Technical specifications:**
- **Speed** : 21 tokens/second
- **Consumption** : 1.59 kWh/million tokens
- **License** : Apache 2.0
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ✅ Security

**Tags:** `Security` `Compact`

### devstral 24B
**Mistral AI & All Hands AI • 24B parameters • Context: 120,000 tokens**

Devstral is an agentic LLM for software engineering tasks.

**Technical specifications:**
- **Speed**: 45 tokens/second tokens/second
- **Consumption**: 5.86 kWh/million tokens
- **License**: Apache 2.0
- **Location**: FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ❌ Reasoning • ✅ Security

**Tags:** `Agent` `Programming` `Open-Source` `Large Context`

**Use Cases:**
- Exploration and modification of codebases
- Agentic
- European

### Cogito 8B
**Deep Cogito • 8B parameters • Context: 32,000 tokens**

Mid-size model from the Cogito family, offering a good balance between reasoning capabilities and efficiency.

**Technical specifications:**
- **Speed** : 30 tokens/second tokens/second
- **Consumption** : 1.11 kWh/million tokens 🌱
- **License** : LLAMA 3.2 Community Licence
- **Location** : FR 🇫🇷

**Capabilities:**
✅ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Agent` `Reasoning` `Versatile` `Efficient`

### Llama 3.1 8B
**Meta • 8B parameters • Context: 32,000 tokens**

Base model of the Llama 3.1 family, offering strong performance for its size.

**Technical specifications :**
- **Speed** : 31 tokens/second tokens/second
- **Consumption** : 1.08 kWh/million tokens 🌱
- **License** : LLAMA 3.1 Community Licence
- **Location** : FR 🇫🇷

**Capabilities :**
❌ Tools/Agent • ❌ Vision • ❌ Reasoning • ❌ Security

**Tags :** `Versatile` `Efficient`

### Phi-4 Reasoning 14B
**Microsoft • 14B parameters • Context: 32,000 tokens**

Microsoft Phi family model, specialized in complex reasoning and mathematics.

**Technical specifications:**
- **Speed** : 71 tokens/second tokens/second ⚡
- **Consumption** : 3.71 kWh/million tokens
- **License** : MIT License
- **Location** : FR 🇫🇷

**Capabilities:**
❌ Tools/Agent • ❌ Vision • ✅ Reasoning • ❌ Security

**Tags:** `Reasoning` `Mathematics` `Programming` `Fast`

## Recommended Use Cases

### Multilingual Dialogue
Chatbots and assistants capable of communicating in multiple languages with automatic detection, context maintenance throughout the conversation, and understanding of linguistic specifics

**Recommended models:**
- Llama 3.3
- Mistral Small 3.1
- Qwen 2.5
- Granite 3.3

### Long Document Analysis
Processing large documents (>100 pages) with context retention across the entire text, key information extraction, relevant summary generation, and answering specific questions about the content

**Recommended Models:**
- Gemma 3
- DeepSeek-R1
- Granite 3.3

### Programming and Development
Code generation and optimization in multiple languages, debugging, refactoring, full feature development, understanding of complex algorithmic implementations, and unit test creation

**Recommended Models:**
- DeepCoder
- QwQ
- DeepSeek-R1
- Granite 3.3
- Devstral

### Visual Analysis
Direct processing of images and visual documents without OCR pre-processing, interpretation of technical diagrams, graphs, tables, drawings, and photos with generation of detailed textual explanations of the visual content

**Recommended models:**
- Granite 3.2 Vision
- Mistral Small 3.1
- Gemma 3
- Qwen2.5-VL

### Security and Compliance
Applications requiring specific security capabilities; sensitive content filtering, reasoning traceability, GDPR/HDS verification, risk minimization, vulnerability analysis, and compliance with sector regulations

**Recommended models:**
- Granite Guardian
- Granite 3.3
- Devstral
- Mistral Small 3.1
- Magistral 24b
- Foundation-Sec-8B

### Lightweight and Embedded Deployments
Applications requiring minimal resource footprint, deployment on devices with limited capacity, real-time inference on standard CPUs, and integration into embedded systems or IoT

**Recommended Models:**
- Gemma 3
- Granite 3.1 MoE
- Granite Guardian
- Granite 3.3