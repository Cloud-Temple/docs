---
title: Concepts
sidebar_position: 3
---

# Concepts and Architecture LLMaaS

## üèóÔ∏è Technical Architecture

### Cloud Temple Infrastructure

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Technical Architecture of LLMaaS Cloud Temple" />

### Core Components

#### 1. **LLMaaS API Gateway**
- **OpenAI Compatible** : Seamless integration with existing ecosystem
- **Rate Limiting** : Quota management by billing tier
- **Load Balancing** : Intelligent distribution across 12 GPU machines
- **Monitoring** : Real-time metrics and alerting

#### 2. **Authentication Service**
- **Secure API Tokens** 
- **Access Control** : Granular permissions per model
- **Audit Trails** : Full access traceability

## ü§ñ Models and Tokens

### Model Catalog

*Full catalog: [Model List](./models)*

### Token Management

#### **Token Types**
- **Input Tokens** : Your prompt and context
- **Output Tokens** : Response generated by the model
- **System Tokens** : Metadata and instructions

#### **Cost Calculation**
```
Total Cost = (Input Tokens √ó 0.9‚Ç¨/M) + (Output Tokens √ó 4‚Ç¨/M) +  (Reasoning Output Tokens √ó 21‚Ç¨/M)
```

#### **Optimization**
- **Context window** : Reuse conversations to save costs
- **Appropriate models** : Choose size according to complexity
- **Max tokens** : Limit response length

### Tokenization

```python
# Example token estimation
def estimate_tokens(text: str) -> int:
    """Approximate estimation: 1 token ‚âà 4 characters"""
    return len(text) // 4

prompt = "Explain photosynthesis"
response_max = 200  # desired max tokens

estimated_input = estimate_tokens(prompt)  # ~6 tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Estimated cost: {total_cost:.6f}‚Ç¨")
```

## üîí Security and Compliance

### SecNumCloud Qualification

The LLMaaS service runs on a Cloud Temple IaaS infrastructure that has the **SecNumCloud 3.2** qualification from ANSSI, ensuring:

#### **Data Protection**
- **End-to-end encryption** : TLS 1.3 for all exchanges
- **Secure storage** : Data encrypted at rest 
- **Environment isolation**

#### **Digital Sovereignty**
- **France Hosting** : Cloud Temple certified datacenters
- **French Law** : GDPR compliance 
- **No exposure** : No data transfer to foreign clouds and no data storage

#### **Audit and Traceability**
- **Full logs** : All interactions traced
- **Retention** : Stored according to legal policies
- **Compliance** 

### Security Controls

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="LLMaaS Security Controls" />

## üìà Performance and Scalability

### Performance Metrics

#### **Latency**
- **Average response time** : < 2 seconds for 8B models
- **First token time** : < 1500ms
- **Streaming throughput** : 15-100 tokens/second depending on model

#### **Throughput**
- **Concurrent requests** : Up to 1000 requests/minute per tenant
- **Auto scaling** : Real-time charge adaptation according to requested models
- **Availability** : 99.9% monthly availability SLA 

### Real-time Monitoring
Access via **Cloud Temple Console**:
- Usage metrics per model
- Latency and throughput graphs
- Performance threshold alerts
- Request history

## üåê Integration and Ecosystem

### OpenAI Compatibility

The LLMaaS service is **compatible** with the OpenAI API:

```python
# Transparent migration
from openai import OpenAI

# Before (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# After (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="your-cloud-temple-token",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Same code!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Cloud Temple model
    messages=[{"role": "user", "content": "Bonjour"}]
)
```

### Supported Ecosystem

#### **AI Frameworks**
- ‚úÖ **LangChain** : Native integration
- ‚úÖ **Haystack** : Document pipeline
- ‚úÖ **Semantic Kernel** : Microsoft orchestration
- ‚úÖ **AutoGen** : Conversational agents

#### **Development Tools**
- ‚úÖ **Jupyter** : Interactive notebooks
- ‚úÖ **Streamlit** : Rapid web applications
- ‚úÖ **Gradio** : AI user interfaces
- ‚úÖ **FastAPI** : Backend APIs

#### **No-Code Platforms**
- ‚úÖ **Zapier** : Automations
- ‚úÖ **Make** : Visual integrations
- ‚úÖ **Bubble** : Web applications

## üîÑ Model Lifecycle

### Model Updates

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="LLMaaS Model Lifecycle" />

### Versioning Policy

- **Stable models** : Fixed versions available for 6 months
- **Experimental models** : Beta versions for early adopters
- **Deprecation** : 3-month notice before removal
- **Migration** : Professional services available to ensure your transitions

## üí° Best Practices

### Cost Optimization

1. **Model selection**
   ```python
   # Simple task ‚Üí lightweight model
   if task_complexity == "simple":
       model = "llama3.2:3b"  # Cheaper
   else:
       model = "llama3.1:70b"  # More capable
   ```

2. **Context management**
   ```python
   # Reuse conversations
   messages = [
       {"role": "system", "content": "You are an AI assistant."},
       {"role": "user", "content": "Question 1"},
       {"role": "assistant", "content": "Answer 1"},
       {"role": "user", "content": "Question 2"}  # Reuses context
   ]
   ```

3. **Token limitation**
   ```python
   response = client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       max_tokens=100,  # Limits length
       temperature=0.7
   )
   ```

### Performance

1. **Asynchronous requests**
   ```python
   import asyncio
   import aiohttp
   
   async def batch_requests(prompts):
       tasks = [process_prompt(prompt) for prompt in prompts]
       return await asyncio.gather(*tasks)
   ```

2. **Streaming for UX**
   ```python
   # Real-time interface
   for chunk in client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       stream=True
   ):
       if chunk.choices[0].delta.content:
           print(chunk.choices[0].delta.content, end="")
   ```

### Security
1. **Input Validation**
   ```python
   def sanitize_input(user_input: str) -> str:
       # Clean potential injections
       cleaned = user_input.replace("```", "")
       return cleaned[:1000]  # Limit the size
   ```

2. **Error Handling**
   ```python
   try:
       response = client.chat.completions.create(...)
   except Exception as e:
       logger.error(f"LLMaaS error: {e}")
       return "Sorry, temporary error."
   ```