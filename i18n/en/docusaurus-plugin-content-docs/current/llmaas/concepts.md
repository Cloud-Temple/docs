---
title: Concepts
sidebar_position: 3
---

# Concepts and Architecture LLMaaS

## Overview

The **LLMaaS** (Large Language Models as a Service) service from Cloud Temple provides secure and sovereign access to the most advanced artificial intelligence models, with the **SecNumCloud** qualification from ANSSI.

## üèóÔ∏è Technical Architecture

### Cloud Temple Infrastructure

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Technical Architecture of LLMaaS Cloud Temple" />

### Main Components

#### 1. **API Gateway LLMaaS**
- **OpenAI Compatible** : Transparent integration with existing ecosystem
- **Rate Limiting** : Quota management by billing tier
- **Load Balancing** : Intelligent distribution on 12 GPU machines
- **Monitoring** : Real-time metrics and alerting

#### 2. **Authentication Service**
- **Secure API Tokens**: Automatic rotation
- **Access Control**: Granular permissions by model
- **Audit Trails**: Full access traceability

## ü§ñ Models and Tokens

### Model Catalog

*Complete Catalog: [List of Models](./models)*

### Token Management

#### **Types of Tokens**
- **Input Tokens** : Your prompt and context
- **Output Tokens** : Response generated by the model
- **System Tokens** : Metadata and instructions

#### **Cost Calculation**
```
Total cost = (Input tokens √ó 0.9‚Ç¨/M) + (Output tokens √ó 4‚Ç¨/M) +  (Reasoning output tokens √ó 21‚Ç¨/M)
```

#### **Optimization**
- **Context window** : Reuse conversations to save
- **Appropriate models** : Choose the size according to complexity
- **Max tokens** : Limit the length of responses

### Tokenization

```python
```

# Example of token estimation
def estimate_tokens(text: str) -> int:
    """Approximate estimation: 1 token ‚âà 4 characters"""
    return len(text) // 4

prompt = "Explain photosynthesis"
response_max = 200  # maximum desired tokens

estimated_input = estimate_tokens(prompt)  # ~6 tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Estimated cost: {total_cost:.6f}‚Ç¨")
```

## üîí Security and Compliance

### SecNumCloud Qualification

The LLMaaS service is hosted on a technical infrastructure that benefits from the **SecNumCloud 3.2** qualification from ANSSI, guaranteeing:

#### **Data Protection**
- **End-to-end Encryption** : TLS 1.3 for all exchanges
- **Secure Storage** : Data encrypted at rest (AES-256)
- **Isolation** : Dedicated environments per tenant

#### **Digital Sovereignty**
- **France Hosting** : Cloud Temple certified Datacenters
- **French Law** : Native GDPR Compliance
- **No Exposure** : No transfer to foreign clouds

#### **Audit and Traceability**
- **Full logs** : All interactions traced
- **Retention** : Retention according to legal policies
- **Compliance** : Audit reports available

### Security Controls

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Security Controls LLMaaS" />

## üìà Performance and Scalability

### Performance Metrics

    In progress

#### **Latency**

    In progress

#### **Bandwidth**

    In progress

### Real-Time Monitoring

Access via **Console Cloud Temple**:
- Usage metrics per model
- Latency and throughput graphs
- Performance threshold alerts
- Request history

## üåê Integration and Ecosystem

### OpenAI Compatibility

The LLMaaS service is **compatible** with the OpenAI API:

```python
```

# Transparent migration
from openai import OpenAI

# Before (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# After (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="votre-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Same code!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Cloud Temple Model
    messages=[{"role": "user", "content": "Bonjour"}]
)
```

### Supported Ecosystem

#### **AI Frameworks**
- ‚úÖ **LangChain** : Native integration
- ‚úÖ **Haystack** : Document pipeline
- ‚úÖ **Semantic Kernel** : Microsoft orchestration
- ‚úÖ **AutoGen** : Conversational agents

#### **Development Tools**
- ‚úÖ **Jupyter** : Interactive Notebooks
- ‚úÖ **Streamlit** : Rapid Web Applications
- ‚úÖ **Gradio** : AI User Interfaces
- ‚úÖ **FastAPI** : Backend APIs

#### **No-Code Platforms**
- ‚úÖ **Zapier** : Automations
- ‚úÖ **Make** : Visual integrations
- ‚úÖ **Bubble** : Web applications

## üîÑ Life Cycle of Models

### Model Updates

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="LLMaaS Model Lifecycle" />

### Versioning Policy

- **Stable Models**: Fixed versions available for 6 months
- **Experimental Models**: Beta versions for early adopters
- **Deprecation**: 3-month notice before removal
- **Migration**: Professional services available to ensure your transitions

## üí° Best Practices

### Cost Optimization

1. **Model Selection**
   ```python
   # Simple task ‚Üí lightweight model
   if task_complexity == "simple":
       model = "llama3.2:3b"  # Cheaper
   else:
       model = "llama3.1:70b"  # More capable
   ```

2. **Context Management**
   ```python
   # Reuse conversations
   messages = [
       {"role": "system", "content": "You are an AI assistant."},
       {"role": "user", "content": "Question 1"},
       {"role": "assistant", "content": "Answer 1"},
       {"role": "user", "content": "Question 2"}  # Reuses context
   ]
   ```

3. **Token Limitation**
   ```python
   response = client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       max_tokens=100,  # Limits the length
       temperature=0.7
   )
   ```

### Performance

1. **Asynchronous Requests**
   ```python
   import asyncio
   import aiohttp
   
   async def batch_requests(prompts):
       tasks = [process_prompt(prompt) for prompt in prompts]
       return await asyncio.gather(*tasks)
   ```

2. **Streaming for UX**
   ```python
   # Real-time Interface
   for chunk in client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       stream=True
   ):
       if chunk.choices[0].delta.content:
           print(chunk.choices[0].delta.content, end="")
   ```

### Security

1. **Input Validation**
   ```python
   def sanitize_input(user_input: str) -> str:
       # Clean potential injections
       cleaned = user_input.replace("```", "")
       return cleaned[:1000]  # Limit the size
   ```

2. **Error Handling**
   ```python
   try:
       response = client.chat.completions.create(...)
   except Exception as e:
       logger.error(f"LLMaaS error: {e}")
       return "Sorry, temporary error."
   ```