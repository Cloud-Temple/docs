---
title: Concepts
sidebar_position: 3
---

# Concepts and Architecture LLMaaS

## üèóÔ∏è Technical Architecture

### Cloud Temple Infrastructure

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Technical Architecture LLMaaS Cloud Temple" />

### Main Components

#### 1. **API Gateway LLMaaS**
- **Compatible with OpenAI** : Transparent integration with existing ecosystem
- **Rate Limiting** : Quota management by billing tier
- **Load Balancing** : Intelligent distribution across 12 GPU machines
- **Monitoring** : Real-time metrics and alerting

#### 2. **Authentication Service**
- **Secure API Tokens** 
- **Access Control**: Granular permissions by model
- **Audit trails**: Full access traceability

## ü§ñ Models and Tokens

### Model Catalog

*Complete Catalog: [Model List](./models)*

### Token Management

#### **Token Types**
- **Input Tokens**: Your prompt and context
- **Output Tokens**: Response generated by the model
- **System Tokens**: Metadata and instructions

#### **Cost Calculation**
```
Total Cost = (Input Tokens √ó 0.9‚Ç¨/M) + (Output Tokens √ó 4‚Ç¨/M) +  (Reasoning Output Tokens √ó 21‚Ç¨/M)
```

#### **Optimization**
- **Context window**: Reuse conversations to save
- **Appropriate models**: Choose the size according to complexity
- **Max tokens**: Limit the length of responses

### Tokenization

```python

# Example of token estimation
def estimate_tokens(text: str) -> int:
    """Approximate estimation: 1 token ‚âà 4 characters"""
    return len(text) // 4

prompt = "Expliquez la photosynth√®se"
response_max = 200  # maximum desired tokens

estimated_input = estimate_tokens(prompt)  # ~6 tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Estimated cost: {total_cost:.6f}‚Ç¨")
```

## üîí Security and Compliance

### SecNumCloud Qualification

The LLMaaS service is hosted on a Cloud Temple IaaS infrastructure that benefits from the **SecNumCloud 3.2** qualification from ANSSI, guaranteeing :

#### **Data Protection**
- **End-to-end Encryption** : TLS 1.3 for all exchanges
- **Secure Storage** : Data encrypted at rest 
- **Environment isolation**

#### **Digital Sovereignty**
- **France Hosting** : Certified Cloud Temple Datacenters
- **French Law** : GDPR Compliance 
- **No Exposure** : No transfers to foreign clouds and no data storage

#### **Audit and Traceability**
- **Full Logs** : All interactions traced
- **Retention** : Retention according to legal policies
- **Compliance**

### Security Controls

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Security Controls LLMaaS" />

## üìà Performance and Scalability

### Performance Metrics

#### **Latency**
- **Average response time** : < 2 seconds for 8B models
- **First token time** : < 1500ms
- **Streaming throughput** : 15-100 tokens/second depending on model

#### **Throughput**
- **Concurrent requests** : Up to 1000 requests/minute per tenant
- **Auto-scaling** : Real-time load adaptation according to requested models
- **Availability** : 99.9% monthly availability SLA target

### Real-Time Monitoring

Access via **Console Cloud Temple**:
- Usage metrics per model
- Latency and throughput graphs
- Performance threshold alerts
- Request history

## üåê Integration and Ecosystem

### OpenAI Compatibility

The LLMaaS service is **compatible** with the OpenAI API :

```python

# Transparent Migration
from openai import OpenAI

# Before (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# After (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="votre-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Same code !
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Cloud Temple Model
    messages=[{"role": "user", "content": "Bonjour"}]
)
```

### Supported Ecosystem

#### **AI Frameworks**
- ‚úÖ **LangChain** : Native integration
- ‚úÖ **Haystack** : Document pipeline
- ‚úÖ **Semantic Kernel** : Microsoft orchestration
- ‚úÖ **AutoGen** : Conversational agents

#### **Development Tools**
- ‚úÖ **Jupyter** : Interactive notebooks
- ‚úÖ **Streamlit** : Rapid web applications
- ‚úÖ **Gradio** : AI user interfaces
- ‚úÖ **FastAPI** : Backend APIs

#### **No-Code Platforms**
- ‚úÖ **Zapier** : Automations
- ‚úÖ **Make** : Visual integrations
- ‚úÖ **Bubble** : Web applications

## üîÑ Life Cycle of Models

### Model Updates

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="LLMaaS Model Lifecycle" />

### Versioning Policy

- **Stable Models**: Fixed versions available for 6 months
- **Experimental Models**: Beta versions for early adopters
- **Deprecation**: 3-month notice before removal
- **Migration**: Professional services available to ensure your transitions

## üí° Best Practices

### Cost Optimization

1. **Model Selection**
   ```python
   # Simple task ‚Üí lightweight model
   if task_complexity == "simple":
       model = "llama3.2:3b"  # Cheaper
   else:
       model = "llama3.1:70b"  # More capable
   ```

2. **Context Management**
   ```python
   # Reuse conversations
   messages = [
       {"role": "system", "content": "You are an AI assistant."},
       {"role": "user", "content": "Question 1"},
       {"role": "assistant", "content": "Answer 1"},
       {"role": "user", "content": "Question 2"}  # Reuse the context
   ]
   ```

3. **Token Limitation**
   ```python
   response = client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       max_tokens=100,  # Limits the length
       temperature=0.7
   )
   ```

### Performance

1. **Asynchronous Requests**
   ```python
   import asyncio
   import aiohttp
   
   async def batch_requests(prompts):
       tasks = [process_prompt(prompt) for prompt in prompts]
       return await asyncio.gather(*tasks)
   ```

2. **Streaming for UX**
   ```python
   # Real-time interface
   for chunk in client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       stream=True
   ):
       if chunk.choices[0].delta.content:
           print(chunk.choices[0].delta.content, end="")
   ```

### Security

1. **Input Validation**
   ```python
   def sanitize_input(user_input: str) -> str:
       # Clean potential injections
       cleaned = user_input.replace("```", "")
       return cleaned[:1000]  # Limit the size
   ```

2. **Error Handling**
   ```python
   try:
       response = client.chat.completions.create(...)
   except Exception as e:
       logger.error(f"LLMaaS error: {e}")
       return "Sorry, temporary error."
   ```