---
title: Concepts
sidebar_position: 3
---

# Concepts and Architecture LLMaaS

## Overview

The **LLMaaS** (Large Language Models as a Service) service from Cloud Temple provides secure and sovereign access to the most advanced AI models, with the **SecNumCloud** certification from ANSSI.

## üèóÔ∏è Technical Architecture

### Cloud Temple Infrastructure

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="LLMaaS Cloud Temple Technical Architecture" />

### Main Components

#### 1. **API Gateway LLMaaS**
- **OpenAI Compatible** : Transparent integration with existing ecosystem
- **Rate Limiting** : Quota management by billing tier
- **Load Balancing** : Intelligent distribution across 12 GPU machines
- **Monitoring** : Real-time metrics and alerting

#### 2. **Authentication Service**
- **Secure API Tokens**: Automatic Rotation
- **Access Control**: Granular Permissions by Model
- **Audit Trails**: Full Access Traceability

## ü§ñ Models and Tokens

### Model Catalog

*Complete Catalog : [List of models](./models)*

### Token Management

#### **Token Types**
- **Input Tokens** : Your prompt and context
- **Output Tokens** : Response generated by the model
- **System Tokens** : Metadata and instructions

#### **Cost Calculation**
```
Total cost = (Input Tokens √ó 0.9‚Ç¨/M) + (Output Tokens √ó 4‚Ç¨/M) +  (Reasoning Output Tokens √ó 21‚Ç¨/M)
```

#### **Optimisation**
- **Context window** : Reuse conversations to save
- **Mod√®les appropri√©s** : Choose the size according to complexity
- **Max tokens** : Limit the length of responses

### Tokenization

```python

# Example token estimation
def estimate_tokens(text: str) -> int:
    """Approximate estimation: 1 token ‚âà 4 characters"""
    return len(text) // 4

prompt = "Explain photosynthesis"
response_max = 200  # max desired tokens

estimated_input = estimate_tokens(prompt)  # ~6 tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Estimated cost: {total_cost:.6f}‚Ç¨")
```

## üîí Security and Compliance

### SecNumCloud Qualification

The LLMaaS service is hosted on a technical infrastructure that benefits from the **SecNumCloud 3.2** qualification from ANSSI, guaranteeing:

#### **Data Protection**
- **End-to-end Encryption** : TLS 1.3 for all exchanges
- **Secure Storage** : Data encrypted at rest (AES-256)
- **Isolation** : Dedicated environments per tenant

#### **Digital Sovereignty**
- **France Hosting** : Cloud Temple certified Datacenters
- **French Law** : Native GDPR Compliance
- **No Exposure** : No transfer to foreign clouds

#### **Audit and Traceability**
- **Full logs** : All traced interactions
- **Retention** : Retention according to legal policies
- **Compliance** : Audit reports available

### Security Controls

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Security Controls LLMaaS" />

### Prompt Security

Prompt analysis is a **native and integrated** security feature of the LLMaaS platform. Enabled by default, it aims to detect and prevent "jailbreak" attempts or malicious prompt injections before they reach the model. This protection relies on a multi-layered approach.

:::tip Contact support for deactivation
It is possible to disable this security analysis for very specific use cases, although this is not recommended. For any questions on this topic or to request deactivation, please contact Cloud Temple support.
:::

#### 1. Structural Analysis (`check_structure`)
- **Malformed JSON Check**: The system detects if the prompt starts with a `{` and tries to parse it as JSON. If parsing succeeds and the JSON contains suspicious keywords (e.g., "system", "bypass"), or if parsing fails unexpectedly, this may indicate an injection attempt.
- **Unicode Normalization**: The prompt is normalized using `unicodedata.normalize('NFKC', prompt)`. If the original prompt differs from its normalized version, this may indicate the use of deceptive Unicode characters (homoglyphs) to bypass filters. For example, "–∞dmin" (Cyrillic) instead of "admin" (Latin).

#### 2. Suspect Pattern Detection (`check_patterns`)
- The system uses regular expressions (`regex`) to identify known prompt attack patterns, in several languages (French, English, Chinese, Japanese).
- **Examples of detected patterns**:
    - **System Commands**: Keywords like "ignore the instructions", "ignore instructions", "ÂøΩÁï•Êåá‰ª§", "ÊåáÁ§∫„ÇíÁÑ°Ë¶ñ".
    - **HTML Injection**: Hidden or malicious HTML tags, for example `<div hidden>`, `<hidden div>`.
    - **Markdown Injection**: Malicious Markdown links, for example `[texte](javascript:...)`, `[text](data:...)`.
    - **Repeated Sequences**: Excessive repetition of words or phrases like "forget forget forget", "forget forget forget".
    - **Special/Mixed Characters**: Use of unusual Unicode characters or script mixing to hide commands (e.g., "s\u0443st√®me").

#### 3. Behavioral Analysis (`check_behavior`)
- The load balancer maintains a history of recent prompts.
- **Fragmentation Detection**: It combines recent prompts to see if an attack is fragmented across multiple requests. For example, if "ignore" is sent in a prompt and "instructions" in the next, the system can detect them together.
- **Repetition Detection**: It identifies if the same prompt is repeated excessively. The current threshold for repetition detection is **30 consecutive identical prompts**.

This multi-layer approach allows detecting a wide range of prompt attacks, from the simplest to the most sophisticated, by combining static content analysis and dynamic behavior analysis.

## üìà Performance and Scalability

### Real-Time Monitoring

Access via **Console Cloud Temple**:
- Usage metrics per model
- Latency and throughput graphs
- Performance threshold alerts
- Request history

## üåê Integration and Ecosystem

### OpenAI Compatibility

The LLMaaS service is **compatible** with the OpenAI API :

```python

# Transparent Migration
from openai import OpenAI

# Before (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# After (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="votre-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Same code!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Cloud Temple Model
    messages=[{"role": "user", "content": "Bonjour"}]
)
```

### Supported Ecosystem

#### **AI Frameworks**
- ‚úÖ **LangChain** : Native Integration
- ‚úÖ **Haystack** : Document Pipeline
- ‚úÖ **Semantic Kernel** : Microsoft Orchestration
- ‚úÖ **AutoGen** : Conversational Agents

#### **Development Tools**
- ‚úÖ **Jupyter** : Interactive notebooks
- ‚úÖ **Streamlit** : Rapid web applications
- ‚úÖ **Gradio** : AI user interfaces
- ‚úÖ **FastAPI** : Backend APIs

#### **No-Code Platforms**
- ‚úÖ **Zapier** : Automations
- ‚úÖ **Make** : Visual Integrations
- ‚úÖ **Bubble** : Web Applications

## üîÑ Lifecycle of Models

### Model Updates

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="Life Cycle of LLMaaS Models" />

### Versioning Policy

- **Stable Models**: Fixed versions available for 6 months
- **Experimental Models**: Beta versions for early adopters
- **Deprecation**: 3-month notice before removal
- **Migration**: Professional services available to ensure your transitions

### Forecasted Lifecycle Plan

The table below presents the forecasted lifecycle of our models. The generative AI ecosystem evolves very quickly, which explains lifecycles that may seem short. Our goal is to provide you with the most performant models available.

However, we are committed to preserving models that are most used by our customers over time. For critical use cases requiring long-term stability, **extended support** phases are possible. Please **contact support** to discuss your specific needs.

This planning is provided for information purposes only and is **reviewed at the beginning of each quarter**.

- **DMP (Go Live Date)**: The date when the model becomes available in production.
- **DSP (End of Support Date)**: The forecasted date from which the model will no longer be maintained. A 3-month notice period is respected before any effective deletion.

| Model                  | Publisher                 | Phase      | DMP        | DSP        |
| :--------------------- | :------------------------ | :--------- | :--------- | :--------- |
| deepcoder:14b          | Agentica x Together AI    | Production | 13/06/2025 | 30/06/2026 |
| cogito:14b             | Deep Cogito               | Production | 13/06/2025 | 30/06/2026 |
| cogito:32b             | Deep Cogito               | Production | 13/06/2025 | 30/06/2026 |
| cogito:3b              | Deep Cogito               | Production | 13/06/2025 | 30/06/2026 |
| cogito:8b              | Deep Cogito               | Production | 13/06/2025 | 30/06/2026 |
| deepseek-r1:14b        | DeepSeek AI               | Production | 13/06/2025 | 31/12/2025 |
| deepseek-r1:32b        | DeepSeek AI               | Production | 13/06/2025 | 31/12/2025 |
| deepseek-r1:671b       | DeepSeek AI               | Production | 13/06/2025 | 31/12/2025 |
| deepseek-r1:70b        | DeepSeek AI               | Production | 13/06/2025 | 31/12/2025 |
| foundation-sec:8b      | Foundation AI ‚Äî Cisco     | Production | 13/06/2025 | 30/09/2025 |
| gemma3:12b             | Google                    | Production | 13/06/2025 | 31/12/2026 |
| gemma3:1b              | Google                    | Production | 13/06/2025 | 31/12/2026 |
| gemma3:27b             | Google                    | Production | 13/06/2025 | 31/12/2026 |
| gemma3:4b              | Google                    | Production | 13/06/2025 | 31/12/2026 |
| granite-embedding:278m | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3-guardian:2b   | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3-guardian:8b   | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3.1-moe:3b      | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3.2-vision:2b   | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3.3:2b          | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3.3:8b          | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| llama3.1:8b            | Meta                      | Production | 13/06/2025 | 31/12/2025 |
| llama3.3:70b           | Meta                      | Production | 13/06/2025 | 31/12/2026 |
| phi4-reasoning:14b     | Microsoft                 | Production | 13/06/2025 | 31/12/2025 |
| magistral:24b          | Mistral AI                | Production | 13/06/2025 | 31/12/2026 |
| mistral-small3.1:24b   | Mistral AI                | Production | 13/06/2025 | 31/12/2026 |
| mistral-small3.2:24b   | Mistral AI                | Production | 23/06/2025 | 30/03/2026 |
| devstral:24b           | Mistral AI & All Hands AI | Production | 13/06/2025 | 31/12/2026 |
| lucie-instruct:7b      | OpenLLM-France            | Production | 13/06/2025 | 30/10/2025 |
| qwen2.5:0.5b           | Qwen Team                 | Production | 13/06/2025 | 31/12/2025 |
| qwen2.5:1.5b           | Qwen Team                 | Production | 13/06/2025 | 31/12/2025 |
| qwen2.5:14b            | Qwen Team                 | Production | 13/06/2025 | 31/12/2025 |
| qwen2.5:32b            | Qwen Team                 | Production | 13/06/2025 | 31/12/2025 |
| qwen2.5:3b             | Qwen Team                 | Production | 13/06/2025 | 31/12/2025 |
| qwen2.5vl:32b          | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:3b           | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:72b          | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:7b           | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:0.6b             | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:1.7b             | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:14b              | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:30b-a3b          | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:4b               | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:8b               | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:32b              | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:235b             | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwq:32b                | Qwen Team                 | Production | 13/06/2025 | 31/12/2025 |

## üí° Best Practices

To get the most out of the LLMaaS API, it is essential to adopt cost, performance, and security optimization strategies.

### Cost Optimization

Mastering costs relies on the intelligent use of tokens and models.

1.  **Model Selection**: Don't use a powerful model for a simple task. A larger model is more capable, but it is also slower and consumes much more energy, directly impacting the cost. Adapt the model size to the complexity of your needs for an optimal balance.

    For example, to process a million tokens:
    - **`Gemma 3 1B`** consumes **0.15 kWh**.
    - **`Llama 3.3 70B`** consumes **11.75 kWh**, which is **78 times more**.

    ```python
    # For sentiment classification, a compact model is sufficient and economical.
    if task == "sentiment_analysis":
        model = "granite3.3:2b"
    # For complex legal analysis, a larger model is necessary.
    elif task == "legal_analysis":
        model = "deepseek-r1:70b"
    ```

2.  **Context Management**: The conversation history (`messages`) is returned with each call, consuming input tokens. For long conversations, consider summary or windowing strategies to retain only relevant information.
    ```python
    # For a long conversation, you can summarize the initial exchanges.
    messages = [
        {"role": "system", "content": "You are an AI assistant."},
        {"role": "user", "content": "Summary of the first 10 exchanges..."},
        {"role": "assistant", "content": "Ok, I have the context."},
        {"role": "user", "content": "Here is my new question."}
    ]
    ```

3.  **Output Token Limitation**: Always use the `max_tokens` parameter to avoid excessively long and costly responses. Set a reasonable limit based on what you expect.
    ```python
    # Request a maximum of 100 words.
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Summarize this document..."}],
        max_tokens=150, # Safety margin for ~100 words
    )
    ```

### Performance

The responsiveness of your application depends on how you manage API calls.

1.  **Asynchronous Requests** : To process multiple requests without waiting for each one to finish, use asynchronous calls. This is particularly useful for backend applications handling a large volume of simultaneous requests.
    ```python
    import asyncio
    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key="...", base_url="...")

    async def process_prompt(prompt: str):
        # Process a single request asynchronously
        response = await client.chat.completions.create(model="granite3.3:8b", messages=[{"role": "user", "content": prompt}])
        return response.choices[0].message.content

    async def batch_requests(prompts: list):
        # Launch multiple tasks in parallel and wait for their completion
        tasks = [process_prompt(p) for p in prompts]
        return await asyncio.gather(*tasks)
    ```

2.  **Streaming for User Experience (UX)** : For user interfaces (chatbots, assistants), streaming is essential. It allows displaying the model's response word by word, giving an impression of immediate responsiveness instead of waiting for the complete response.
    ```python
    # Display the response in real-time in a user interface
    response_stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Tell me a story."}],
        stream=True
    )
    for chunk in response_stream:
        if chunk.choices[0].delta.content:
            # Display the text fragment in the UI
            print(chunk.choices[0].delta.content, end="", flush=True)
    ```

### Security

The security of your application is critical, especially when handling user inputs.

1. **Input Validation and Sanitization**: Never trust user inputs. Before sending them to the API, sanitize them to remove any potentially malicious code or "prompt injection" instructions. Also limit their size to prevent abuse.
    ```python
    def sanitize_input(user_input: str) -> str:
        # Simple example: remove code markers and limit length.
        # More robust libraries can be used for advanced sanitization.
        cleaned = user_input.replace("`", "").replace("'", "").replace("\"", "")
        return cleaned[:2000]  # Limit the size to 2000 characters
    ```

2. **Robust Error Handling**: Always wrap your API calls in `try...except` blocks to handle network errors, API errors (e.g., 429 Rate Limit, 500 Internal Server Error) and provide a degraded but functional user experience.
    ```python
    from openai import APIError, APITimeoutError

    try:
        response = client.chat.completions.create(...)
    except APITimeoutError:
        # Handle the case where the request takes too long
        return "The service is taking longer than expected, please try again."
    except APIError as e:
        # Handle specific API errors
        logger.error(f"LLMaaS API Error: {e.status_code} - {e.message}")
        return "Sorry, an error occurred with the AI service."
    except Exception as e:
        # Handle all other errors (network, etc.)
        logger.error(f"An unexpected error occurred: {e}")
        return "Sorry, an unexpected error occurred."
    ```