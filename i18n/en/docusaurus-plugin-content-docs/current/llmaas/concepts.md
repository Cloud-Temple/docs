---
title: Concepts
sidebar_position: 3
---

# Concepts and Architecture of LLMaaS

## Overview

The **LLMaaS** (Large Language Models as a Service) offering from Cloud Temple provides secure and sovereign access to the most advanced artificial intelligence models, with the **SecNumCloud certification** from ANSSI.

## üèóÔ∏è Technical Architecture

### Cloud Infrastructure Temple

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Technical Architecture of LLMaaS Cloud Temple" />

### Main Components

#### 1. **API Gateway LLMaaS**
- **OpenAI Compatible**: Seamless integration with existing ecosystem
- **Rate Limiting**: Quota management per billing tier
- **Load Balancing**: Intelligent distribution across 12 GPU machines
- **Monitoring**: Real-time metrics and alerting

#### 2. **Authentication Service**
- **Secure API Tokens**: Automatic rotation
- **Access Control**: Granular permissions per model
- **Audit Trails**: Full access traceability

## ü§ñ Models and Tokens

### Model Catalog

*Complete catalog: [Model List](./models)*

### Token Management

#### **Token Types**
- **Input tokens**: Your prompt and context
- **Output tokens**: Response generated by the model
- **System tokens**: Metadata and instructions

#### **Cost Calculation**
```
Total cost = (Input tokens √ó 0.9‚Ç¨/M) + (Output tokens √ó 4‚Ç¨/M) + (Reasoning output tokens √ó 21‚Ç¨/M)
```

#### **Optimization**
- **Context window**: Reuse conversations to save costs
- **Appropriate models**: Choose size based on complexity
- **Max tokens**: Limit response length

### Tokenization

```python
# Token Estimation Example
def estimate_tokens(text: str) -> int:
    """Approximate estimation: 1 token ‚âà 4 characters"""
    return len(text) // 4

prompt = "Explain photosynthesis"
response_max = 200  # maximum desired tokens

estimated_input = estimate_tokens(prompt)  # ~6 tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Estimated cost: {total_cost:.6f}‚Ç¨")
```

## üîí Security and Compliance

### SecNumCloud Qualification

The LLMaaS service is hosted on a technical infrastructure that holds the **SecNumCloud 3.2 qualification** from ANSSI, ensuring:

#### **Data Protection**
- **End-to-end Encryption**: TLS 1.3 for all communications
- **Secure Storage**: Data encrypted at rest (AES-256)
- **Isolation**: Dedicated environments per tenant

#### **Digital Sovereignty**
- **Hosting in France**: Cloud Temple datacenters certified
- **French law**: Native GDPR compliance
- **No exposure**: No data transfers to foreign clouds

#### **Audit and Traceability**
- **Complete logs**: All interactions tracked
- **Retention**: Stored according to legal policies
- **Compliance**: Audit reports available

### Security Controls

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Security Controls LLMaaS" />

### Prompt Security

Prompt analysis is a **native and built-in** security feature of the LLMaaS platform. Enabled by default, it aims to detect and prevent attempts at "jailbreaking" or injecting malicious prompts before they even reach the model. This protection is based on a multi-layered approach.

:::tip Contact Support for Disabling
It is possible to disable this security analysis for very specific use cases, although this is not recommended. For any questions regarding this or to request deactivation, please contact Cloud Temple support.
:::

#### 1. Structural Analysis (`check_structure`)
- **Malformed JSON detection**: The system checks whether the prompt starts with a `{` and attempts to parse it as JSON. If parsing succeeds and the JSON contains suspicious keywords (e.g., "system", "bypass"), or if parsing fails unexpectedly, this may indicate an injection attempt.
- **Unicode normalization**: The prompt is normalized using `unicodedata.normalize('NFKC', prompt)`. If the original prompt differs from its normalized version, this may indicate the use of deceptive Unicode characters (homoglyphs) to bypass filters. For example, "–∞dmin" (Cyrillic) instead of "admin" (Latin).

#### 2. Suspicious Pattern Detection (`check_patterns`)
- The system uses regular expressions (`regex`) to identify known attack patterns in prompts, across multiple languages (French, English, Chinese, Japanese).
- **Examples of detected patterns**:
    - **System Commands**: Keywords such as "ignore the instructions", "ignore instructions", "ÂøΩÁï•Êåá‰ª§", "ÊåáÁ§∫„ÇíÁÑ°Ë¶ñ".
    - **HTML Injection**: Hidden or malicious HTML tags, for example `<div hidden>`, `<hidden div>`.
    - **Markdown Injection**: Malicious Markdown links, for example `[text](javascript:...)`, `[text](data:...)`.
    - **Repeated Sequences**: Excessive repetition of words or phrases such as "forget forget forget", "oublie oublie oublie".
    - **Special/Mixed Characters**: Use of unusual Unicode characters or mixing scripts to obfuscate commands (e.g., "s\u0443st√®me").

#### 3. Behavioral Analysis (`check_behavior`)
- The load balancer maintains a history of recent prompts.
- **Fragmentation Detection**: It combines recent prompts to check whether an attack is fragmented across multiple requests. For example, if "ignore" is sent in one prompt and "instructions" in the next, the system can detect them together.
- **Repetition Detection**: It identifies if the same prompt is repeated excessively. The current threshold for repetition detection is 30 consecutive identical prompts.

This multi-layered approach enables detection of a wide range of prompt attacks, from simple to highly sophisticated, by combining static content analysis with dynamic behavioral analysis.

## üìà Performance and Scalability

### Real-Time Monitoring

Access via **Cloud Temple Console**:
- Model usage metrics
- Latency and throughput graphs
- Performance threshold alerts
- Request history

## üåê Integration and Ecosystem

### OpenAI Compatibility

The LLMaaS service is **compatible** with the OpenAI API:

```python
# Transparent migration
from openai import OpenAI

# Before (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# After (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="your-cloud-temple-token",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Same code!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Cloud Temple model
    messages=[{"role": "user", "content": "Hello"}]
)
```

### Supported Ecosystem

#### **AI Frameworks**
- ‚úÖ **LangChain** : Native integration
- ‚úÖ **Haystack** : Document pipelines
- ‚úÖ **Semantic Kernel** : Microsoft orchestration
- ‚úÖ **AutoGen** : Conversational agents

#### **Development Tools**
- ‚úÖ **Jupyter** : Interactive notebooks
- ‚úÖ **Streamlit** : Rapid web applications
- ‚úÖ **Gradio** : AI user interfaces
- ‚úÖ **FastAPI** : Backend APIs

#### **No-Code Platforms**
- ‚úÖ **Zapier** : Automations
- ‚úÖ **Make** : Visual integrations
- ‚úÖ **Bubble** : Web applications

## üîÑ Model Lifecycle

### Model Updates

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="LLMaaS Model Lifecycle" />

### Versioning Policy

- **Stable Models**: Fixed versions available for 6 months  
- **Experimental Models**: Beta versions for early adopters  
- **Deprecation**: 3-month notice before removal  
- **Migration**: Professional services available to support your transitions

### Projected Lifecycle Planning

The table below outlines the projected lifecycle of our models. The generative AI ecosystem evolves rapidly, which explains why lifecycle durations may appear short. Our goal is to provide you with the most performant models available at any given time.

That said, we are committed to maintaining models that are most widely used by our clients over time. For critical use cases requiring long-term stability, extended **support phases** are possible. Please **contact support** to discuss your specific requirements.

This planning is provided for informational purposes only and is **reviewed at the beginning of each quarter**.

- **DMP (Date of Production Launch)**: The date when the model becomes available in production.
- **DSP (Date of Support End)**: The projected date from which the model will no longer be maintained. A 3-month notice period is observed before any actual deprecation.

| Model                  | Publisher                 | Phase      | DMP        | DSP        |
| :--------------------- | :------------------------ | :--------- | :--------- | :--------- |
| deepcoder:14b          | Agentica x Together AI    | Production | 13/06/2025 | 30/06/2026 |
| cogito:32b             | Deep Cogito               | Production | 13/06/2025 | 30/06/2026 |
| cogito:3b              | Deep Cogito               | Production | 13/06/2025 | 30/06/2026 |
| cogito:8b              | Deep Cogito               | Production | 13/06/2025 | 30/06/2026 |
| deepseek-r1:14b        | DeepSeek AI               | Production | 13/06/2025 | 31/12/2025 |
| deepseek-r1:32b        | DeepSeek AI               | Production | 13/06/2025 | 31/12/2025 |
| gemma3:12b             | Google                    | Production | 13/06/2025 | 31/12/2026 |
| gemma3:1b              | Google                    | Production | 13/06/2025 | 31/12/2026 |
| gemma3:27b             | Google                    | Production | 13/06/2025 | 31/12/2026 |
| gemma3:4b              | Google                    | Production | 13/06/2025 | 31/12/2026 |
| granite-embedding:278m | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3-guardian:2b   | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3-guardian:8b   | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3.2-vision:2b   | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3.3:2b          | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| granite3.3:8b          | IBM                       | Production | 13/06/2025 | 31/12/2026 |
| llama3.3:70b           | Meta                      | Production | 13/06/2025 | 31/12/2026 |
| magistral:24b          | Mistral AI                | Production | 13/06/2025 | 31/12/2026 |
| mistral-small3.1:24b   | Mistral AI                | Production | 13/06/2025 | 31/12/2026 |
| mistral-small3.2:24b   | Mistral AI                | Production | 23/06/2025 | 30/03/2026 |
| devstral:24b           | Mistral AI & All Hands AI | Production | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:32b          | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:3b           | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:72b          | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:7b           | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:0.6b             | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:1.7b             | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:14b              | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:30b-a3b          | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:4b               | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:8b               | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:32b              | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwen3:235b             | Qwen Team                 | Production | 13/06/2025 | 31/12/2026 |
| qwq:32b                | Qwen Team                 | Production | 13/06/2025 | 31/12/2025 |

### Depreciated Models

The world of LLMs is evolving rapidly. To ensure our clients have access to the most advanced technologies, we regularly deprecate models that no longer meet current standards or are no longer in use. The models listed below are no longer available on the public platform. However, they can be reactivated for specific projects upon request.

| Model              | Provider              | Status   | Depreciation Date    |
| :----------------- | :-------------------- | :------- | :------------------- |
| cogito:14b         | Deep Cogito           | Deprecated | 17/10/2025           |
| deepseek-r1:671b   | DeepSeek AI           | Deprecated | 17/10/2025           |
| deepseek-r1:70b    | DeepSeek AI           | Deprecated | 17/10/2025           |
| foundation-sec:8b  | Foundation AI ‚Äî Cisco | Deprecated | 17/10/2025           |
| granite3.1-moe:3b  | IBM                   | Deprecated | 17/10/2025           |
| llama3.1:8b        | Meta                  | Deprecated | 17/10/2025           |
| phi4-reasoning:14b | Microsoft             | Deprecated | 17/10/2025           |
| lucie-instruct:7b  | OpenLLM-France        | Deprecated | 17/10/2025           |
| qwen2.5:0.5b       | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:1.5b       | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:14b        | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:32b        | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:3b         | Qwen Team             | Deprecated | 17/10/2025           |

## üí° Best Practices

To get the most out of the LLMaaS API, it is essential to adopt strategies for optimizing costs, performance, and security.

### Cost Optimization

Mastering costs relies on intelligent use of tokens and models.

1.  **Model Selection**: Don't use an overly powerful model for simple tasks. Larger models are more capable, but they are also slower and consume significantly more energy, directly impacting cost. Match the model size to the complexity of your task for optimal balance.

    For example, processing one million tokens:
    - **`Gemma 3 1B`** consumes **0.15 kWh**.
    - **`Llama 3.3 70B`** consumes **11.75 kWh**, which is **78 times more**.

    ```python
    # For sentiment classification, a compact model is sufficient and cost-effective.
    if task == "sentiment_analysis":
        model = "granite3.3:2b"
    # For complex legal analysis, a larger model is required.
    elif task == "legal_analysis":
        model = "deepseek-r1:70b"
    ```

2.  **Context Management**: The conversation history (`messages`) is sent back with every call, consuming input tokens. For long conversations, consider strategies like summarization or windowing to retain only relevant information.
    ```python
    # For long conversations, summarize the initial exchanges.
    messages = [
        {"role": "system", "content": "You are an AI assistant."},
        {"role": "user", "content": "Summary of the first 10 exchanges..."},
        {"role": "assistant", "content": "OK, I have the context."},
        {"role": "user", "content": "Here is my new question."}
    ]
    ```

3.  **Output Token Limitation**: Always use the `max_tokens` parameter to prevent excessively long and costly responses. Set a reasonable limit based on your expected output.
    ```python
    # Request a summary of at most 100 words.
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Summarize this document..."}],
        max_tokens=150,  # Safety margin for ~100 words
    )
    ```

### Performance

The responsiveness of your application depends on how you manage API calls.

1.  **Asynchronous Requests**: To handle multiple requests without waiting for each one to complete, use asynchronous calls. This is especially useful for backend applications processing a large volume of simultaneous requests.
    ```python
    import asyncio
    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key="...", base_url="...")

    async def process_prompt(prompt: str):
        # Processes a single request asynchronously
        response = await client.chat.completions.create(model="granite3.3:8b", messages=[{"role": "user", "content": prompt}])
        return response.choices[0].message.content

    async def batch_requests(prompts: list):
        # Launches multiple tasks in parallel and waits for their completion
        tasks = [process_prompt(p) for p in prompts]
        return await asyncio.gather(*tasks)
    ```

2.  **Streaming for User Experience (UX)**: For user interfaces (chatbots, assistants), streaming is essential. It allows the model's response to be displayed word by word in real time, creating the impression of immediate responsiveness instead of waiting for the full response.
    ```python
    # Displays the response in real time in a user interface
    response_stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Tell me a story."}],
        stream=True
    )
    for chunk in response_stream:
        if chunk.choices[0].delta.content:
            # Display the text chunk in the UI
            print(chunk.choices[0].delta.content, end="", flush=True)
    ```

### Security

The security of your application is critical, especially when handling user inputs.

1.  **Input Validation and Sanitization**: Never trust user inputs. Before sending them to the API, sanitize them to remove any potentially malicious code or "prompt injection" instructions. Also, limit their size to prevent abuse.
    ```python
    def sanitize_input(user_input: str) -> str:
        # Simple example: remove code delimiters and limit length.
        # More robust libraries can be used for advanced sanitization.
        cleaned = user_input.replace("`", "").replace("'", "").replace("\"", "")
        return cleaned[:2000]  # Limit length to 2000 characters
    ```

2.  **Robust Error Handling**: Always wrap your API calls in `try...except` blocks to handle network errors, API errors (e.g., 429 Rate Limit, 500 Internal Server Error), and provide a degraded but functional user experience.
    ```python
    from openai import APIError, APITimeoutError

    try:
        response = client.chat.completions.create(...)
    except APITimeoutError:
        # Handle case where the request takes too long
        return "The service is taking longer than expected, please try again."
    except APIError as e:
        # Handle specific API errors
        logger.error(f"LLMaaS API Error: {e.status_code} - {e.message}")
        return "Sorry, an error occurred with the AI service."
    except Exception as e:
        # Handle all other errors (network, etc.)
        logger.error(f"An unexpected error occurred: {e}")
        return "Sorry, an unexpected error occurred."
    ```
