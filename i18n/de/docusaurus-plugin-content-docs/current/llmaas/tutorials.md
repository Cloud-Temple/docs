---
title: Tutorials
sidebar_position: 6
---

# LLMaaS-Tutorials

## √úbersicht

Diese erweiterten Tutorials behandeln Integration, Optimierung und Best Practices, um LLMaaS Cloud Temple in der Produktion voll auszusch√∂pfen. Jedes Tutorial enth√§lt getesteten Code und reale Performance-Metriken.

## üöÄ LangChain-Integrationen und -Frameworks

### 1. Grundlegende Integration mit LangChain

Dieses erste Beispiel demonstriert, wie unsere LLMaaS-API mit dem beliebten LangChain-Framework integriert werden kann, indem ein benutzerdefinierter "Wrapper" erstellt wird. Ein Wrapper ist eine Klasse, die unsere API "kapselt", um sie mit den internen Mechanismen von LangChain kompatibel zu machen.

#### Code Explained

Der folgende Code definiert eine `CloudTempleLLM`-Klasse, die von der Basis `LLM`-Klasse in LangChain erbt. Dies erm√∂glicht uns, benutzerdefiniertes Verhalten zu definieren, w√§hrend die Kompatibilit√§t mit dem LangChain-√ñkosystem (Ketten, Agenten usw.) erhalten bleibt.

1.  **`CloudTempleLLM(LLM)`**: Unsere Klasse erbt von `LLM`, was die Implementierung bestimmter Methoden, insbesondere `_call`, erfordert.
2.  **`_call(self, prompt: str, ...)`**: Dies ist der Kern unseres Wrappers. Jedes Mal, wenn LangChain unser Sprachmodell aufrufen muss, wird diese Methode aufgerufen. Darin formatieren wir eine standardm√§√üige HTTP POST-Anfrage mit den korrekten Headern (`Authorization`) und der erwarteten `Payload` f√ºr unseren API-Endpunkt `/v1/chat/completions`.
3.  **`example_langchain_basic()`**: Diese Demonstrationsfunktion zeigt, wie unser Wrapper verwendet wird. Wir instanziieren ihn, erstellen ein `PromptTemplate`, um unsere Anfrage zu strukturieren, und kombinieren sie zu einer `LLMChain`. Wenn wir die Kette (`chain.run(...)`) ausf√ºhren, ruft LangChain intern die von uns definierte `_call`-Methode auf.

Dieser Ansatz ist n√ºtzlich, wenn Sie die volle Kontrolle dar√ºber ben√∂tigen, wie LangChain mit der API interagiert, ist aber ausf√ºhrlicher als die Verwendung des `ChatOpenAI`-Clients (siehe [API-Referenz](./api#langchain)).

```python
# Abh√§ngigkeiten installieren

# pip install langchain requests pydantic

from langchain.llms.base import LLM
from langchain.schema import LLMResult, Generation
from typing import Optional, List, Any
from pydantic import Field
import requests
import json
import os

# --- Konfiguration ---

# Es wird empfohlen, Ihren API-Schl√ºssel in einer Umgebungsvariable zu speichern
API_KEY = os.getenv("LLMAAS_API_KEY", "Ihr-API-Schl√ºssel-hier")
BASE_URL = "https://api.ai.cloud-temple.com/v1"

class CloudTempleLLM(LLM):
    """
    Benutzerdefinierter LangChain-Wrapper f√ºr die LLMaaS-API von Cloud Temple.
    Diese Klasse erm√∂glicht die Nutzung unserer API als Standard-LLM innerhalb von LangChain.
    """
    
    api_key: str = Field(default="")
    model_name: str = Field(default="granite3.3:8b")
    temperature: float = Field(default=0.7)
    max_tokens: int = Field(default=1000)
    
    @property
    def _llm_type(self) -> str:
        """Eindeutiger Bezeichner f√ºr unseren LLM-Typ."""
        return "cloud_temple_llmaas"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """
        Hauptmethode, die den Aufruf an die LLMaaS-API durchf√ºhrt.
        LangChain verwendet diese Methode f√ºr jede Anfrage an das Modell.
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        if stop:
            payload["stop"] = stop
        
        # POST-Anfrage an die API ausf√ºhren
        response = requests.post(
            f"{BASE_URL}/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        response.raise_for_status()  # L√∂st eine Ausnahme bei HTTP-Fehlern aus
        result = response.json()
        
        # Inhalt der Assistentenmeldung zur√ºckgeben
        return result['choices'][0]['message']['content']

# --- Beispielnutzung ---
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

def example_langchain_wrapper():
    """Demonstriert die Verwendung des LLM-Wrappers mit einer LangChain-Kette."""
    
    # 1. Unser benutzerdefiniertes LLM initialisieren
    llm = CloudTempleLLM(
        api_key=API_KEY,
        model_name="granite3.3:8b"
    )
    
    # 2. Eine Prompt-Vorlage erstellen, um Anfragen zu strukturieren
    template = """
    Sie sind ein Experte in {domaine}. 
    Beantworten Sie diese Frage detailliert und professionell:
    
    Frage: {question}
    
    Antwort:
    """
    prompt = PromptTemplate(
        input_variables=["domaine", "question"],
        template=template
    )
    
    # 3. Eine Kette erstellen, die den Prompt und das LLM kombiniert
    chain = LLMChain(llm=llm, prompt=prompt)
    
    # 4. Die Kette mit spezifischen Variablen ausf√ºhren
    result = chain.run(
        domaine="Cybersicherheit",
        question="Was sind die Best Practices f√ºr die Sicherung einer REST-API?"
    )
    
    return result

# --- Test durchf√ºhren ---
if __name__ == "__main__":
    if API_KEY == "Ihr-API-Schl√ºssel-hier":
        print("Bitte setzen Sie Ihren LLMAAS_API_KEY in Ihren Umgebungsvariablen.")
    else:
        response = example_langchain_wrapper()
        print("Antwort des Cybersicherheitsexperten:\n")
        print(response)
```

### 2. RAG (Retrieval-Augmented Generation) mit der LLMaaS-API

RAG ist eine leistungsstarke Technik, die es einem Large Language Model (LLM) erm√∂glicht, Fragen zu beantworten, indem es eine externe Wissensbasis nutzt. Dieses Tutorial f√ºhrt Sie durch die Erstellung eines einfachen RAG-Pipelines mithilfe unserer API f√ºr Embeddings und Generierung sowie FAISS, einer Bibliothek f√ºr Vektor√§hnlichkeit, um einen In-Memory-Index zu erstellen.

#### Code Explained

Die Pipeline ist in mehrere logische Schritte unterteilt:

1.  **Konfiguration**: Wir importieren die notwendigen Bibliotheken und laden unseren API-Schl√ºssel aus den Umgebungsvariablen. Wir definieren die zu verwendenden Modelle: `granite-embedding:278m` f√ºr die Vektorisierung und `granite3.3:8b` f√ºr die Antwortgenerierung.
2.  **`LLMaaSEmbeddings`**: Wie im vorherigen Beispiel ben√∂tigen wir einen Wrapper, um mit unserer Embeddings-API zu interagieren. Diese Klasse ist daf√ºr verantwortlich, Text-Chunks in numerische Vektoren (Embeddings) umzuwandeln.
3.  **`setup_rag_pipeline`**: Diese Funktion orchestriert die Erstellung der Pipeline.
    *   **Laden von Dokumenten**: `DirectoryLoader` l√§dt Textdateien aus unserer Wissensbasis.
    *   **Aufteilung in Chunks**: `RecursiveCharacterTextSplitter` teilt die Dokumente in kleinere Teile. Dies ist entscheidend, damit das Embedding-Modell den Text effizient verarbeiten und eine genaue √Ñhnlichkeitssuche gew√§hrleisten kann.
    *   **Vektorisierung und Indexierung**: `FAISS.from_documents` ist ein wichtiger Schritt. Es nimmt die Text-Chunks, verwendet unsere `LLMaaSEmbeddings`-Klasse, um die API aufzurufen und entsprechende Vektoren zu erhalten, und speichert diese Vektoren dann in einem In-Memory-FAISS-Index.
    *   **LLM-Konfiguration**: Wir verwenden `ChatOpenAI`, das nativ mit unserer API f√ºr die Antwortgenerierungs-Teil kompatibel ist.
    *   **Erstellen der `RetrievalQA`-Kette**: Dies ist die LangChain-Kette, die alles miteinander verbindet. Wenn eine Frage gestellt wird, tut sie Folgendes:
        a. Verwendet den `retriever` (basierend auf unserem FAISS-Index), um die relevantesten Text-Chunks zu finden.
        b. "F√ºllt" (stapelt) diese Chunks zusammen mit der Frage in einen Prompt.
        c. Sendet diesen angereicherten Prompt an das LLM, um eine kontextuell relevante Antwort zu generieren.
4.  **Ausf√ºhrung**: Die `main`-Funktion simuliert die reale Nutzung, indem sie tempor√§re Wissensdateien erstellt, die Pipeline aufbaut und eine Frage stellt.

```python
import os
import tempfile
import shutil
from pathlib import Path
from dotenv import load_dotenv
from typing import List

# --- LangChain Imports ---
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# --- Konfiguration ---

# Umgebungsvariablen laden (z.B. LLMAAS_API_KEY)
load_dotenv()
API_KEY = os.getenv("LLMAAS_API_KEY")
BASE_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
EMBEDDING_MODEL = "granite-embedding:278m"
LLM_MODEL = "granite3.3:8b"

# --- Benutzerdefinierte Embedding-Klasse ---
class LLMaaSEmbeddings(Embeddings):
    """Benutzerdefinierte Embedding-Klasse f√ºr die Cloud Temple LLMaaS API."""
    def __init__(self, api_key: str, model_name: str):
        if not api_key:
            raise ValueError("Der LLMaaS API-Schl√ºssel darf nicht leer sein.")
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = BASE_URL
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

    def _embed(self, texts: List[str]) -> List[List[float]]:
        import httpx
        payload = {"input": texts, "model": self.model_name}
        try:
            with httpx.Client(timeout=60.0) as client:
                response = client.post(f"{self.base_url}/embeddings", headers=self.headers, json=payload)
                response.raise_for_status()
                data = response.json()['data']
                data.sort(key=lambda e: e['index'])
                return [item['embedding'] for item in data]
        except httpx.HTTPStatusError as e:
            print(f"HTTP-Fehler bei der Embedding-Generierung: {e.response.text}")
            raise
        except Exception as e:
            print(f"Ein unerwarteter Fehler ist bei der Embedding-Generierung aufgetreten: {e}")
            raise

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embed(texts)

    def embed_query(self, text: str) -> List[float]:
        # Die _embed-Methode erwartet eine Liste, daher umschlie√üen wir den einzelnen Text.
        return self._embed([text])[0]

# --- RAG-Pipeline ---
def setup_rag_pipeline(documents_path: str):
    """Vollst√§ndige Konfiguration der RAG-Pipeline mit LLMaaS-Tools."""
    print("1. Laden und Aufteilen von Dokumenten...")
    loader = DirectoryLoader(documents_path, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(documents)
    print(f"   -> {len(documents)} Dokument(e) geladen und in {len(splits)} Chunks aufgeteilt.")
    
    print(f"2. Embeddings √ºber LLMaaS erstellen (Modell: {EMBEDDING_MODEL})...")
    embeddings = LLMaaSEmbeddings(api_key=API_KEY, model_name=EMBEDDING_MODEL)
    
    print("3. In-Memory-Vektorindex (FAISS) erstellen...")
    vectorstore = FAISS.from_documents(splits, embeddings)
    print("   -> FAISS-Index erfolgreich erstellt.")
    
    print(f"4. LLM konfigurieren (Modell: {LLM_MODEL})...")
    # Fix f√ºr Pydantic/LangChain-Kompatibilit√§t
    from langchain_core.caches import BaseCache
    from langchain_core.callbacks.base import Callbacks
    ChatOpenAI.model_rebuild()
    
    llm = ChatOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=LLM_MODEL,
        temperature=0.3,
        model_kwargs={"max_tokens": 300}
    )
    
    print("5. Question/Answer-Kette (RAG) erstellen...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    print("   -> RAG-Pipeline bereit.")
    return qa_chain

# --- Ausf√ºhrung ---
def main():
    """Hauptfunktion zur Ausf√ºhrung der End-to-End-RAG-Pipeline."""
    if not API_KEY:
        print("Fehler: Die Umgebungsvariable LLMAAS_API_KEY ist nicht gesetzt.")
        return

    # Tempor√§re Testdokumente erstellen
    temp_dir = tempfile.mkdtemp()
    print(f"\nErstelle Testdokumente in: {temp_dir}")
    try:
        documents_content = {
            "overview.txt": "Cloud Temple ist ein franz√∂sischer souver√§ner Cloud-Anbieter, der SecNumCloud-qualifiziert ist.",
            "pricing.txt": "Die LLMaaS-API-Preise betragen 0,9 ‚Ç¨ pro Million Input-Token und 4 ‚Ç¨ pro Million Output-Token."
        }
        for filename, content in documents_content.items():
            with open(Path(temp_dir) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # Pipeline einrichten und ausf√ºhren
        rag_chain = setup_rag_pipeline(temp_dir)
        
        print("\n--- RAG-Pipeline abfragen ---")
        question = "Wie hoch ist die Output-Token-Rate f√ºr die LLMaaS-API bei Cloud Temple?"
        result = rag_chain({"query": question})
        
        print(f"\nFrage: {question}")
        print(f"Antwort: {result['result']}")
        print("\nVerwendete Quellen f√ºr die Antwort:")
        for source in result["source_documents"]:
            print(f"- Datei: {os.path.basename(source.metadata['source'])}")
            print(f"  Inhalt: \"{source.page_content}\"")

    finally:
        # Tempor√§res Verzeichnis bereinigen
        print(f"\nBereinige tempor√§res Verzeichnis: {temp_dir}")
        shutil.rmtree(temp_dir)

if __name__ == "__main__":
    main()
```

### 3. Integration mit einer Vektordatenbank (Qdrant)

F√ºr RAG-Anwendungen in der Produktion wird die Verwendung einer dedizierten Vektordatenbank wie **Qdrant** empfohlen. Im Gegensatz zu FAISS, das im Speicher arbeitet, bietet Qdrant Datenpersistenz, erweiterte Suchfunktionen und eine bessere Skalierbarkeit.

#### Code Explained

Dieses Tutorial passt die vorherige RAG-Pipeline an die Verwendung von Qdrant an.

1.  **Voraussetzungen**: Der erste Schritt ist der Start einer Qdrant-Instanz. Am einfachsten ist die Verwendung von Docker.
2.  **`setup_qdrant_rag_pipeline`**:
    *   **Embeddings und Dokumente**: Die Erstellung von Embeddings und Dokumenten bleibt dieselbe wie im vorherigen Beispiel.
    *   **Verbindung zu Qdrant**: Anstatt einen FAISS-Index zu erstellen, verwenden wir `Qdrant.from_documents`. Diese LangChain-Methode verwaltet mehrere Schritte:
        a. Sie stellt eine Verbindung zu Ihrer Qdrant-Instanz √ºber die bereitgestellte URL her.
        b. Sie erstellt eine neue "Sammlung" (das √Ñquivalent einer Tabelle in einer SQL-Datenbank), falls sie noch nicht existiert.
        c. Sie ruft unsere `LLMaaSEmbeddings`-Klasse auf, um die Dokumente zu vektorisieren.
        d. Sie f√ºgt die Dokumente und ihre Vektoren in die Qdrant-Sammlung ein.
    *   **`force_recreate=True`**: F√ºr dieses Tutorial verwenden wir diesen Parameter, um sicherzustellen, dass die Sammlung bei jedem Lauf leer ist. In der Produktion w√ºrden Sie ihn auf `False` setzen, um Ihre Daten zu erhalten.
3.  Der Rest der Pipeline (LLM-Konfiguration, Erstellung der `RetrievalQA`-Kette) bleibt identisch, was die Flexibilit√§t von LangChain demonstriert: Ein einfacher Wechsel der `retriever`-Quelle (Informationsabrufer) erm√∂glicht den Wechsel von FAISS zu Qdrant.

:::info Voraussetzungen: Qdrant starten
F√ºr dieses Tutorial ben√∂tigen Sie eine Qdrant-Instanz. Sie k√∂nnen diese einfach mit Docker starten:

```bash


# 1. Laden Sie das neueste Qdrant-Image herunter
docker pull qdrant/qdrant

# 2. Starten Sie den Qdrant-Container
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
```
:::

Der folgende Code zeigt, wie die RAG-Pipeline an die Verwendung von Qdrant als Vektordatenbank angepasst werden kann.

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Qdrant
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List
from langchain_core.embeddings import Embeddings

# (Die LLMaaSEmbeddings-Klasse ist dieselbe wie im vorherigen Beispiel,
# Wir verwenden sie hier wieder. Stellen Sie sicher, dass sie in Ihrem Skript definiert ist.)

# --- Konfiguration ---
load_dotenv()
API_KEY = os.getenv("LLMAAS_API_KEY")
BASE_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
EMBEDDING_MODEL = "granite-embedding:278m"
LLM_MODEL = "granite3.3:8b"
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_COLLECTION_NAME = "tutorial_collection"

# --- Embedding-Klasse (aus vorherigem Beispiel wiederverwendet) ---
class LLMaaSEmbeddings(Embeddings):
    def __init__(self, api_key: str, model_name: str):
        if not api_key: raise ValueError("API-Schl√ºssel ist erforderlich.")
        self.api_key, self.model_name, self.base_url = api_key, model_name, BASE_URL
        self.headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
    def _embed(self, texts: List[str]) -> List[List[float]]:
        import httpx
        payload = {"input": texts, "model": self.model_name}
        with httpx.Client(timeout=60.0) as client:
            r = client.post(f"{self.base_url}/embeddings", headers=self.headers, json=payload)
            r.raise_for_status()
            data = r.json()['data']
            data.sort(key=lambda e: e['index'])
            return [item['embedding'] for item in data]
    def embed_documents(self, texts: List[str]) -> List[List[float]]: return self._embed(texts)
    def embed_query(self, text: str) -> List[float]: return self._embed([text])[0]

def setup_qdrant_rag_pipeline():
    """Konfiguriert und gibt eine RAG-Pipeline unter Verwendung von Qdrant zur√ºck."""
    print("1. LLMaaS Embedding-Client initialisieren...")
    embeddings = LLMaaSEmbeddings(api_key=API_KEY, model_name=EMBEDDING_MODEL)

    print("2. Dokumente vorbereiten...")
    documents_content = [
        "Cloud Temple ist ein franz√∂sischer souver√§ner Cloud-Anbieter, der SecNumCloud-zertifiziert ist.",
        "Die LLMaaS-Preise betragen 0,9 ‚Ç¨ f√ºr Input- und 4 ‚Ç¨ f√ºr Output-Token pro Million Token."
    ]
    documents = [Document(page_content=d) for d in documents_content]
    
    print(f"3. Verbindung zu Qdrant herstellen und Sammlung '{QDRANT_COLLECTION_NAME}' f√ºllen...")
    vectorstore = Qdrant.from_documents(
        documents,
        embeddings,
        url=QDRANT_URL,
        collection_name=QDRANT_COLLECTION_NAME,
        force_recreate=True, # Stellt eine saubere Sammlung f√ºr das Tutorial sicher
    )
    print("   -> Sammlung erfolgreich erstellt und gef√ºllt.")

    print(f"4. LLM konfigurieren (Modell: {LLM_MODEL})...")
    llm = ChatOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=LLM_MODEL,
        temperature=0.3
    )

    print("5. RAG-Kette erstellen...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    print("   -> Qdrant-basierte RAG-Pipeline bereit.")
    return qa_chain

# --- Ausf√ºhrung ---
def main_qdrant():
    """Hauptfunktion zur Ausf√ºhrung der RAG-Pipeline mit Qdrant."""
    if not API_KEY:
        print("Fehler: Die Umgebungsvariable LLMAAS_API_KEY ist nicht gesetzt.")
        return
        
    try:
        rag_chain = setup_qdrant_rag_pipeline()
        question = "Wie lauten die Preisdetails f√ºr die LLMaaS-API von Cloud Temple?"

        print(f"\n--- Pipeline abfragen ---")
        result = rag_chain({"query": question})

        print(f"\nFrage: {question}")
        print(f"Antwort: {result['result']}")
        print("\nF√ºr die Antwort verwendete Quellen:")
        for source in result["source_documents"]:
            print(f"- Inhalt: \"{source.page_content}\"")
            
    except Exception as e:
        print(f"\nEin Fehler ist aufgetreten: {e}")
        print("Bitte stellen Sie sicher, dass der Qdrant-Container l√§uft.")

if __name__ == "__main__":
    main_qdrant()
```

### 4. LangChain-Agenten mit Tools

Ein Agent ist ein LLM, das mehr als nur eine Frage beantwortet ‚Äì es kann eine Reihe von **Tools** (Funktionen, APIs usw.) verwenden, um eine komplexere Antwort zu erstellen. Es kann Schlussfolgerungen ziehen, ein Problem zerlegen, ein geeignetes Tool ausw√§hlen, es ausf√ºhren, das Ergebnis beobachten und diesen Zyklus wiederholen, bis es zu einer endg√ºltigen Antwort kommt.

#### Code Explained

Dieses Beispiel erstellt einen einfachen Agenten, der zwei Tools verwenden kann: eines zum Abfragen einer (simulierten) Cloud Temple-API und eines zum Ausf√ºhren von Berechnungen.

1.  **Tool-Definitionen**: Die Klassen `CloudTempleAPITool` und `CalculatorTool` erben von `BaseTool`. Jedes Tool hat:
    *   Einen `Namen`: einen einfachen und beschreibenden Namen.
    *   Eine `Beschreibung`: **entscheidend** ‚Äì dies ist, was das LLM liest, um zu entscheiden, welches Tool verwendet werden soll. Es muss sehr klar sein, was das Tool tut und wann es verwendet werden soll.
    *   Eine `_run`-Methode: der eigentliche Code, der ausgef√ºhrt wird, wenn der Agent dieses Tool ausw√§hlt.

2.  **`create_agent_with_tools`**:
    *   **LLM-Initialisierung**: Wir verwenden unseren `CloudTempleLLM`-Wrapper, der im ersten Tutorial definiert wurde.
    *   **Tool-Liste**: Wir stellen dem Agenten eine Liste von Tools zur Verf√ºgung, die er verwenden darf.
    *   **Agent-Prompt**: Der Prompt ist sehr spezifisch. Es ist ein "Reasoning-Prompt", der das LLM anweist, wie es denken (`Thought`), eine Aktion ausw√§hlen (`Action`), eine Eingabe f√ºr diese Aktion bereitstellen (`Action Input`) und das Ergebnis beobachten (`Observation`) soll. Dies ist der Kernmechanismus des hier verwendeten ReAct-Frameworks (Reasoning and Acting).
    *   **Agent-Erstellung**: `create_react_agent` setzt das LLM, die Tools und den Prompt zusammen, um den Agenten zu erstellen.
    *   **`AgentExecutor`**: Dies ist die Engine, die den Agenten in einer Schleife ausf√ºhrt, bis er eine `Final Answer` produziert. Der Parameter `verbose=True` ist sehr n√ºtzlich, um den "inneren Dialog" des Agenten (seine Gedanken, Aktionen usw.) zu sehen.

```python
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain.tools import BaseTool
from langchain.prompts import PromptTemplate
import requests
import json
import os

# (Die CloudTempleLLM-Klasse ist dieselbe wie im ersten Beispiel)

# --- Tool-Definitionen ---

class CloudTempleAPITool(BaseTool):
    """Ein Tool, das den Aufruf einer internen API simuliert, um Informationen √ºber Dienste abzurufen."""
    name = "cloud_temple_api_checker"
    description = "N√ºtzlich, um Informationen √ºber Dienste, Produkte und Angebote von Cloud Temple zu erhalten."

    def _run(self, query: str) -> str:
        # In einem realen Szenario w√ºrde dies eine echte API aufrufen.
        print(f"--- CloudTempleAPITool aufgerufen mit der Abfrage: '{query}' ---")
        if "Dienst" in query.lower():
            return "Cloud Temple bietet folgende Dienste an: IaaS, PaaS, LLMaaS, Managed Security."
        return "Informationen nicht gefunden."

    async def _arun(self, query: str) -> str:
        # Eine asynchrone Implementierung ist f√ºr dieses Beispiel nicht erforderlich.
        raise NotImplementedError("Das API-Tool unterst√ºtzt keine asynchrone Ausf√ºhrung.")

class SimpleCalculatorTool(BaseTool):
    """Ein einfaches Tool zum Ausf√ºhren mathematischer Berechnungen."""
    name = "simple_calculator"
    description = "N√ºtzlich zum Ausf√ºhren einfacher mathematischer Berechnungen. Nimmt einen g√ºltigen Python-Ausdruck entgegen."

    def _run(self, expression: str) -> str:
        print(f"--- SimpleCalculatorTool aufgerufen mit dem Ausdruck: '{expression}' ---")
        try:
            # WARNUNG: eval() ist in der Produktion gef√§hrlich. Dies dient nur zu Demonstrationszwecken.
            return str(eval(expression))
        except Exception as e:
            return f"Berechnungsfehler: {e}"

    async def _arun(self, expression: str) -> str:
        raise NotImplementedError("Das Rechner-Tool unterst√ºtzt keine asynchrone Ausf√ºhrung.")

# --- Agenten-Erstellung ---

def create_agent():
    """Konfiguriert und gibt einen LangChain-Agenten mit den definierten Tools zur√ºck."""
    print("1. Initialisiere das LLM f√ºr den Agenten...")
    llm = CloudTempleLLM(api_key=os.getenv("LLMAAS_API_KEY", "Ihr-API-Schl√ºssel-hier"))

    tools = [CloudTempleAPITool(), SimpleCalculatorTool()]
    
    # Die Prompt-Vorlage ist entscheidend: Sie leitet das LLM in seinen √úberlegungen.
    template = """
    Beantworten Sie die folgenden Fragen so gut wie m√∂glich. Sie haben Zugriff auf die folgenden Tools:

    {tools}

    Verwenden Sie das folgende Format:

    Frage: die Frage, die Sie beantworten m√ºssen
    Thought: Sie m√ºssen immer dar√ºber nachdenken, was Sie tun werden
    Action: die auszuf√ºhrende Aktion, muss einer von [{tool_names}] sein
    Action Input: die Eingabe f√ºr die Aktion
    Observation: das Ergebnis der Aktion
    ... (diese Thought/Action/Action Input/Observation-Sequenz kann sich wiederholen)
    Thought: Ich kenne jetzt die endg√ºltige Antwort.
    Final Answer: die endg√ºltige Antwort auf die urspr√ºngliche Frage

    Beginnen Sie!

    Frage: {input}
    Thought:{agent_scratchpad}
    """
    
    prompt = PromptTemplate.from_template(template)
    
    print("2. Erstellen des Agenten mit dem ReAct-Framework...")
    agent = create_react_agent(llm, tools, prompt)

    # Der AgentExecutor ist f√ºr die Ausf√ºhrung der Agentenzyklen verantwortlich.
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    print("   -> Agent bereit.")
    return agent_executor

# --- Ausf√ºhrung ---

def run_agent():
    """F√ºhrt den Agenten mit verschiedenen Fragen aus, um seine F√§higkeiten zu testen."""
    if os.getenv("LLMAAS_API_KEY") is None:
        print("Bitte setzen Sie Ihren LLMAAS_API_KEY.")
        return
        
    agent_executor = create_agent()
    
    print("\n--- Test 1: Frage, die ein Informationstool erfordert ---")
    question1 = "Welche Dienste bietet Cloud Temple an?"
    response1 = agent_executor.invoke({"input": question1})
    print(f"\nEndg√ºltige Agentenantwort: {response1['output']}")
    
    print("\n--- Test 2: Frage, die eine Berechnung erfordert ---")
    question2 = "Was ist das Ergebnis von 125 * 8 + 50?"
    response2 = agent_executor.invoke({"input": question2})
    print(f"\nEndg√ºltige Agentenantwort: {response2['output']}")

if __name__ == "__main__":
    run_agent()
```

### 5. OpenAI SDK-Integration

**Nahtlose Migration von OpenAI**

```python
from openai import OpenAI

# Cloud Temple LLMaaS-Konfiguration

def setup_cloud_temple_client():
    """OpenAI-Client-Konfiguration f√ºr Cloud Temple"""
    
    client = OpenAI(
        api_key="Ihr-Cloud-Temple-API-Schl√ºssel",
        base_url="https://api.ai.cloud-temple.com/v1"
    )
    
    return client

def test_openai_compatibility():
    """Kompatibilit√§tstest mit dem OpenAI SDK"""
    
    client = setup_cloud_temple_client()
    
    # Standard-Chat-Vervollst√§ndigung
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[
            {"role": "system", "content": "Sie sind ein professioneller KI-Assistent."},
            {"role": "user", "content": "Erkl√§ren Sie die Cloud-Native-Architektur."}
        ],
        max_tokens=300,
        temperature=0.7
    )
    
    print(f"Antwort: {response.choices[0].message.content}")
    
    # Streaming
    stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[
            {"role": "user", "content": "Schreiben Sie ein Gedicht √ºber KI."}
        ],
        stream=True,
        max_tokens=200
    )
    
    print("Streaming:")
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
    print()

# Kompatibilit√§tstest
test_openai_compatibility()
```

### 5. Semantic Kernel Integration (Microsoft)

[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) ist ein Open-Source-SDK von Microsoft, das die Integration von LLMs in .NET-, Python- und Java-Anwendungen erm√∂glicht. Obwohl es f√ºr Azure OpenAI-Dienste optimiert ist, kann es dank seiner Flexibilit√§t mit jeder OpenAI-kompatiblen API, einschlie√ülich unserer eigenen, verwendet werden.

#### Code Explained

Dieses Beispiel erfordert nicht das vollst√§ndige Semantic Kernel SDK. Es demonstriert, wie das **Konzept einer "semantischen Funktion"** mit einem einfachen Aufruf unserer API implementiert werden kann. Eine semantische Funktion ist im Wesentlichen ein strukturierter Prompt, der an ein Large Language Model (LLM) gesendet wird, um eine bestimmte Aufgabe auszuf√ºhren.

1.  **`semantic_kernel_simple()`**: Diese Funktion simuliert eine "Zusammenfassungsfunktion".
2.  **Strukturierter Prompt**: Wir verwenden eine `system`-Nachricht, um dem LLM eine Rolle zuzuweisen ("Sie sind ein Experte f√ºr Zusammenfassungen"), und eine `user`-Nachricht, die den zusammenzufassenden Text enth√§lt. Dies ist der Kern des Konzepts der semantischen Funktion.
3.  **Direkter API-Aufruf**: Ein einfacher `requests.post`-Aufruf unseres `/v1/chat/completions`-Endpunkts reicht aus, um die Funktion auszuf√ºhren.

Dieses Beispiel veranschaulicht die Flexibilit√§t der LLMaaS-API, die als grundlegender Baustein f√ºr die Textgenerierung in jedem Framework dienen kann ‚Äì selbst in solchen, f√ºr die keine offizielle Integration existiert.

```python
import requests
import os
from dotenv import load_dotenv

def semantic_kernel_simulation():
    """
    Simuliert eine "semantische Funktion" zur Zusammenfassung, indem die LLMaaS-API direkt aufgerufen wird.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Bitte setzen Sie die Umgebungsvariable LLMAAS_API_KEY.")
        return

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    text_to_summarize = """
    K√ºnstliche Intelligenz (KI) transformiert zahlreiche Industriesektoren durch die Automatisierung von Aufgaben, 
    die Optimierung von Prozessen und die Erm√∂glichung fortschrittlicher pr√§diktiver Analysen. 
    Cloud Temple erm√∂glicht Unternehmen mit seinem souver√§nen und SecNumCloud-zertifizierten LLMaaS-Angebot, 
    diese KI-Funktionen zu integrieren und dabei die Sicherheit und Vertraulichkeit ihrer Daten zu gew√§hrleisten.
    """
    
    # Der Prompt kombiniert eine Anweisung (Systemrolle) und Daten (Benutzerrolle)
    payload = {
        "model": "granite3.3:8b",
        "messages": [
            {"role": "system", "content": "Sie sind ein erfahrener Assistent zum Zusammenfassen technischer Dokumente."},
            {"role": "user", "content": f"Fassen Sie den folgenden Text in einem einzigen pr√§gnanten Satz zusammen: {text_to_summarize}"}
        ],
        "max_tokens": 100,
        "temperature": 0.5
    }
    
    try:
        response = requests.post(
            "https://api.ai.cloud-temple.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        summary = result['choices'][0]['message']['content']
        
        print("Originaltext:\n", text_to_summarize)
        print("\nGenerierte Zusammenfassung:\n", summary)
        return summary
        
    except requests.exceptions.RequestException as e:
        print(f"Ein API-Fehler ist aufgetreten: {e}")

if __name__ == "__main__":
    semantic_kernel_simulation()
```

### 6. Haystack Framework

[Haystack](https://haystack.deepset.ai/) ist ein weiteres leistungsstarkes Open-Source-Framework zum Erstellen von semantischen Such-, RAG- und Agent-Anwendungen. Wie beim Semantic Kernel kann unsere API direkt integriert werden.

#### Code Explained

Dieses Beispiel simuliert eine einfache Haystack-artige "Pipeline" zur Beantwortung von Fragen innerhalb eines bestimmten Kontexts (Question Answering).

1.  **`process_with_context`**: Diese Funktion stellt den Kern einer QA-Pipeline dar. Sie nimmt einen `context` (z. B. einen Absatz aus einem Dokument) und eine `question` entgegen.
2.  **Kontextbezogener Prompt**: Der Prompt ist sorgf√§ltig strukturiert, um sowohl den Kontext als auch die Frage zu enthalten. Dies ist eine grundlegende Technik bei RAG: Wir stellen dem LLM relevante Informationen zur Verf√ºgung, damit es eine sachliche Antwort formulieren kann.
3.  **API-Aufruf**: Auch hier reicht ein einfacher `requests.post`-Aufruf unserer API aus. Das LLM erh√§lt den Kontext und die Frage, und seine Aufgabe ist es, eine Antwort zu synthetisieren, die sich *ausschlie√ülich* auf die bereitgestellten Informationen st√ºtzt.

Dieses Beispiel demonstriert die Flexibilit√§t der LLMaaS-API, die als grundlegender Baustein f√ºr die Textgenerierung in jedem Framework dienen kann ‚Äì selbst in solchen, f√ºr die keine offizielle Integration existiert.

```python
import requests
import os
from dotenv import load_dotenv

def haystack_simulation():
    """
    Simuliert eine Question-Answering-Pipeline im Haystack-Stil
    unter Verwendung eines direkten Aufrufs der LLMaaS-API.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Bitte setzen Sie die Umgebungsvariable LLMAAS_API_KEY.")
        return

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # Der Kontext sind die Informationen, die das LLM verwenden darf.
    context = """
    Eine souver√§ne Cloud ist eine Cloud-Computing-Infrastruktur, die vollst√§ndig 
    innerhalb der Grenzen eines bestimmten Landes enthalten und dessen Gesetzen unterworfen ist. 
    Die Hauptvorteile sind die garantierte Datenresidenz, die Einhaltung lokaler Vorschriften 
    (wie der DSGVO in Europa) und ein verbesserter Schutz vor dem Zugriff durch ausl√§ndische Unternehmen 
    unter extraterritorialen Gesetzen wie dem U.S. CLOUD Act.
    """
    
    question = "Was sind die Vorteile einer souver√§nen Cloud?"
    
    # Der Prompt leitet das LLM an, seine Antwort ausschlie√ülich auf den bereitgestellten Kontext zu st√ºtzen.
    prompt = f"""
    Beantworten Sie die Frage ausschlie√ülich auf der Grundlage des folgenden Kontexts.
    
    Context:
    ---
    {context}
    ---
    
    Question: {question}
    """
    
    payload = {
        "model": "granite3.3:8b",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 200,
        "temperature": 0.2  # Niedrige Temperatur f√ºr eine faktische Antwort
    }
    
    try:
        response = requests.post(
            "https://api.ai.cloud-temple.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        answer = result['choices'][0]['message']['content']
        
        print(f"Frage: {question}")
        print("\nGenerierte Antwort:\n", answer)
        return answer
        
    except requests.exceptions.RequestException as e:
        print(f"Ein API-Fehler ist aufgetreten: {e}")

if __name__ == "__main__":
    haystack_simulation()
```

### 7. LlamaIndex-Integration

[LlamaIndex](https://www.llamaindex.ai/) ist ein Framework, das auf die Erstellung von RAG-Anwendungen spezialisiert ist. Es bietet High-Level-Komponenten f√ºr die Datenerfassung, Indexierung und Abfrage. Unsere API, die mit der OpenAI-Schnittstelle kompatibel ist, l√§sst sich sehr einfach integrieren.

#### Code Explained

Dieses Beispiel demonstriert, wie LlamaIndex konfiguriert wird, um die LLMaaS-API f√ºr die Textgenerierung zu verwenden, w√§hrend ein lokales Embedding-Modell f√ºr die Vektorisierung verwendet wird.

1.  **`setup_and_run_llamaindex`**: Diese einzelne Funktion orchestriert den gesamten Prozess.
    *   **LLM-Konfiguration**: LlamaIndex bietet eine `OpenAILike`-Klasse, die die Verbindung zu jeder mit dem OpenAI-Format kompatiblen API erm√∂glicht. Geben Sie einfach Ihre `api_base` und einen `api_key` an. Dies ist der einfachste Weg, Ihr LLM kompatibel zu machen.
    *   **Embedding-Konfiguration**: F√ºr dieses Beispiel verwenden wir ein lokales Embedding-Modell (`HuggingFaceEmbedding`). Dies veranschaulicht die Flexibilit√§t von LlamaIndex, das das Mischen verschiedener Komponenten erm√∂glicht. Sie k√∂nnten genauso einfach die `LLMaaSEmbeddings`-Klasse aus fr√ºheren Beispielen verwenden, um unsere Embedding-API zu nutzen.
    *   **`Settings`**: Das `Settings`-Objekt in LlamaIndex ist eine bequeme M√∂glichkeit, Standardkomponenten (LLM, Embedding-Modell, Chunk-Gr√∂√üe usw.) zu konfigurieren, die von anderen LlamaIndex-Objekten verwendet werden.
    *   **Datenerfassung**: `SimpleDirectoryReader` l√§dt Dokumente aus einem Verzeichnis.
    *   **Index-Erstellung**: `VectorStoreIndex.from_documents` ist die High-Level-Methode von LlamaIndex. Sie √ºbernimmt automatisch das Chunking, die Vektorisierung von Chunks (unter Verwendung des in `Settings` konfigurierten `embed_model`) und die Erstellung von In-Memory-Indizes.
    *   **Abfrage-Engine**: `.as_query_engine()` erstellt eine einfache Schnittstelle zum Stellen von Fragen an Ihren Index. Wenn Sie `.query()` aufrufen, vektorisiert die Engine Ihre Frage, ruft die relevantesten Dokumente aus dem Index ab und sendet sie zusammen mit der Frage an das LLM (in `Settings` konfiguriert), um eine Antwort zu generieren.

```python


# Abh√§ngigkeiten:

# pip install llama-index llama-index-llms-openai-like llama-index-embeddings-huggingface

import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import shutil

def setup_and_run_llamaindex():
    """
    Konfiguriert und f√ºhrt eine einfache RAG-Pipeline mit LlamaIndex und der LLMaaS-API aus.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Bitte setzen Sie die Umgebungsvariable LLMAAS_API_KEY.")
        return

    # 1. Konfigurieren Sie das LLM, um auf die LLMaaS-API √ºber die OpenAILike-Schnittstelle zu zeigen
    print("1. Konfiguriere das LLM, um auf die LLMaaS-API zu zeigen...")
    llm = OpenAILike(
        api_key=api_key,
        api_base="https://api.ai.cloud-temple.com/v1",
        model="granite3.3:8b",
        is_chat_model=True,
        # Manchmal ist es notwendig, Kontextfensterparameter f√ºr bestimmte Modelle hinzuzuf√ºgen
        # context_window=3900, 
    )

    # 2. Konfigurieren Sie das Embedding-Modell (in diesem Beispiel lokal zur Vereinfachung)
    print("2. Konfiguriere das lokale Embedding-Modell...")
    embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # 3. Wenden Sie die Konfigurationen global √ºber das Settings-Objekt von LlamaIndex an
    Settings.llm = llm
    Settings.embed_model = embed_model
    print("   -> LLM und Embedding-Modell konfiguriert.")

    # 4. Erstellen Sie eine einfache Wissensbasis in einem tempor√§ren Verzeichnis
    print("4. Erstelle und lade eine tempor√§re Wissensbasis...")
    temp_dir = "temp_llama_data"
    os.makedirs(temp_dir, exist_ok=True)
    knowledge_file = os.path.join(temp_dir, "knowledge.txt")
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write("Das LLMaaS-Angebot von Cloud Temple ist eine souver√§ne generative KI-L√∂sung, "
                "die vollst√§ndig in Frankreich betrieben und von der ANSSI SecNumCloud-zertifiziert ist.")
    
    documents = SimpleDirectoryReader(temp_dir).load_data()
    print(f"   -> {len(documents)} Dokument(e) geladen.")

    # 5. Erstellen Sie den Vektorindex. LlamaIndex √ºbernimmt das Chunking und Embedding automatisch.
    print("5. Erstelle den Vektorindex...")
    index = VectorStoreIndex.from_documents(documents)
    print("   -> Index erstellt.")

    # 6. Erstellen Sie die Abfrage-Engine und fragen Sie die Wissensbasis ab
    print("6. Erstelle die Abfrage-Engine und frage ab...")
    query_engine = index.as_query_engine()
    question = "Was sind die Souver√§nit√§tsgarantien des LLMaaS-Angebots?"
    response = query_engine.query(question)
    
    print(f"\nFrage: {question}")
    print(f"Antwort: {response}")

    # Tempor√§res Verzeichnis bereinigen
    shutil.rmtree(temp_dir)
    print(f"\nTempor√§res Verzeichnis '{temp_dir}' gel√∂scht.")

if __name__ == "__main__":
    setup_and_run_llamaindex()
```

### 8. Konfiguration der CLINE-Erweiterung f√ºr VSCode

Dieses Tutorial f√ºhrt Sie durch die Einrichtung der CLINE-Erweiterung in Visual Studio Code, um die Sprachmodelle von Cloud Temple direkt aus Ihrem Editor zu nutzen.

#### Konfigurationsschritte

1.  **CLINE-Einstellungen √∂ffnen**: √ñffnen Sie in VSCode die Einstellungen f√ºr die CLINE-Erweiterung.
2.  **Neues Modell erstellen**: F√ºgen Sie eine neue Modellkonfiguration hinzu.
3.  **Felder ausf√ºllen**: Konfigurieren Sie die Felder wie unten gezeigt, basierend auf dem untenstehenden Bild.

    ![CLINE-Konfiguration f√ºr LLMaaS](./images/cline_configuration.png)

    *   **API-Anbieter**: W√§hlen Sie `OpenAI Kompatibel`.
    *   **Basis-URL**: Geben Sie den LLMaaS-API-Endpunkt von Cloud Temple ein: `https://api.ai.cloud-temple.com/v1`.
    *   **OpenAI-kompatibler API-Schl√ºssel**: F√ºgen Sie den API-Schl√ºssel ein, den Sie von der Cloud Temple-Konsole generiert haben.
    
    :::tip API-Schl√ºssel generieren
    Um Ihren API-Schl√ºssel zu generieren, gehen Sie zur Cloud Temple-Konsole, navigieren Sie zu **LLMaaS** > **API-Schl√ºssel** und klicken Sie dann auf **"API-Schl√ºssel erstellen"**.
    
    ![API-Schl√ºssel aus der Konsole erstellen](./images/console_create_api_key.png)
    :::
    
    *   **Modell-ID**: Geben Sie das Modell an, das Sie verwenden m√∂chten, z. B. `qwen3-coder:30b`. Eine Liste der verf√ºgbaren Modelle finden Sie im Abschnitt [Modelle](./models.md).
    *   **Modellkonfiguration**:
        *   **Unterst√ºtzt Bilder**: Aktivieren Sie dieses Kontrollk√§stchen, wenn das Modell Bilder unterst√ºtzt.
        *   **Unterst√ºtzt Browsernutzung**: Aktivieren Sie dieses Kontrollk√§stchen.
        *   **Kontextfenstergr√∂√üe**: Geben Sie die Kontextfenstergr√∂√üe des Modells ein (z. B. `128000`).
        *   **Maximale Ausgabe-Token**: Lassen Sie den Wert `‚àí1` f√ºr eine unbegrenzte Ausgabe standardm√§√üig.
        *   **Temperatur**: Stellen Sie die Temperatur nach Ihren Bed√ºrfnissen ein (z. B. `0`).

Sie k√∂nnen nun ein Modell in CLINE ausw√§hlen und es verwenden, um Code zu generieren, Fragen zu beantworten und vieles mehr.

## üí° Erweiterte Beispiele

Das folgende GitHub-Verzeichnis enth√§lt eine Sammlung von Codebeispielen und Skripten, die die verschiedenen Funktionen und Anwendungsf√§lle des LLM as a Service (LLMaaS)-Angebots von Cloud Temple demonstrieren:

[Cloud-Temple/product-llmaas-how-to](https://github.com/Cloud-Temple/product-llmaas-how-to/tree/main)

Dort finden Sie praktische Anleitungen f√ºr:

-   **Informationsgewinnung und Textanalyse:** F√§higkeit, Dokumente zu analysieren und strukturierte Daten wie Entit√§ten, Ereignisse, Beziehungen und Attribute zu extrahieren, unter Nutzung dom√§nenspezifischer Ontologien (z. B. Recht, Personalwesen, IT).

-   **Konversationelle Interaktion und Chatbots:** Entwicklung von Konversationsagenten, die dialogf√§hig sind, den Gespr√§chsverlauf aufrechterhalten, System-Prompts verwenden und externe Tools aufrufen k√∂nnen.

-   **Audio-Transkription (Speech-to-Text):** Umwandlung von Audioinhalten in Text, auch f√ºr gro√üe Dateien, unter Verwendung von Techniken wie Segmentierung, Normalisierung und Stapelverarbeitung.

-   **Text√ºbersetzung:** √úbersetzung von Dokumenten von einer Sprache in eine andere, wobei der Kontext √ºber mehrere Segmente hinweg verwaltet wird, um die Koh√§renz zu verbessern.

-   **Modellverwaltung und -bewertung:** Auflistung verf√ºgbarer Sprachmodelle √ºber die API, √úberpr√ºfung ihrer Spezifikationen und Durchf√ºhrung von Tests zum Vergleich der Leistung.

-   **Echtzeit-Antwort-Streaming:** Demonstration der F√§higkeit, Modellantworten schrittweise (Token f√ºr Token) zu empfangen und anzuzeigen, unerl√§sslich f√ºr interaktive Anwendungen.

-   **RAG-Pipeline mit In-Memory-Wissensbasis:** Lehrreiches RAG-Demonstrationsprogramm, das den Workflow der Retrieval-Augmented Generation veranschaulicht. Nutzt die LLMaaS-API f√ºr Embedding und Generierung, mit Vektorspeicherung im Speicher (FAISS) f√ºr ein klares Verst√§ndnis des Prozesses.

-   **RAG-Pipeline mit Vektordatenbank (Qdrant):** Vollst√§ndiges, containerisiertes RAG-Demonstrationsprogramm, das Qdrant als Vektordatenbank verwendet. Die LLMaaS-API wird f√ºr das Embedding von Dokumenten und die Generierung angereicherter Antworten verwendet.

-   **OCR- und Dokumentenanalyse (DeepSeek-OCR):** Umfassende Anleitung und Demonstrationswerkzeug zur Konvertierung von Bildern und PDFs in strukturiertes Markdown, zum Extrahieren von Tabellen und zum Transkribieren mathematischer Formeln. Siehe die [spezielle Dokumentation](./ocr).
