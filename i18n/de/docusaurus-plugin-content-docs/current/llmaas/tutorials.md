---
title: Tutorials 
sidebar_position: 6
---

# Tutorials LLMaaS

## √úbersicht

Diese fortgeschrittenen Tutorials decken die Integration, Optimierung und Best Practices ab, um LLMaaS Cloud Temple in der Produktion voll auszusch√∂pfen. Jeder Tutorial enth√§lt getesteten Code und reale Leistungsmetriken.

## üöÄ Integrationen von LangChain und Frameworks

### 1. Grundlegende Integration mit LangChain

Dieses erste Beispiel zeigt, wie die LLMaaS-API mit dem popul√§ren Framework LangChain integriert wird, indem ein benutzerdefinierter "Wrapper" erstellt wird. Ein Wrapper ist eine Klasse, die unsere API "umh√ºllt", um sie mit den internen Mechanismen von LangChain kompatibel zu machen.

#### Der Code erkl√§rt

Der folgende Code definiert eine Klasse `CloudTempleLLM`, die von der Basisklasse `LLM` von LangChain erbt. Dies erm√∂glicht es uns, ein benutzerdefiniertes Verhalten zu definieren, w√§hrend wir gleichzeitig kompatibel mit dem LangChain-√ñkosystem (Chains, Agents usw.) bleiben.

1.  **`CloudTempleLLM(LLM)`** : Unsere Klasse erbt von `LLM`, was uns verpflichtet, bestimmte Methoden zu implementieren, insbesondere `_call`.
2.  **`_call(self, prompt: str, ...)`** : Dies ist der Kern unseres Wrappers. Jedes Mal, wenn LangChain unseren Sprachmodell aufrufen muss, ruft es diese Methode auf. Darin formatieren wir eine standardm√§√üige HTTP-POST-Anfrage mit den richtigen Headern (`Authorization`) und dem vom unserer API `/v1/chat/completions` erwarteten `payload`.
3.  **`exemple_langchain_basic()`** : Diese Demonstrationsfunktion zeigt, wie unser Wrapper verwendet wird. Wir instanziieren ihn, erstellen ein `PromptTemplate`, um unsere Anfrage zu strukturieren, und kombinieren sie in einer `LLMChain`. Wenn wir die Kette ausf√ºhren (`chain.run(...)`), ruft LangChain im Hintergrund die von uns definierte `_call`-Methode auf.

Dieser Ansatz ist n√ºtzlich, wenn Sie die volle Kontrolle √ºber die Weise haben m√∂chten, wie LangChain mit der API interagiert, ist aber ausf√ºhrlicher als die Verwendung des Clients `ChatOpenAI` (siehe [API Reference](./api#langchain)).

```python

# Installation der Abh√§ngigkeiten

# pip install langchain requests pydantic

from langchain.llms.base import LLM
from langchain.schema import LLMResult, Generation
from typing import Optional, List, Any
from pydantic import Field
import requests
import json
import os

# --- Konfiguration ---

# Es wird empfohlen, Ihren API-Schl√ºssel in einer Umgebungsvariablen zu speichern
API_KEY = os.getenv("LLMAAS_API_KEY", "votre-cl√©-api-ici")
BASE_URL = "https://api.ai.cloud-temple.com/v1"

class CloudTempleLLM(LLM):
    """
    Wrapper LangChain personnalis√© pour l'API LLMaaS de Cloud Temple.
    Cette classe permet d'utiliser notre API comme un LLM standard dans LangChain.
    """
    
    api_key: str = Field()
    model_name: str = Field(default="granite3.3:8b")
    temperature: float = Field(default=0.7)
    max_tokens: int = Field(default=1000)
    
    @property
    def _llm_type(self) -> str:
        """Identifiant unique pour notre type de LLM."""
        return "cloud_temple_llmaas"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """
        La m√©thode principale qui effectue l'appel √† l'API LLMaaS.
        LangChain utilise cette m√©thode pour chaque requ√™te au mod√®le.
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        if stop:
            payload["stop"] = stop
        
        # Ex√©cution de la requ√™te POST vers l'API
        response = requests.post(
            f"{BASE_URL}/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        response.raise_for_status()  # L√®ve une exception en cas d'erreur HTTP
        result = response.json()
        
        # Retourne le contenu du message de l'assistant
        return result['choices'][0]['message']['content']

# --- Beispielanwendung ---
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

def beispiel_langchain_wrapper():
    """Zeigt die Verwendung des LLM-Wrappers mit einer LangChain-Kette an."""
    
    # 1. Initialisierung unseres benutzerdefinierten LLMs
    llm = CloudTempleLLM(
        api_key=API_KEY,
        model_name="granite3.3:8b"
    )
    
    # 2. Erstellung einer Prompt-Vorlage zur Strukturierung der Anfragen
    template = """
    Sie sind ein Experte im Bereich {domaine}. 
    Beantworten Sie diese Frage detailliert und professionell:
    
    Frage: {question}
    
    Antwort:
    """
    prompt = PromptTemplate(
        input_variables=["domaine", "question"],
        template=template
    )
    
    # 3. Erstellung einer Kette, die den Prompt und den LLM kombiniert
    chain = LLMChain(llm=llm, prompt=prompt)
    
    # 4. Ausf√ºhrung der Kette mit spezifischen Variablen
    result = chain.run(
        domaine="Cybersicherheit",
        question="Welche sind die besten Praktiken, um eine REST-API zu sichern?"
    )
    
    return result

# --- Teststart ---
if __name__ == "__main__":
    if API_KEY == "votre-cl√©-api-ici":
        print("Bitte konfigurieren Sie Ihre LLMAAS_API_KEY in Ihren Umgebungsvariablen.")
    else:
        reponse = exemple_langchain_wrapper()
        print("Antwort des Sicherheitsexperten:\n")
        print(reponse)
```

### 2. RAG (Retrieval-Augmented Generation) mit der LLMaaS-API

RAG ist eine leistungsstarke Technik, die es einem LLM erm√∂glicht, Fragen anhand einer externen Wissensbasis zu beantworten. Dieses Tutorial f√ºhrt Sie durch die Erstellung eines einfachen RAG-Pipelines unter Verwendung unserer API f√ºr Embeddings und Generierung sowie FAISS, einer Bibliothek f√ºr Vektor√§hnlichkeit, um einen In-Memory-Index zu erstellen.

#### Der Code erl√§utert

Der Pipeline besteht aus mehreren logischen Schritten:

1.  **Konfiguration**: Wir importieren die notwendigen Bibliotheken und laden unsere API-Schl√ºssel aus den Umgebungsvariablen. Wir definieren die zu verwendenden Modelle: `granite-embedding:278m` f√ºr die Vektorisierung und `granite3.3:8b` f√ºr die Generierung.
2.  **`LLMaaSEmbeddings`**: Wie im vorherigen Beispiel ben√∂tigen wir einen Wrapper, um mit unserer Embeddings-API zu interagieren. Diese Klasse verwaltet die Umwandlung von Textst√ºcken (Chunks) in numerische Vektoren (Embeddings).
3.  **`setup_rag_pipeline`**: Diese Funktion koordiniert die Erstellung der Pipeline.
    *   **Dokumente laden**: `DirectoryLoader` l√§dt Textdateien aus unserer Wissensbasis.
    *   **Aufteilung in Chunks**: `RecursiveCharacterTextSplitter` teilt Dokumente in kleinere St√ºcke. Dies ist entscheidend, damit das Embeddings-Modell den Text effizient verarbeiten kann und die √Ñhnlichkeitssuche pr√§zise ist.
    *   **Vektorisierung und Indizierung**: `FAISS.from_documents` ist ein entscheidender Schritt. Sie nimmt die Textchucks, verwendet unsere Klasse `LLMaaSEmbeddings`, um die API aufzurufen und die entsprechenden Vektoren zu erhalten, und speichert diese Vektoren in einem FAISS-Index im Speicher.
    *   **Konfiguration des LLM**: Wir verwenden `ChatOpenAI`, das nativ mit unserer API f√ºr die Antwortgenerierung kompatibel ist.
    *   **Erstellung der Kette `RetrievalQA`**: Dies ist die LangChain-Kette, die alle Elemente verkn√ºpft. Wenn eine Frage gestellt wird, tut sie Folgendes:
        a. Nutzt den `retriever` (auf Basis unseres FAISS-Index), um die relevantesten Textchucks zu finden.
        b. "Stuff" (f√ºgt) diese Chucks mit der Frage in einen Prompt ein.
        c. Sendet diesen angereicherten Prompt an das LLM, um eine kontextuelle Antwort zu generieren.
4.  **Ausf√ºhrung**: Die Funktion `main` simuliert eine reale Nutzung, indem sie tempor√§re Wissensdateien erstellt, die Pipeline aufbaut und eine Frage stellt.

```python
import os
import tempfile
import shutil
from pathlib import Path
from dotenv import load_dotenv
from typing import List


# --- Imports LangChain ---
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# --- Konfiguration ---

# L√§dt die Umgebungsvariablen (z.B. LLMAAS_API_KEY)
load_dotenv()
API_KEY = os.getenv("LLMAAS_API_KEY")
BASE_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
EMBEDDING_MODEL = "granite-embedding:278m"
LLM_MODEL = "granite3.3:8b"

# --- Benutzerdefinierte Embedding-Klasse ---
class LLMaaSEmbeddings(Embeddings):
    """Benutzerdefinierte Embedding-Klasse f√ºr die LLMaaS-API von Cloud Temple."""
    def __init__(self, api_key: str, model_name: str):
        if not api_key:
            raise ValueError("Die LLMaaS-API-Schl√ºssel darf nicht leer sein.")
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = BASE_URL
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

    def _embed(self, texts: List[str]) -> List[List[float]]:
        import httpx
        payload = {"input": texts, "model": self.model_name}
        try:
            with httpx.Client(timeout=60.0) as client:
                response = client.post(f"{self.base_url}/embeddings", headers=self.headers, json=payload)
                response.raise_for_status()
                data = response.json()['data']
                data.sort(key=lambda e: e['index'])
                return [item['embedding'] for item in data]
        except httpx.HTTPStatusError as e:
            print(f"HTTP-Fehler bei der Generierung der Embeddings: {e.response.text}")
            raise
        except Exception as e:
            print(f"Eine unerwartete Fehler ist bei der Generierung der Embeddings aufgetreten: {e}")
            raise

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._embed(texts)

    def embed_query(self, text: str) -> List[float]:
        # Die Methode _embed erwartet eine Liste, daher kapseln wir den einzelnen Text.
        return self._embed([text])[0]

# --- RAG-Pipeline ---
def setup_rag_pipeline(documents_path: str):
    """Vollst√§ndige Konfiguration der RAG-Pipeline mit den LLMaaS-Tools."""
    print("1. Laden und Aufteilen der Dokumente...")
    loader = DirectoryLoader(documents_path, glob="*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(documents)
    print(f"   -> {len(documents)} Dokument(e) geladen und in {len(splits)} chunks aufgeteilt.")
    
    print(f"2. Erstellung der Embeddings √ºber LLMaaS (Modell: {EMBEDDING_MODEL})...")
    embeddings = LLMaaSEmbeddings(api_key=API_KEY, model_name=EMBEDDING_MODEL)
    
    print("3. Erstellung des in-Memory-Vektorindexes (FAISS)...")
    vectorstore = FAISS.from_documents(splits, embeddings)
    print("   -> FAISS-Index erfolgreich erstellt.")
    
    print(f"4. Konfiguration des LLM (Modell: {LLM_MODEL})...")
    # Korrektur f√ºr die Kompatibilit√§t Pydantic/LangChain
    from langchain_core.caches import BaseCache
    from langchain_core.callbacks.base import Callbacks
    ChatOpenAI.model_rebuild()
    
    llm = ChatOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=LLM_MODEL,
        temperature=0.3,
        model_kwargs={"max_tokens": 300}
    )
    
    print("5. Erstellung der Frage/Antwort-Kette (RAG)...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    print("   -> RAG-Pipeline bereit.")
    return qa_chain

# --- Ausf√ºhrung ---
def main():
    """Hauptfunktion zum Ausf√ºhren des end-to-end RAG-Pipelines."""
    if not API_KEY:
        print("Fehler: Die Umgebungsvariable LLMAAS_API_KEY ist nicht definiert.")
        return

    # Erstellen von tempor√§ren Testdokumenten
    temp_dir = tempfile.mkdtemp()
    print(f"\nErstellung von Testdokumenten in: {temp_dir}")
    try:
        documents_content = {
            "overview.txt": "Cloud Temple ist ein franz√∂sischer Anbieter von souver√§nen Cloud-Diensten mit der Zertifizierung SecNumCloud.",
            "pricing.txt": "Die Preise der LLMaaS-API betragen 0,90 ‚Ç¨ pro Million Eingabetokens und 4 ‚Ç¨ pro Million Ausgabetokens."
        }
        for filename, content in documents_content.items():
            with open(Path(temp_dir) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # Pipeline konfigurieren und ausf√ºhren
        rag_chain = setup_rag_pipeline(temp_dir)
        
        print("\n--- Abfragen der RAG-Pipeline ---")
        question = "Wie hoch ist der Preis f√ºr Ausgabetokens der LLMaaS-API von Cloud Temple?"
        result = rag_chain({"query": question})
        
        print(f"\nFrage: {question}")
        print(f"Antwort: {result['result']}")
        print("\nQuellen, die f√ºr die Antwort verwendet wurden:")
        for source in result["source_documents"]:
            print(f"- Datei: {os.path.basename(source.metadata['source'])}")
            print(f"  Inhalt: \"{source.page_content}\"")

    finally:
        # Tempor√§res Verzeichnis bereinigen
        print(f"\nBereinigung des tempor√§ren Verzeichnisses: {temp_dir}")
        shutil.rmtree(temp_dir)

if __name__ == "__main__":
    main()
```

### 3. Integration mit einer Vektor-Datenbank (Qdrant)

F√ºr Produktionsanwendungen der RAG-Technologie wird die Verwendung einer spezialisierten Vektor-Datenbank wie **Qdrant** empfohlen. Im Gegensatz zu FAISS, das im Speicher arbeitet, bietet Qdrant Datenpersistenz, erweiterte Suchfunktionen und bessere Skalierbarkeit.

#### Der Code erkl√§rt

Dieses Tutorial passt den vorherigen RAG-Pipeline an, um Qdrant zu verwenden.

1.  **Voraussetzungen**: Der erste Schritt besteht darin, eine Instanz von Qdrant zu starten. Der einfachste Weg ist die Verwendung von Docker.
2.  **`setup_qdrant_rag_pipeline`**:
    *   **Einbettungen und Dokumente**: Die Erstellung der Einbettungen und Dokumente bleibt identisch wie im vorherigen Beispiel.
    *   **Verbindung zu Qdrant**: Anstatt einen FAISS-Index zu erstellen, verwenden wir `Qdrant.from_documents`. Diese Methode von LangChain verwaltet mehrere Schritte:
        a. Sie verbindet sich mit Ihrer Qdrant-Instanz √ºber die angegebene URL.
        b. Sie erstellt eine neue "Sammlung" (das √Ñquivalent einer Tabelle in einer SQL-Datenbank), falls sie noch nicht existiert.
        c. Sie ruft unsere Klasse `LLMaaSEmbeddings` auf, um die Dokumente zu vektorisieren.
        d. Sie f√ºgt die Dokumente und ihre Vektoren in die Qdrant-Sammlung ein.
    *   **`force_recreate=True`**: F√ºr dieses Tutorial verwenden wir diesen Parameter, um sicherzustellen, dass die Sammlung bei jeder Ausf√ºhrung leer ist. In der Produktion w√ºrden Sie ihn auf `False` setzen, um Ihre Daten beizubehalten.
3.  **Der Rest des Pipelines** (Konfiguration des LLM, Erstellung der Kette `RetrievalQA`) ist identisch, was die Flexibilit√§t von LangChain zeigt: Es reicht aus, die Quelle des `retriever` (der Informationssuche) zu √§ndern, um von FAISS zu Qdrant zu wechseln.

:::info Voraussetzungen: Qdrant starten
F√ºr dieses Tutorial ben√∂tigen Sie eine Qdrant-Instanz. Sie k√∂nnen sie leicht mit Docker starten:

```bash

# 1. Das letzte Image von Qdrant herunterladen
docker pull qdrant/qdrant

# 2. Qdrant-Container starten
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
```
:::

Der folgende Code zeigt, wie der RAG-Pipeline angepasst wird, um Qdrant als Vektor-Datenbank zu verwenden.

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Qdrant
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List
from langchain_core.embeddings import Embeddings


# (Die Klasse LLMaaSEmbeddings ist die gleiche wie im vorherigen Beispiel,

# wir verwenden sie hier wieder. Stellen Sie sicher, dass sie in Ihrem Skript definiert ist.)

# --- Konfiguration ---
load_dotenv()
API_KEY = os.getenv("LLMAAS_API_KEY")
BASE_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
EMBEDDING_MODEL = "granite-embedding:278m"
LLM_MODEL = "granite3.3:8b"
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_COLLECTION_NAME = "tutorial_collection"

# --- Embedding-Klasse (wiederholt aus dem vorherigen Beispiel) ---
class LLMaaSEmbeddings(Embeddings):
    def __init__(self, api_key: str, model_name: str):
        if not api_key: raise ValueError("API Key ist erforderlich.")
        self.api_key, self.model_name, self.base_url = api_key, model_name, BASE_URL
        self.headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
    def _embed(self, texts: List[str]) -> List[List[float]]:
        import httpx
        payload = {"input": texts, "model": self.model_name}
        with httpx.Client(timeout=60.0) as client:
            r = client.post(f"{self.base_url}/embeddings", headers=self.headers, json=payload)
            r.raise_for_status()
            data = r.json()['data']
            data.sort(key=lambda e: e['index'])
            return [item['embedding'] for item in data]
    def embed_documents(self, texts: List[str]) -> List[List[float]]: return self._embed(texts)
    def embed_query(self, text: str) -> List[float]: return self._embed([text])[0]

def setup_qdrant_rag_pipeline():
    """Konfiguriert und gibt einen RAG-Pipeline mit Qdrant zur√ºck."""
    print("1. Initialisierung des Embedding-Clients LLMaaS...")
    embeddings = LLMaaSEmbeddings(api_key=API_KEY, model_name=EMBEDDING_MODEL)

    print("2. Vorbereitung der Dokumente...")
    documents_content = [
        "Cloud Temple ist ein franz√∂sischer Anbieter von souver√§nen Cloud-Diensten mit der Zertifizierung SecNumCloud.",
        "Die Preise f√ºr LLMaaS betragen 0,90 ‚Ç¨ f√ºr Eingabe und 4 ‚Ç¨ f√ºr Ausgabe pro Million Tokens."
    ]
    documents = [Document(page_content=d) for d in documents_content]
    
    print(f"3. Verbindung zu Qdrant und Bev√∂lkerung der Sammlung '{QDRANT_COLLECTION_NAME}'...")
    vectorstore = Qdrant.from_documents(
        documents,
        embeddings,
        url=QDRANT_URL,
        collection_name=QDRANT_COLLECTION_NAME,
        force_recreate=True, # Stellt eine saubere Sammlung f√ºr das Tutorial sicher
    )
    print("   -> Sammlung erfolgreich erstellt und bef√ºllt.")

    print(f"4. Konfiguration des LLM ({LLM_MODEL})...")
    llm = ChatOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=LLM_MODEL,
        temperature=0.3
    )

    print("5. Erstellung der RAG-Kette...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )
    print("   -> RAG-Pipeline mit Qdrant bereit.")
    return qa_chain

# --- Ausf√ºhrung ---
def main_qdrant():
    """Hauptfunktion zum Ausf√ºhren des RAG-Pipelines mit Qdrant."""
    if not API_KEY:
        print("Fehler: Die Umgebungsvariable LLMAAS_API_KEY ist nicht definiert.")
        return
        
    try:
        rag_chain = setup_qdrant_rag_pipeline()
        question = "Welche Preise hat die LLMaaS-API von Cloud Temple ?"
        
        print(f"\n--- Abfrage des Pipelines ---")
        result = rag_chain({"query": question})

        print(f"\nFrage: {question}")
        print(f"Antwort: {result['result']}")
        print("\nQuellen, die f√ºr die Antwort verwendet wurden:")
        for source in result["source_documents"]:
            print(f"- Inhalt: \"{source.page_content}\"")
            
    except Exception as e:
        print(f"\nEin Fehler ist aufgetreten: {e}")
        print("Stellen Sie sicher, dass der Qdrant-Container ausgef√ºhrt wird.")

if __name__ == "__main__":
    main_qdrant()
```

### 4. LangChain-Agenten mit Werkzeugen

Ein Agent ist ein LLM, das nicht nur eine Frage beantwortet, sondern einen **Satz von Werkzeugen** (Funktionen, APIs usw.) verwenden kann, um eine komplexere Antwort zu konstruieren. Er kann sich √ºberlegen, ein Problem zerteilen, ein Werkzeug w√§hlen, es ausf√ºhren, das Ergebnis beobachten und diesen Zyklus wiederholen, bis eine Endantwort erzielt wird.

#### Der Code erl√§utert

Dieses Beispiel erstellt einen einfachen Agenten, der zwei Tools verwenden kann: eines zur Abfrage einer (simulierten) Cloud Temple API und eines zur Durchf√ºhrung von Berechnungen.

1.  **Definition der Tools**: Die Klassen `CloudTempleAPITool` und `CalculatorTool` erben von `BaseTool`. Jedes Tool hat:
    *   Ein `name`: Ein einfacher und beschreibender Name.
    *   Eine `description`: **Entscheidend**, dies ist das, was der LLM liest, um zu entscheiden, welches Tool verwendet werden soll. Sie muss sehr klar sein, was das Tool tut und wann es verwendet werden soll.
    *   Eine Methode `_run`: Der Code, der tats√§chlich ausgef√ºhrt wird, wenn der Agent dieses Tool w√§hlt.
2.  **`create_agent_with_tools`**:
    *   **Initialisierung des LLM**: Wir verwenden unseren Wrapper `CloudTempleLLM`, der im ersten Tutorial definiert wurde.
    *   **Liste der Tools**: Wir geben dem Agenten die Liste der Tools, die er verwenden darf.
    *   **Agent-Prompt**: Der Prompt ist sehr spezifisch. Es handelt sich um einen ‚ÄûDenkprozess-Prompt‚Äú, der dem LLM anweist, wie zu denken (`Thought`), eine Aktion zu w√§hlen (`Action`), eine Eingabe f√ºr diese Aktion bereitzustellen (`Action Input`) und das Ergebnis zu beobachten (`Observation`). Dies ist das zentrale Mechanismus des ReAct-Frameworks (Reasoning and Acting), das hier verwendet wird.
    *   **Erstellung des Agents**: `create_react_agent` verbindet LLM, Tools und Prompt, um den Agenten zu erstellen.
    *   **`AgentExecutor`**: Dies ist der Motor, der den Agenten in einer Schleife ausf√ºhrt, bis er eine `Final Answer` produziert. Der Parameter `verbose=True` ist sehr n√ºtzlich, um den ‚Äûinneren Dialog‚Äú des Agenten (seine Gedanken, Aktionen usw.) zu sehen.

```python
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain.tools import BaseTool
from langchain.prompts import PromptTemplate
import requests
import json
import os


# (Die Klasse CloudTempleLLM ist die gleiche wie im ersten Beispiel)

# --- Definition der Werkzeuge ---

class CloudTempleAPITool(BaseTool):
    """Ein Werkzeug, das einen Aufruf einer internen API simuliert, um Informationen zu Diensten zu erhalten."""
    name = "cloud_temple_api_checker"
    description = "N√ºtzlich, um Informationen zu Diensten, Produkten und Angeboten von Cloud Temple zu erhalten."

    def _run(self, query: str) -> str:
        # In einem echten Fall w√ºrde dies einen echten API-Aufruf ausl√∂sen.
        print(f"--- Werkzeug CloudTempleAPITool mit der Anfrage aufgerufen: '{query}' ---")
        if "service" in query.lower():
            return "Cloud Temple bietet folgende Dienste an: IaaS, PaaS, LLMaaS, Managed Security."
        return "Information nicht gefunden."

    async def _arun(self, query: str) -> str:
        # Asynchrone Implementierung ist f√ºr dieses Beispiel nicht erforderlich.
        raise NotImplementedError("Das API-Werkzeug unterst√ºtzt keine asynchrone Ausf√ºhrung.")

class SimpleCalculatorTool(BaseTool):
    """Ein einfaches Werkzeug zur Durchf√ºhrung mathematischer Berechnungen."""
    name = "simple_calculator"
    description = "N√ºtzlich zur Durchf√ºhrung einfacher mathematischer Berechnungen. Akzeptiert einen g√ºltigen Python-Ausdruck."

    def _run(self, expression: str) -> str:
        print(f"--- Werkzeug SimpleCalculatorTool mit dem Ausdruck aufgerufen: '{expression}' ---")
        try:
            # ACHTUNG: eval() ist in der Produktion gef√§hrlich. Dies ist nur f√ºr die Demo.
            return str(eval(expression))
        except Exception as e:
            return f"Berechnungsfehler: {e}"

    async def _arun(self, expression: str) -> str:
        raise NotImplementedError("Das Rechenwerkzeug unterst√ºtzt keine asynchrone Ausf√ºhrung.")

# --- Erstellung des Agents ---

def create_agent():
    """Konfiguriert und gibt einen LangChain-Agenten mit den definierten Tools zur√ºck."""
    print("1. Initialisierung des LLM f√ºr den Agenten...")
    llm = CloudTempleLLM(api_key=os.getenv("LLMAAS_API_KEY", "votre-cl√©-api-ici"))

    tools = [CloudTempleAPITool(), SimpleCalculatorTool()]
    
    # Der Prompt-Template ist entscheidend: Er leitet das Denken des LLMs.
    template = """
    Beantworte die folgenden Fragen so gut du kannst. Du hast Zugriff auf die folgenden Tools:

    {tools}

    Verwende das folgende Format:

    Frage: Die Frage, auf die du antworten musst
    Gedanke: Du musst immer dar√ºber nachdenken, was du tun wirst
    Aktion: Die auszuf√ºhrende Aktion, muss einer der folgenden sein [{tool_names}]
    Aktionseingabe: Die Eingabe f√ºr die Aktion
    Beobachtung: Das Ergebnis der Aktion
    ... (diese Sequenz aus Gedanke/Aktion/Aktionseingabe/Beobachtung kann sich wiederholen)
    Gedanke: Ich kenne nun die endg√ºltige Antwort.
    Endg√ºltige Antwort: Die endg√ºltige Antwort auf die urspr√ºngliche Frage

    Los geht's!

    Frage: {input}
    Gedanke:{agent_scratchpad}
    """
    
    prompt = PromptTemplate.from_template(template)
    
    print("2. Erstellung des Agents mit dem ReAct-Framework...")
    agent = create_react_agent(llm, tools, prompt)

    # Der AgentExecutor ist f√ºr die Ausf√ºhrung der Agenten-Zyklen verantwortlich.
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    print("   -> Agent bereit.")
    return agent_executor

# --- Ausf√ºhrung ---

def run_agent():
    """F√ºhrt den Agenten mit verschiedenen Fragen aus, um seine F√§higkeiten zu testen."""
    if os.getenv("LLMAAS_API_KEY") is None:
        print("Bitte konfigurieren Sie Ihre LLMAAS_API_KEY.")
        return
        
    agent_executor = create_agent()
    
    print("\n--- Test 1: Frage, die ein Informationswerkzeug erfordert ---")
    question1 = "Welche Dienste bietet Cloud Temple an?"
    response1 = agent_executor.invoke({"input": question1})
    print(f"\nEndg√ºltige Antwort des Agents: {response1['output']}")
    
    print("\n--- Test 2: Frage, die eine Berechnung erfordert ---")
    question2 = "Was ist das Ergebnis von 125 * 8 + 50?"
    response2 = agent_executor.invoke({"input": question2})
    print(f"\nEndg√ºltige Antwort des Agents: {response2['output']}")

if __name__ == "__main__":
    run_agent()
```

### 5. OpenAI SDK-Integration

**Transparente Migration von OpenAI**

```python
from openai import OpenAI

# Konfiguration f√ºr Cloud Temple LLMaaS
def setup_cloud_temple_client():
    """Client-Konfiguration f√ºr OpenAI f√ºr Cloud Temple"""
    
    client = OpenAI(
        api_key="your-cloud-temple-api-key",
        base_url="https://api.ai.cloud-temple.com/v1"
    )
    
    return client

def test_openai_compatibility():
    """Test der Kompatibilit√§t mit dem OpenAI-SDK"""
    
    client = setup_cloud_temple_client()
    
    # Standard-Chat-Completion
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[
            {"role": "system", "content": "Sie sind ein professioneller IA-Assistent."},
            {"role": "user", "content": "Erkl√§ren Sie mir die Cloud-Native-Architektur."}
        ],
        max_tokens=300,
        temperature=0.7
    )
    
    print(f"Antwort: {response.choices[0].message.content}")
    
    # Streaming
    stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[
            {"role": "user", "content": "Schreibe ein Gedicht √ºber KI."}
        ],
        stream=True,
        max_tokens=200
    )
    
    print("Stream:")
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="")
    print()

# Kompatibilit√§tstest
test_openai_compatibility()

### 5. Semantic Kernel-Integration (Microsoft)

[Semantic Kernel](https://learn.microsoft.com/fr-fr/semantic-kernel/overview/) ist ein open-source SDK von Microsoft, das es erm√∂glicht, LLMs in .NET-, Python- und Java-Anwendungen zu integrieren. Obwohl es f√ºr Azure OpenAI-Dienste optimiert ist, erm√∂glicht seine Flexibilit√§t die Verwendung mit jeder kompatiblen OpenAI-API, einschlie√ülich unserer eigenen.

#### Der Code erl√§utert

Dieses Beispiel ben√∂tigt nicht das vollst√§ndige Semantic Kernel SDK. Es zeigt, wie der **Begriff "semantische Funktion"** durch einen einfachen Aufruf unserer API implementiert werden kann. Eine semantische Funktion ist im Grunde ein strukturierter Prompt, der an ein LLM gesendet wird, um eine bestimmte Aufgabe zu erledigen.

1.  **`semantic_kernel_simple()`** : Diese Funktion simuliert eine "Zusammenfassungsfunktion".
2.  **Strukturierter Prompt** : Wir verwenden eine `system`-Nachricht, um dem LLM eine Rolle zu geben ("Sie sind ein Experte f√ºr Dokumentensynthesen.") und eine `user`-Nachricht mit dem zu zusammenfassenden Text. Dies ist der Kern des Konzepts der semantischen Funktion.
3.  **Direkter API-Aufruf** : Ein einfacher Aufruf `requests.post` an unseren Endpunkt `/v1/chat/completions` reicht aus, um die Funktion auszuf√ºhren.

Dieses Beispiel zeigt, dass es nicht immer notwendig ist, ein schweres Framework zu verwenden. F√ºr einfache und gut definierte Aufgaben ist ein direkter API-Aufruf von LLMaaS oft die effizienteste und leistungsst√§rkste L√∂sung.

```python
import requests
import os
from dotenv import load_dotenv

def semantic_kernel_simulation():
    """
    Simuliert eine "semantische Funktion" zur Zusammenfassung durch direkten Aufruf der LLMaaS-API.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Bitte setzen Sie die Umgebungsvariable LLMAAS_API_KEY.")
        return

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    text_to_summarize = """
    K√ºnstliche Intelligenz (KI) ver√§ndert viele Industriebereiche durch Automatisierung von Aufgaben, 
    Optimierung von Prozessen und erm√∂glicht fortgeschrittene pr√§diktive Analysen. 
    Cloud Temple mit seinem souver√§nen und zertifizierten LLMaaS-Angebot gem√§√ü SecNumCloud erm√∂glicht Unternehmen 
    die Integration dieser KI-F√§higkeiten, wobei die Sicherheit und Vertraulichkeit ihrer Daten gew√§hrleistet wird.
    """
    
    # Der Prompt kombiniert eine Anweisung (Systemrolle) und Daten (Nutzerrolle)
    payload = {
        "model": "granite3.3:8b",
        "messages": [
            {"role": "system", "content": "Sie sind ein Experte f√ºr die Zusammenfassung technischer Dokumente."},
            {"role": "user", "content": f"Zusammenfassung des folgenden Textes in einer kurzen Satz: {text_to_summarize}"}
        ],
        "max_tokens": 100,
        "temperature": 0.5
    }
    
    try:
        response = requests.post(
            "https://api.ai.cloud-temple.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        summary = result['choices'][0]['message']['content']
        
        print("Originaltext:\n", text_to_summarize)
        print("\nGenerierte Zusammenfassung:\n", summary)
        return summary
        
    except requests.exceptions.RequestException as e:
        print(f"Es ist ein API-Fehler aufgetreten: {e}")

if __name__ == "__main__":
    semantic_kernel_simulation()
```

### 6. Framework Haystack

[Haystack](https://haystack.deepset.ai/) ist ein weiteres leistungsstarkes Open-Source-Framework zur Erstellung von Anwendungen f√ºr semantische Suche, RAG und Agenten. Wie bei Semantic Kernel kann unsere API direkt integriert werden.

#### Der Code erl√§utert

Dieses Beispiel simuliert einen grundlegenden Haystack-Pipeline f√ºr die Antwortsuche in einem gegebenen Kontext (Question Answering).

1.  **`process_with_context`** : Diese Funktion stellt den Kern eines QA-Pipelines dar. Sie erh√§lt einen `Kontext` (z. B. einen Absatz eines Dokuments) und eine `Frage`.
2.  **Kontextueller Prompt** : Der Prompt ist sorgf√§ltig strukturiert, um sowohl den Kontext als auch die Frage zu enthalten. Dies ist eine grundlegende Technik in RAG: Wir geben dem LLM relevante Informationen, damit es eine faktische Antwort formulieren kann.
3.  **API-Aufruf** : Ein einfacher `requests.post`-Aufruf an unsere API reicht aus. Das LLM erh√§lt den Kontext und die Frage, und seine Aufgabe besteht darin, eine Antwort basierend *nur* auf den bereitgestellten Informationen zu synthetisieren.

Dieses Beispiel zeigt die Flexibilit√§t der LLMaaS-API, die als Grundbaustein f√ºr die Textgenerierung in jedem Framework dienen kann, sogar in solchen, f√ºr die keine offizielle Integration existiert.

```python
import requests
import os
from dotenv import load_dotenv

def haystack_simulation():
    """
    Simuliert einen Question-Answering-Pipeline-Modus von Haystack
    durch einen direkten API-Aufruf an LLMaaS.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Bitte setzen Sie die Umgebungsvariable LLMAAS_API_KEY.")
        return

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # Der Kontext ist die Information, die das LLM verwenden darf.
    context = """
    Ein souver√§ner Cloud-Service ist eine Cloud-Computing-Infrastruktur, die vollst√§ndig innerhalb der Grenzen eines bestimmten Landes liegt und dessen Gesetzen unterliegt. 
    Die Hauptvorteile sind die Gew√§hrleistung der Datenresidenz, die Einhaltung lokaler Vorschriften (z. B. der DSGVO in Europa) und ein erh√∂hter Schutz vor Zugriffen durch ausl√§ndische Entit√§ten gem√§√ü extraterritorialen Gesetzen wie der US-CLOUD Act.
    """
    
    question = "Welche Vorteile bietet ein souver√§ner Cloud-Service?"
    
    # Der Prompt leitet das LLM an, seine Antwort auf den bereitgestellten Kontext zu st√ºtzen.
    prompt = f"""
    Basierend ausschlie√ülich auf dem folgenden Kontext beantworte die Frage.
    
    Kontext:
    ---
    {context}
    ---
    
    Frage: {question}
    """
    
    payload = {
        "model": "granite3.3:8b",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 200,
        "temperature": 0.2 # Niedrige Temperatur f√ºr eine faktische Antwort
    }
    
    try:
        response = requests.post(
            "https://api.ai.cloud-temple.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        answer = result['choices'][0]['message']['content']
        
        print(f"Frage: {question}")
        print("\nGenerierte Antwort:\n", answer)
        return answer
        
    except requests.exceptions.RequestException as e:
        print(f"Es ist ein API-Fehler aufgetreten: {e}")

if __name__ == "__main__":
    haystack_simulation()
```

### 7. Integration von LlamaIndex

[LlamaIndex](https://www.llamaindex.ai/) ist ein Framework, das speziell f√ºr die Erstellung von RAG-Anwendungen entwickelt wurde. Es bietet hochwertige Komponenten f√ºr die Daten-Eingabe, Indexierung und Abfrage. Unsere API, die mit der OpenAI-Schnittstelle kompatibel ist, integriert sich sehr einfach.

#### Der Code erkl√§rt

Dieses Beispiel zeigt, wie LlamaIndex so konfiguriert wird, dass die LLMaaS-API f√ºr die Textgenerierung verwendet wird, w√§hrend gleichzeitig ein lokaler Embedding-Modell f√ºr die Vectorisierung genutzt wird.

1.  **`setup_and_run_llamaindex`** : Diese Funktion koordiniert den gesamten Prozess.
    *   **Konfiguration des LLM** : LlamaIndex bietet eine Klasse `OpenAILike`, die es erm√∂glicht, sich an jede API anzuschlie√üen, die das OpenAI-Format respektiert. Es reicht aus, unsere `api_base` und eine `api_key` bereitzustellen. Dies ist die einfachste Methode, um unsere LLM kompatibel zu machen.
    *   **Konfiguration der Embeddings** : F√ºr dieses Beispiel verwenden wir ein lokales Embedding-Modell (`HuggingFaceEmbedding`). Dies zeigt die Flexibilit√§t von LlamaIndex, das es erm√∂glicht, Komponenten zu kombinieren. Sie k√∂nnten genauso gut die Klasse `LLMaaSEmbeddings` aus fr√ºheren Beispielen verwenden, um unsere Embedding-API zu nutzen.
    *   **`Settings`** : Das `Settings`-Objekt von LlamaIndex ist eine praktische M√∂glichkeit, die Standardkomponenten (LLM, Embedding-Modell, Chunk-Gr√∂√üe usw.) zu konfigurieren, die von anderen LlamaIndex-Objekten verwendet werden.
    *   **Daten-Ingestion** : `SimpleDirectoryReader` l√§dt Dokumente aus einem Verzeichnis.
    *   **Erstellung des Indexes** : `VectorStoreIndex.from_documents` ist die Hochlevel-Methode von LlamaIndex. Sie verwaltet automatisch das Aufteilen in Chunks, die Vectorisierung der Chunks (unter Verwendung des `embed_model`, das in `Settings` konfiguriert ist) und die Erstellung des Indexes im Speicher.
    *   **Abfrage-Motor** : `.as_query_engine()` erstellt eine einfache Schnittstelle, um Fragen an unseren Index zu stellen. Wenn Sie `.query()` aufrufen, vectorisiert der Motor Ihre Frage, findet die relevantesten Dokumente im Index und sendet diese mit der Frage an das LLM (das in `Settings` konfiguriert ist), um eine Antwort zu generieren.

```python

# Abh√§ngigkeiten:

# pip install llama-index llama-index-llms-openai-like llama-index-embeddings-huggingface

import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import shutil

def setup_and_run_llamaindex():
    """
    Konfiguriert und f√ºhrt einen einfachen RAG-Pipeline mit LlamaIndex und der LLMaaS-API aus.
    """
    load_dotenv()
    api_key = os.getenv("LLMAAS_API_KEY")
    if not api_key:
        print("Bitte setzen Sie die Umgebungsvariable LLMAAS_API_KEY.")
        return

    # 1. Konfiguration des LLMs, um die LLMaaS-API √ºber die OpenAILike-Schnittstelle zu verwenden
    print("1. Konfiguration des LLMs, um auf die LLMaaS-API zu verweisen...")
    llm = OpenAILike(
        api_key=api_key,
        api_base="https://api.ai.cloud-temple.com/v1",
        model="granite3.3:8b",
        is_chat_model=True,
        # Manchmal ist es notwendig, Kontextparameter f√ºr bestimmte Modelle hinzuzuf√ºgen
        # context_window=3900, 
    )

    # 2. Konfiguration des Embedding-Modells (lokal in diesem Beispiel f√ºr Einfachheit)
    print("2. Konfiguration des lokalen Embedding-Modells...")
    embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # 3. Anwenden der globalen Konfiguration √ºber das Settings-Objekt von LlamaIndex
    Settings.llm = llm
    Settings.embed_model = embed_model
    print("   -> LLM und Embedding-Modell konfiguriert.")

    # 4. Erstellung einer einfachen Wissensbasis in einem tempor√§ren Verzeichnis
    print("4. Erstellung und Laden einer tempor√§ren Wissensbasis...")
    temp_dir = "temp_llama_data"
    os.makedirs(temp_dir, exist_ok=True)
    knowledge_file = os.path.join(temp_dir, "knowledge.txt")
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write("Das LLMaaS-Angebot von Cloud Temple ist eine souver√§ne generative KI-L√∂sung, "
                "die vollst√§ndig in Frankreich betrieben wird und von der ANSSI als SecNumCloud qualifiziert ist.")
    
    documents = SimpleDirectoryReader(temp_dir).load_data()
    print(f"   -> {len(documents)} Dokument(e) geladen.")

    # 5. Erstellung des Vektorindex. LlamaIndex verwaltet das Chunking und das Embedding.
    print("5. Erstellung des Vektorindex...")
    index = VectorStoreIndex.from_documents(documents)
    print("   -> Index erstellt.")

    # 6. Erstellung des Abfragesystems und Abfrage der Wissensbasis
    print("6. Erstellung des Abfragesystems und Abfrage...")
    query_engine = index.as_query_engine()
    question = "Welche Souver√§nit√§tsgarantien bietet das LLMaaS-Angebot?"
    response = query_engine.query(question)
    
    print(f"\nFrage: {question}")
    print(f"Antwort: {response}")

    # Bereinigung des tempor√§ren Verzeichnisses
    shutil.rmtree(temp_dir)
    print(f"\nTempor√§res Verzeichnis '{temp_dir}' gel√∂scht.")

if __name__ == "__main__":
    setup_and_run_llamaindex()
```

### 8. Konfiguration der CLINE-Erweiterung f√ºr VSCode

Dieses Tutorial f√ºhrt Sie durch die Konfiguration der CLINE-Erweiterung in Visual Studio Code, um die Sprachmodelle von Cloud Temple direkt in Ihrem Editor zu verwenden.

#### Konfigurationsschritte

1.  **CLINE-Einstellungen √∂ffnen**: √ñffnen Sie in VSCode die Einstellungen der CLINE-Erweiterung.
2.  **Neues Modell erstellen**: F√ºgen Sie eine neue Modellkonfiguration hinzu.
3.  **Felder ausf√ºllen**: Konfigurieren Sie die Felder wie folgt, basierend auf dem Bild unten.

    ![CLINE-Konfiguration f√ºr LLMaaS](./images/cline_configuration.png)

    *   **API Provider**: W√§hlen Sie `OpenAI Compatible`.
    *   **Base URL**: Geben Sie den Endpunkt der Cloud Temple LLMaaS-API ein: `https://api.ai.cloud-temple.com/v1`.
    *   **OpenAI Compatible API Key**: F√ºgen Sie den API-Schl√ºssel ein, den Sie in der Cloud Temple-Konsole generiert haben.
    
    :::tip API-Schl√ºssel generieren
    Um Ihren API-Schl√ºssel zu generieren, gehen Sie zur Cloud Temple-Konsole, zum Abschnitt **LLMaaS** > **API-Schl√ºssel**, und klicken Sie dann auf **"API-Schl√ºssel erstellen"**.
    
    ![API-Schl√ºssel aus der Konsole erstellen](./images/console_create_api_key.png)
    :::
    
    *   **Model ID**: Geben Sie das Modell an, das Sie verwenden m√∂chten, z. B. `qwen3-coder:30b`. Eine Liste der verf√ºgbaren Modelle finden Sie im Abschnitt [Modelle](./models.md).
    *   **Model Configuration**:
        *   **Supports Images**: Aktivieren Sie dieses Kontrollk√§stchen, wenn das Modell Bilder unterst√ºtzt.
        *   **Supports browser use**: Aktivieren Sie dieses Kontrollk√§stchen.
        *   **Context Window Size**: Geben Sie die Gr√∂√üe des Kontextfensters des Modells an (z. B. `128000`).
        *   **Max Output Tokens**: Lassen Sie den Wert auf `-1` f√ºr eine standardm√§√üig unbegrenzte Ausgabe.
        *   **Temperature**: Passen Sie die Temperatur nach Bedarf an (z. B. `0`).

Sie k√∂nnen jetzt ein Modell in CLINE ausw√§hlen und es verwenden, um Code zu generieren, Fragen zu beantworten usw.

## üí° Fortgeschrittene Beispiele

Sie finden im folgenden GitHub-Verzeichnis eine Sammlung von Code-Beispielen und Skripten, die die verschiedenen Funktionen und Anwendungsf√§lle des LLM as a Service (LLMaaS)-Angebots von Cloud Temple demonstrieren:

[Cloud-Temple/product-llmaas-how-to](https://github.com/Cloud-Temple/product-llmaas-how-to/tree/main)

Darin finden Sie praktische Anleitungen f√ºr:
- __Informationsextraktion und Textanalyse:__ F√§higkeit, Dokumente zu analysieren, um strukturierte Daten wie Entit√§ten, Ereignisse, Beziehungen und Attribute zu extrahieren, basierend auf spezifischen Ontologien f√ºr bestimmte Bereiche (z. B. Recht, Personalwesen, IT).

- __Konversationelle Interaktion und Chatbots:__ Entwicklung konversationeller Agenten, die mit Benutzern kommunizieren, Austauschverl√§ufe verfolgen, Systemanweisungen (System-Prompts) verwenden und externe Tools aufrufen k√∂nnen.

- __Audio-Transkription (Speech-to-Text):__ Konvertierung von Audio-Inhalten in Text, einschlie√ülich gro√üer Dateien, durch Techniken wie Aufteilung, Normalisierung und Batch-Verarbeitung.

- __Text√ºbersetzung:__ √úbersetzung von Dokumenten von einer Sprache in eine andere, wobei der Kontext √ºber mehrere Segmente verwaltet wird, um die Koh√§renz zu verbessern.

- __Modellverwaltung und -bewertung:__ Auflistung der √ºber die API verf√ºgbaren Sprachmodelle, Anzeige ihrer Spezifikationen und Ausf√ºhrung von Tests zur Vergleich der Leistung.

- __Echtzeit-Antwort-Streaming:__ Demonstration der F√§higkeit, Antworten der Modelle in Echtzeit (Token f√ºr Token) zu empfangen und anzuzeigen, was f√ºr interaktive Anwendungen entscheidend ist.
- __RAG-Pipeline mit Speicher-Knowledge-Base:__ P√§dagogischer RAG-Demonstrator, der den Funktionsweise des Retrieval-Augmented Generation veranschaulicht. Nutzt die LLMaaS-API f√ºr Embedding und Generierung, mit Speicherung der Vektoren im Speicher (FAISS) f√ºr eine klare Verst√§ndnis des Prozesses.
- __RAG-Pipeline mit Vektor-Datenbank (Qdrant):__ Vollst√§ndiger und containerisierter RAG-Demonstrator, der Qdrant als Vektor-Datenbank verwendet. Die LLMaaS-API wird f√ºr die Embedding von Dokumenten und die Generierung erweiterten Antworten genutzt.
