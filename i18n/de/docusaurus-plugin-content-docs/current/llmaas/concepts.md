---
title: Konzepte
sidebar_position: 3
---

# Konzepte und Architektur von LLMaaS

## Ãœberblick

Der Dienst **LLMaaS** (Large Language Models as a Service) von Cloud Temple bietet einen sicheren und souverÃ¤nen Zugang zu den fortschrittlichsten Modellen der kÃ¼nstlichen Intelligenz, mit der **SecNumCloud-Zertifizierung** der ANSSI.

## ğŸ—ï¸ Technische Architektur

### Cloud Temple-Infrastruktur

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Technische Architektur von LLMaaS Cloud Temple" />

### Hauptkomponenten

#### 1. **API Gateway LLMaaS**
- **OpenAI-kompatibel** : Nahtlose Integration in bestehende Ã–kosysteme
- **Rate Limiting** : Verwaltung von Quoten je Abonnementstier
- **Load Balancing** : Intelligente Verteilung auf 12 GPU-Server
- **Monitoring** : Echtzeit-Metriken und Alarmierung

#### 2. **Authentifizierungsdienst**
- **Sichere API-Token**: Automatische Rotation
- **Zugriffskontrolle**: Granulare Berechtigungen pro Modell
- **Audit-Trails**: VollstÃ¤ndige RÃ¼ckverfolgbarkeit der Zugriffe

## ğŸ¤– Modelle und Token

### Modellkatalog

*VollstÃ¤ndiger Katalog: [Liste der Modelle](./models)*

### Token-Verwaltung

#### **Token-Typen**
- **Eingabe-Token**: Ihr Prompt und der Kontext
- **Ausgabe-Token**: Vom Modell generierte Antwort
- **System-Token**: Metadaten und Anweisungen

#### **Kostenberechnung**
```
Gesamtkosten = (Eingabe-Token Ã— 0,9â‚¬/M) + (Ausgabe-Token Ã— 4â‚¬/M) + (Reasoning-Ausgabe-Token Ã— 21â‚¬/M)
```

#### **Optimierung**
- **Kontextfenster**: Wiederverwenden Sie GesprÃ¤che, um Kosten zu sparen
- **Passende Modelle**: WÃ¤hlen Sie die GrÃ¶ÃŸe entsprechend der KomplexitÃ¤t
- **Maximale Tokens**: Begrenzen Sie die LÃ¤nge der Antworten

### Tokenisierung

```python
# Beispiel zur SchÃ¤tzung von Tokens
def estimate_tokens(text: str) -> int:
    """Approximative SchÃ¤tzung: 1 Token â‰ˆ 4 Zeichen"""
    return len(text) // 4

prompt = "ErklÃ¤ren Sie die Photosynthese"
response_max = 200  # gewÃ¼nschte maximale Anzahl an Tokens

estimated_input = estimate_tokens(prompt)  # ~6 Tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"GeschÃ¤tzter Kostenbetrag: {total_cost:.6f}â‚¬")
```

## ğŸ”’ Sicherheit und Compliance

### SecNumCloud-Qualifizierung

Der LLMaaS-Dienst wird auf einer technischen Infrastruktur gehostet, die die **SecNumCloud 3.2-Qualifizierung** der ANSSI besitzt, was Folgendes garantiert:

#### **Datenschutz**
- **End-zu-End-VerschlÃ¼sselung**: TLS 1.3 fÃ¼r alle Kommunikationen
- **Sichere Speicherung**: Daten im Ruhezustand verschlÃ¼sselt (AES-256)
- **Isolierung**: Dedizierte Umgebungen pro Mandant

#### **Digitale SouverÃ¤nitÃ¤t**
- **Hosting in Frankreich**: Zertifizierte Cloud Temple-Rechenzentren
- **FranzÃ¶sisches Recht**: Native DSGVO-KonformitÃ¤t
- **Keine Exposition**: Keine Datentransfers in auslÃ¤ndische Clouds

#### **Audit und RÃ¼ckverfolgbarkeit**
- **VollstÃ¤ndige Protokolle**: Alle Interaktionen werden verfolgt
- **Aufbewahrung**: Speicherung gemÃ¤ÃŸ gesetzlichen Richtlinien
- **Compliance**: Audit-Berichte verfÃ¼gbar

### Sicherheitskontrollen

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Sicherheitskontrollen LLMaaS" />

### Prompt-Sicherheit

Die Prompt-Analyse ist eine **native und integrierte** Sicherheitsfunktion der LLMaaS-Plattform. Sie ist standardmÃ¤ÃŸig aktiviert und zielt darauf ab, Versuche von "Jailbreaking" oder Injektion bÃ¶sartiger Prompts zu erkennen und zu verhindern, bevor sie das Modell Ã¼berhaupt erreichen. Dieser Schutz basiert auf einem mehrschichtigen Ansatz.

:::tip Support kontaktieren zur Deaktivierung
Es ist mÃ¶glich, diese Sicherheitsanalyse fÃ¼r sehr spezifische AnwendungsfÃ¤lle zu deaktivieren, obwohl dies nicht empfohlen wird. Bei Fragen dazu oder zur Beantragung der Deaktivierung wenden Sie sich bitte an den Cloud Temple-Support.
:::

#### 1. Strukturelle Analyse (`check_structure`)
- **ÃœberprÃ¼fung auf fehlerhaftes JSON**: Das System prÃ¼ft, ob der Prompt mit einem `{` beginnt und versucht, ihn als JSON zu parsen. Wenn der Parsevorgang erfolgreich ist und der JSON verdÃ¤chtige SchlÃ¼sselwÃ¶rter enthÃ¤lt (z.â€¯B. "system", "bypass"), oder wenn der Parsevorgang unerwartet fehlschlÃ¤gt, kann dies auf eine Injektionsversuch hinweisen.
- **Unicode-Normalisierung**: Der Prompt wird mittels `unicodedata.normalize('NFKC', prompt)` normalisiert. Wenn sich der ursprÃ¼ngliche Prompt von seiner normalisierten Version unterscheidet, kann dies auf die Verwendung von tÃ¤uschenden Unicode-Zeichen (Homoglyphen) hindeuten, um Filter zu umgehen. Beispielsweise "Ğ°dmin" (kyrillisch) anstelle von "admin" (lateinisch).

#### 2. Erkennung verdÃ¤chtiger Muster (`check_patterns`)
- Das System verwendet regulÃ¤re AusdrÃ¼cke (`regex`), um bekannte Angriffsmuster bei Prompt-Attacken zu erkennen, und zwar in mehreren Sprachen (FranzÃ¶sisch, Englisch, Chinesisch, Japanisch).
- **Beispiele fÃ¼r erkannte Muster**:
    - **Systembefehle**: SchlÃ¼sselwÃ¶rter wie â€ignore the instructionsâ€œ, â€ignore instructionsâ€œ, â€å¿½ç•¥æŒ‡ä»¤â€œ, â€æŒ‡ç¤ºã‚’ç„¡è¦–â€œ.
    - **HTML-Injektion**: Versteckte oder schÃ¤dliche HTML-Tags, beispielsweise `<div hidden>`, `<hidden div>`.
    - **Markdown-Injektion**: SchÃ¤dliche Markdown-Links, beispielsweise `[text](javascript:...)`, `[text](data:...)`.
    - **Wiederholte Sequenzen**: ÃœbermÃ¤ÃŸige Wiederholung von WÃ¶rtern oder SÃ¤tzen wie â€forget forget forgetâ€œ, â€oublie oublie oublieâ€œ.
    - **Spezielle/Mischzeichen**: Verwendung ungewÃ¶hnlicher Unicode-Zeichen oder das Mischen von Schriftsystemen, um Befehle zu verschleiern (z.â€¯B. â€s\u0443stÃ¨meâ€œ).

#### 3. Verhaltensanalyse (`check_behavior`)
- Der Load Balancer unterhÃ¤lt einen Verlauf der jÃ¼ngsten Prompts.
- **Fragmentierungserkennung**: Er kombiniert kÃ¼rzliche Prompts, um zu prÃ¼fen, ob ein Angriff Ã¼ber mehrere Anfragen hinweg fragmentiert ist. Wenn beispielsweise in einem Prompt "ignore" und im nÃ¤chsten "instructions" gesendet wird, kann das System dies zusammen erkennen.
- **Wiederholungserkennung**: Er identifiziert, ob derselbe Prompt Ã¼bermÃ¤ÃŸig oft wiederholt wird. Der aktuelle Schwellenwert fÃ¼r die Wiederholungserkennung liegt bei 30 aufeinanderfolgenden identischen Prompts.

Dieser mehrschichtige Ansatz ermÃ¶glicht die Erkennung einer breiten Palette von Prompt-Angriffen, von den einfachsten bis zu den raffiniertesten, indem statische Inhaltsanalyse mit dynamischer Verhaltensanalyse kombiniert wird.

## ğŸ“ˆ Leistung und Skalierbarkeit

### Echtzeit-Ãœberwachung

Zugriff Ã¼ber **Cloud Temple Konsole**:
- Nutzungsmetriken pro Modell
- Latenz- und Durchsatzdiagramme
- Alarme bei Leistungsgrenzen
- Anfragehistorie

## ğŸŒ Integration und Ã–kosystem

### OpenAI-KompatibilitÃ¤t

Der LLMaaS-Service ist **kompatibel** mit der OpenAI-API:

```python
# Transparente Migration
from openai import OpenAI

# Vorher (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# After (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="your-cloud-temple-token",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Identischer Code!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Cloud-Temple-Modell
    messages=[{"role": "user", "content": "Hallo"}]
)
```

### UnterstÃ¼tztes Ã–kosystem

#### **KI-Frameworks**
- âœ… **LangChain** : Native Integration
- âœ… **Haystack** : Dokumenten-Pipelines
- âœ… **Semantic Kernel** : Microsoft-Orchestrierung
- âœ… **AutoGen** : Konversationelle Agenten

#### **Entwicklungstools**
- âœ… **Jupyter** : Interaktive Notebooks
- âœ… **Streamlit** : Schnelle Webanwendungen
- âœ… **Gradio** : BenutzeroberflÃ¤chen fÃ¼r KI
- âœ… **FastAPI** : Backend-APIs

#### **No-Code-Plattformen**
- âœ… **Zapier** : Automatisierungen
- âœ… **Make** : Visuelle Integrationen
- âœ… **Bubble** : Webanwendungen

## ğŸ”„ Lebenszyklus von Modellen

### Modell-Updates

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="LLMaaS Modell-Lebenszyklus" />

### Versionierungsrichtlinie

- **Stabile Modelle**: Feste Versionen, verfÃ¼gbar fÃ¼r 6 Monate
- **Experimentelle Modelle**: Beta-Versionen fÃ¼r Early Adopters
- **Deprecation**: 3 Monate VorankÃ¼ndigung vor Entfernung
- **Migration**: Professionelle Dienstleistungen verfÃ¼gbar, um Ihre ÃœbergÃ¤nge zu unterstÃ¼tzen

### Vorhersehbarer Lebenszyklus

Die folgende Tabelle zeigt den vorhersehbaren Lebenszyklus unserer Modelle. Das Ã–kosystem der generativen KI entwickelt sich sehr schnell, was zu scheinbar kurzen Lebenszyklen fÃ¼hren kann. Unser Ziel ist es, Ihnen Zugang zu den leistungsstÃ¤rksten Modellen derzeit zu gewÃ¤hren.

Dennoch verpflichten wir uns, die Modelle, die am hÃ¤ufigsten von unseren Kunden genutzt werden, Ã¼ber einen lÃ¤ngeren Zeitraum zu erhalten. FÃ¼r kritische AnwendungsfÃ¤lle, die eine langfristige StabilitÃ¤t erfordern, sind **erweiterte Support-Phasen** mÃ¶glich. ZÃ¶gern Sie nicht, den **Support zu kontaktieren**, um Ihre spezifischen Anforderungen zu besprechen.

Dieser Plan wird als Richtwert bereitgestellt und wird **zu Beginn jedes Quartals Ã¼berprÃ¼ft**.

- **DMP (Datum der MarkteinfÃ¼hrung)**: Das Datum, ab dem das Modell in der Produktion verfÃ¼gbar ist.
- **DSP (Datum des Support-Endes)**: Das vorhersehbare Datum, ab dem das Modell nicht mehr gewartet wird. Ein Vorlaufzeitraum von 3 Monaten wird eingehalten, bevor das Modell endgÃ¼ltig entfernt wird.

| Modell                  | Herausgeber               | Phase      | DMP        | DSP        |
| :---------------------- | :------------------------ | :--------- | :--------- | :--------- |
| deepcoder:14b           | Agentica x Together AI    | Produktion | 13/06/2025 | 30/12/2025 |
| cogito:32b              | Deep Cogito               | Produktion | 13/06/2025 | 30/06/2026 |
| cogito:3b               | Deep Cogito               | Produktion | 13/06/2025 | 30/12/2025 |
| cogito:8b               | Deep Cogito               | Produktion | 13/06/2025 | 30/12/2025 |
| deepseek-r1:14b         | DeepSeek AI               | Produktion | 13/06/2025 | 31/12/2025 |
| deepseek-r1:32b         | DeepSeek AI               | Produktion | 13/06/2025 | 31/12/2025 |
| deepseek-ocr            | Qwen Team                 | Produktion | 22/11/2025 | 31/12/2026 |
| gemma3:12b              | Google                    | Produktion | 13/06/2025 | 31/12/2026 |
| gemma3:1b               | Google                    | Produktion | 13/06/2025 | 31/12/2025 |
| gemma3:27b              | Google                    | Produktion | 13/06/2025 | 30/03/2026 |
| gemma3:4b               | Google                    | Produktion | 13/06/2025 | 31/12/2025 |
| gpt-oss:120b            | Qwen Team                 | Produktion | 11/11/2025 | 30/06/2026 |
| gpt-oss:20b             | Qwen Team                 | Produktion | 08/08/2025 | 30/03/2026 |
| embeddinggemma:300m     | Google                    | Produktion | 10/09/2025 | 31/12/2026 |
| granite-embedding:278m  | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3-embedding:0.6b    | Qwen Team                 | Produktion | 18/10/2025 | 31/12/2026 |
| granite3-guardian:2b    | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3-guardian:8b    | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3.2-vision:2b    | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3.3:2b           | IBM                       | Produktion | 13/06/2025 | 31/12/2025 |
| granite3.3:8b           | IBM                       | Produktion | 13/06/2025 | 31/12/2025 |
| granite4-small-h:32b    | IBM                       | Produktion | 03/10/2025 | 30/09/2026 |
| granite4-tiny-h:7b      | IBM                       | Produktion | 03/10/2025 | 30/09/2026 |
| llama3.3:70b            | Meta                      | Produktion | 13/06/2025 | 31/12/2026 |
| magistral:24b           | Mistral AI                | Produktion | 13/06/2025 | 30/03/2026 |
| mistral-small3.1:24b    | Mistral AI                | Produktion | 13/06/2025 | 31/12/2025 |
| mistral-small3.2:24b    | Mistral AI                | Produktion | 23/06/2025 | 30/06/2026 |
| devstral:24b            | Mistral AI & All Hands AI | Produktion | 13/06/2025 | 30/03/2026 |
| qwen2.5vl:32b           | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen2.5vl:3b            | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen2.5vl:72b           | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen2.5vl:7b            | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen3:0.6b              | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen3:1.7b              | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen3:14b               | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen3:30b-a3b           | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen3-2507:30b-a3b      | Qwen Team                 | Produktion | 30/08/2025 | 30/03/2026 |
| qwen3-2507-think:4b     | Qwen Team                 | Produktion | 31/08/2025 | 30/03/2026 |
| qwen3-2507:4b           | Qwen Team                 | Produktion | 31/08/2025 | 30/03/2026 |
| qwen3:4b                | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen3:8b                | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen3-coder:30b         | Qwen Team                 | Produktion | 02/08/2025 | 30/03/2026 |
| qwen3-2507:235b         | Qwen Team                 | Produktion | 02/08/2025 | 30/03/2026 |
| qwen3:32b               | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwq:32b                 | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen3-next:80b          | Qwen Team                 | Produktion | 04/11/2025 | 30/03/2026 |

### Veraltete Modelle

Die Welt der LLMs entwickelt sich sehr schnell. Um unseren Kunden Zugang zu den leistungsstÃ¤rksten Technologien zu gewÃ¤hrleisten, werden Modelle, die nicht mehr den aktuellen Standards entsprechen oder nicht mehr genutzt werden, regelmÃ¤ÃŸig als veraltet markiert. Die unten aufgefÃ¼hrten Modelle sind auf der Ã¶ffentlichen Plattform nicht mehr verfÃ¼gbar. Sie kÃ¶nnen jedoch auf Anfrage fÃ¼r spezifische Projekte reaktiviert werden.

| Modell                   | Anbieter              | Status   | Datum der Einstellung |
| :----------------------- | :-------------------- | :------- | :-------------------- |
| cogito:14b               | Deep Cogito           | Veraltet | 17/10/2025            |
| deepseek-r1:671b         | DeepSeek AI           | Veraltet | 17/10/2025            |
| deepseek-r1:70b          | DeepSeek AI           | Veraltet | 17/10/2025            |
| foundation-sec:8b        | Foundation AI â€” Cisco | Veraltet | 17/10/2025            |
| granite3.1-moe:3b        | IBM                   | Veraltet | 17/10/2025            |
| llama3.1:8b              | Meta                  | Veraltet | 17/10/2025            |
| phi4-reasoning:14b       | Microsoft             | Veraltet | 17/10/2025            |
| lucie-instruct:7b        | OpenLLM-France        | Veraltet | 17/10/2025            |
| qwen2.5:0.5b             | Qwen Team             | Veraltet | 17/10/2025            |
| qwen2.5:1.5b             | Qwen Team             | Veraltet | 17/10/2025            |
| qwen2.5:14b              | Qwen Team             | Veraltet | 17/10/2025            |
| qwen2.5:32b              | Qwen Team             | Veraltet | 17/10/2025            |
| qwen2.5:3b               | Qwen Team             | Veraltet | 17/10/2025            |
| qwen3:235b               |                       | Veraltet | 22/11/2025            |
| qwen3-2507-think:30b-a3b |                       | Veraltet | 14/11/2025            |
| gemma3:12b               |                       | Veraltet | 21/11/2025            |

## ğŸ’¡ Best Practices

Um das Beste aus der LLMaaS-API herauszuholen, ist es wichtig, Strategien zur Optimierung von Kosten, Leistung und Sicherheit zu Ã¼bernehmen.

### Kostenoptimierung

Die Kostenkontrolle basiert auf einer intelligenten Nutzung von Tokens und Modellen.

1.  **Modellauswahl**: Verwenden Sie kein Ã¼bermÃ¤chtiges Modell fÃ¼r einfache Aufgaben. Ein grÃ¶ÃŸeres Modell ist leistungsfÃ¤higer, aber auch langsamer und verbraucht deutlich mehr Energie, was sich direkt auf die Kosten auswirkt. Passen Sie die ModellgrÃ¶ÃŸe an die KomplexitÃ¤t Ihrer Anforderung an, um ein optimales Gleichgewicht zu erreichen.

    Beispiel: FÃ¼r die Verarbeitung einer Million Tokens:
    - **`Gemma 3 1B`** verbraucht **0,15 kWh**.
    - **`Llama 3.3 70B`** verbraucht **11,75 kWh**, also **78-mal mehr**.

    ```python
    # FÃ¼r eine Sentiment-Analyse reicht ein kompaktes Modell aus und ist wirtschaftlich.
    if task == "sentiment_analysis":
        model = "granite3.3:2b"
    # FÃ¼r eine komplexe juristische Analyse ist ein grÃ¶ÃŸeres Modell erforderlich.
    elif task == "legal_analysis":
        model = "deepseek-r1:70b"
    ```

2.  **Context-Management**: Der GesprÃ¤chsverlauf (`messages`) wird bei jedem Aufruf zurÃ¼ckgesendet und verbraucht Eingabetokens. Bei langen GesprÃ¤chen sollten Strategien wie Zusammenfassung oder Fenstern (Windowing) in Betracht gezogen werden, um nur relevante Informationen zu speichern.
    ```python
    # Bei langen GesprÃ¤chen kann man die ersten Austausche zusammenfassen.
    messages = [
        {"role": "system", "content": "Sie sind ein KI-Assistent."},
        {"role": "user", "content": "Zusammenfassung der ersten 10 Austausche..."},
        {"role": "assistant", "content": "Ok, ich habe den Kontext."},
        {"role": "user", "content": "Hier ist meine neue Frage."}
    ]
    ```

3.  **BeschrÃ¤nkung der Ausgabetokens**: Verwenden Sie stets den Parameter `max_tokens`, um zu lange und kostspielige Antworten zu vermeiden. Legen Sie eine angemessene Obergrenze basierend auf Ihren Erwartungen fest.
    ```python
    # Maximal 100 WÃ¶rter im Zusammenfassung erfordern.
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Fassen Sie dieses Dokument zusammen..."}],
        max_tokens=150,  # Puffer fÃ¼r ca. 100 WÃ¶rter
    )
    ```

### Performance

Die ReaktionsfÃ¤higkeit Ihrer Anwendung hÃ¤ngt davon ab, wie Sie API-Aufrufe verwalten.

1.  **Asynchrone Anfragen**: Um mehrere Anfragen zu verarbeiten, ohne auf das Ende jeder einzelnen zu warten, verwenden Sie asynchrone Aufrufe. Dies ist besonders nÃ¼tzlich fÃ¼r Backend-Anwendungen, die einen groÃŸen Volumen an gleichzeitigen Anfragen verarbeiten mÃ¼ssen.
    ```python
    import asyncio
    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key="...", base_url="...")

    async def process_prompt(prompt: str):
        # Verarbeitet eine einzelne Anfrage asynchron
        response = await client.chat.completions.create(model="granite3.3:8b", messages=[{"role": "user", "content": prompt}])
        return response.choices[0].message.content

    async def batch_requests(prompts: list):
        # Startet mehrere Aufgaben parallel und wartet auf deren Abschluss
        tasks = [process_prompt(p) for p in prompts]
        return await asyncio.gather(*tasks)
    ```

2.  **Streaming fÃ¼r die Benutzererfahrung (UX)**: FÃ¼r BenutzeroberflÃ¤chen (Chatbots, Assistenten) ist Streaming unerlÃ¤sslich. Es ermÃ¶glicht die schrittweise Anzeige der Modellantwort Buchstabe fÃ¼r Buchstabe, was den Eindruck einer sofortigen ReaktionsfÃ¤higkeit vermittelt, anstatt auf die vollstÃ¤ndige Antwort warten zu mÃ¼ssen.
    ```python
    # Zeigt die Antwort in Echtzeit in einer BenutzeroberflÃ¤che an
    response_stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "ErzÃ¤hl mir eine Geschichte."}],
        stream=True
    )
    for chunk in response_stream:
        if chunk.choices[0].delta.content:
            # Zeigt den Textabschnitt in der UI an
            print(chunk.choices[0].delta.content, end="", flush=True)
    ```

### Sicherheit

Die Sicherheit Ihrer Anwendung ist von entscheidender Bedeutung, besonders wenn Sie Benutzereingaben verarbeiten.

1.  **Validierung und Bereinigung von Eingaben (Sanitization)**: Vertrauen Sie niemals auf Benutzereingaben. Bereinigen Sie diese, bevor Sie sie an die API senden, um potenziell schÃ¤dlichen Code oder Anweisungen zur "Prompt-Injektion" zu entfernen. Begrenzen Sie auÃŸerdem die LÃ¤nge, um Missbrauch zu verhindern.
    ```python
    def sanitize_input(user_input: str) -> str:
        # Einfaches Beispiel: Entfernen von Code-Formatierungen und Begrenzung der LÃ¤nge.
        # FÃ¼r erweiterte Sanitization kÃ¶nnen robustere Bibliotheken verwendet werden.
        cleaned = user_input.replace("`", "").replace("'", "").replace("\"", "")
        return cleaned[:2000]  # Begrenzt die LÃ¤nge auf 2000 Zeichen
    ```

2.  **Robuste Fehlerbehandlung**: Umgeben Sie alle API-Aufrufe immer mit `try...except`-BlÃ¶cken, um Netzwerkfehler, API-Fehler (z.â€¯B. 429 Rate Limit, 500 Internal Server Error) zu behandeln und eine degradierte, aber funktionale Benutzererfahrung zu gewÃ¤hrleisten.
    ```python
    from openai import APIError, APITimeoutError

    try:
        response = client.chat.completions.create(...)
    except APITimeoutError:
        # Behandlung des Falls, dass die Anfrage zu lange dauert
        return "Der Dienst benÃ¶tigt lÃ¤nger als erwartet, bitte versuchen Sie es erneut."
    except APIError as e:
        # Behandlung spezifischer API-Fehler
        logger.error(f"API-Fehler LLMaaS: {e.status_code} - {e.message}")
        return "Entschuldigung, es ist ein Fehler mit dem KI-Service aufgetreten."
    except Exception as e:
        # Behandlung aller anderen Fehler (Netzwerk usw.)
        logger.error(f"Ein unerwarteter Fehler ist aufgetreten: {e}")
        return "Entschuldigung, ein unerwarteter Fehler ist aufgetreten."
