---
title: Konzepte
sidebar_position: 3
---

# Concepts and Architecture of LLMaaS

## Overview

The **LLMaaS** (Large Language Models as a Service) service by Cloud Temple provides secure and sovereign access to the most advanced artificial intelligence models, with the **SecNumCloud certification** from ANSSI.

## üèóÔ∏è Technische Architektur

### Cloud Infrastructure Temple

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Technical Architecture of LLMaaS Cloud Temple" />

### Hauptkomponenten

#### 1. **API Gateway LLMaaS**
- **OpenAI-kompatibel** : Nahtlose Integration in bestehende √ñkosysteme
- **Rate Limiting** : Verwaltung von Quoten je Abonnementstier
- **Load Balancing** : Intelligente Verteilung auf 12 GPU-Server
- **Monitoring** : Echtzeit-Metriken und Alarmierung

#### 2. **Authentication Service**
- **Secure API Tokens**: Automatic rotation
- **Access Control**: Granular permissions per model
- **Audit Trails**: Full access traceability

## ü§ñ Models and Tokens

### Model Catalog

*Complete catalog: [List of models](./models)*

### Token Management

#### **Token Types**
- **Input Tokens**: Your prompt and context
- **Output Tokens**: Response generated by the model
- **System Tokens**: Metadata and instructions

#### **Cost Calculation**
```
Total cost = (Input tokens √ó 0.9‚Ç¨/M) + (Output tokens √ó 4‚Ç¨/M) + (Reasoning output tokens √ó 21‚Ç¨/M)
```

#### **Optimierung**
- **Contextfenster**: Wiederverwenden Sie Gespr√§che, um Kosten zu sparen
- **Passende Modelle**: W√§hlen Sie die Gr√∂√üe entsprechend der Komplexit√§t
- **Maximale Tokens**: Begrenzen Sie die L√§nge der Antworten

### Tokenisierung

```python
# Beispiel zur Sch√§tzung von Tokens
def estimate_tokens(text: str) -> int:
    """Approximative Sch√§tzung: 1 Token ‚âà 4 Zeichen"""
    return len(text) // 4

prompt = "Erkl√§ren Sie die Photosynthese"
response_max = 200  # gew√ºnschte maximale Anzahl an Tokens

estimated_input = estimate_tokens(prompt)  # ~6 Tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Gesch√§tzter Kostenbetrag: {total_cost:.6f}‚Ç¨")
```

## üîí Security and Compliance

### SecNumCloud Certification

The LLMaaS service is hosted on a technical infrastructure that holds the **SecNumCloud 3.2 certification** from ANSSI, ensuring:

#### **Data Protection**
- **End-to-end Encryption**: TLS 1.3 for all communications
- **Secure Storage**: Data encrypted at rest (AES-256)
- **Isolation**: Dedicated environments per tenant

#### **Digital Sovereignty**
- **Hosting in France**: Cloud Temple data centers with certified compliance  
- **French Law**: Native GDPR compliance  
- **No Exposure**: No data transfers to foreign clouds

#### **Audit and Traceability**
- **Complete logs**: All interactions tracked
- **Retention**: Stored according to legal policies
- **Compliance**: Audit reports available

### Security Controls

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Security Controls LLMaaS" />

### Prompt Security

Prompt analysis is a **native and integrated** security feature of the LLMaaS platform. Enabled by default, it aims to detect and prevent attempts at "jailbreaking" or injecting malicious prompts before they even reach the model. This protection is based on a multi-layered approach.

:::tip Contact Support for Disabling
It is possible to disable this security analysis for very specific use cases, although this is not recommended. For any questions regarding this or to request deactivation, please contact Cloud Temple support.
:::

#### 1. Strukturelle Analyse (`check_structure`)
- **√úberpr√ºfung auf fehlerhaftes JSON**: Das System pr√ºft, ob der Prompt mit einem `{` beginnt und versucht, ihn als JSON zu parsen. Wenn der Parsevorgang erfolgreich ist und der JSON verd√§chtige Schl√ºsselw√∂rter enth√§lt (z.‚ÄØB. "system", "bypass"), oder wenn der Parsevorgang unerwartet fehlschl√§gt, kann dies auf eine Injektionsversuch hinweisen.
- **Unicode-Normalisierung**: Der Prompt wird mittels `unicodedata.normalize('NFKC', prompt)` normalisiert. Wenn sich der urspr√ºngliche Prompt von seiner normalisierten Version unterscheidet, kann dies auf die Verwendung von t√§uschenden Unicode-Zeichen (Homoglyphen) hindeuten, um Filter zu umgehen. Beispielsweise "–∞dmin" (kyrillisch) anstelle von "admin" (lateinisch).

#### 2. Detection von verd√§chtigen Mustern (`check_patterns`)
- Das System verwendet regul√§re Ausdr√ºcke (`regex`), um bekannte Angriffsmuster f√ºr Prompt-Injektionen zu erkennen, und zwar in mehreren Sprachen (Franz√∂sisch, Englisch, Chinesisch, Japanisch).
- **Beispiele f√ºr erkannte Muster**:
    - **Systembefehle**: Schl√ºsselw√∂rter wie ‚Äûignore the instructions‚Äú, ‚Äûignore instructions‚Äú, ‚ÄûÂøΩÁï•Êåá‰ª§‚Äú, ‚ÄûÊåáÁ§∫„ÇíÁÑ°Ë¶ñ‚Äú.
    - **HTML-Injektion**: Versteckte oder sch√§dliche HTML-Tags, beispielsweise `<div hidden>`, `<hidden div>`.
    - **Markdown-Injektion**: Sch√§dliche Markdown-Links, beispielsweise `[text](javascript:...)`, `[text](data:...)`.
    - **Wiederholte Sequenzen**: √úberm√§√üige Wiederholung von W√∂rtern oder S√§tzen wie ‚Äûforget forget forget‚Äú, ‚Äûoublie oublie oublie‚Äú.
    - **Spezielle/Sonstige Zeichen**: Verwendung ungew√∂hnlicher Unicode-Zeichen oder Mischung verschiedener Schriftsysteme, um Befehle zu verschleiern (z.‚ÄØB. ‚Äûs\u0443st√®me‚Äú).

#### 3. Behavioral Analysis (`check_behavior`)
- The load balancer maintains a history of recent prompts.
- **Fragmentation Detection**: It combines recent prompts to check whether an attack is fragmented across multiple requests. For example, if "ignore" is sent in one prompt and "instructions" in the next, the system can detect them together.
- **Repetition Detection**: It identifies if the same prompt is repeated excessively. The current threshold for repetition detection is 30 consecutive identical prompts.

This multi-layered approach enables the detection of a wide range of prompt attacks, from the simplest to the most sophisticated, by combining static content analysis with dynamic behavioral analysis.

## üìà Leistungsf√§higkeit und Skalierbarkeit

### Real-time Monitoring

Access via **Cloud Temple Console**:
- Usage metrics by model
- Latency and throughput graphs
- Alerts on performance thresholds
- Request history

## üåê Integration und √ñkosystem

### OpenAI-Kompatibilit√§t

Der LLMaaS-Service ist **kompatibel** mit der OpenAI-API:

```python
# Transparente Migration
from openai import OpenAI

# Vorher (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# After (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="your-cloud-temple-token",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Identischer Code!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Cloud-Temple-Modell
    messages=[{"role": "user", "content": "Hallo"}]
)
```

### Supported Ecosystem

#### **AI Frameworks**
- ‚úÖ **LangChain** : Native integration
- ‚úÖ **Haystack** : Document pipelines
- ‚úÖ **Semantic Kernel** : Microsoft orchestration
- ‚úÖ **AutoGen** : Conversational agents

#### **Entwicklungstools**
- ‚úÖ **Jupyter** : Interaktive Notebooks
- ‚úÖ **Streamlit** : Schnelle Webanwendungen
- ‚úÖ **Gradio** : Benutzeroberfl√§chen f√ºr KI
- ‚úÖ **FastAPI** : Backend-APIs

#### **No-Code-Plattformen**
- ‚úÖ **Zapier** : Automatisierungen
- ‚úÖ **Make** : Visuelle Integrationen
- ‚úÖ **Bubble** : Webanwendungen

## üîÑ Lebenszyklus von Modellen

### Model Updates

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="LLMaaS Model Lifecycle" />

### Versioning Policy

- **Stable models**: Fixed versions available for 6 months  
- **Experimental models**: Beta versions for early adopters  
- **Deprecation**: 3-month notice before removal  
- **Migration**: Professional services available to support your transitions

### Projektierter Lebenszyklus

Die folgende Tabelle zeigt den prognostizierten Lebenszyklus unserer Modelle. Der √ñkosystem der generativen KI entwickelt sich sehr schnell, was zu scheinbar kurzen Lebenszyklen f√ºhren kann. Unser Ziel ist es, Ihnen Zugang zu den leistungsst√§rksten Modellen derzeit zu gew√§hren.

Dennoch verpflichten wir uns, die Modelle, die am h√§ufigsten von unseren Kunden genutzt werden, √ºber einen l√§ngeren Zeitraum zu erhalten. F√ºr kritische Anwendungsf√§lle, die eine langfristige Stabilit√§t erfordern, sind **erweiterte Support-Phasen** m√∂glich. Z√∂gern Sie nicht, den **Support zu kontaktieren**, um Ihre spezifischen Anforderungen zu besprechen.

Dieser Plan dient lediglich als Richtwert und wird **zu Beginn jedes Quartals √ºberarbeitet**.

- **DMP (Datum der Markteinf√ºhrung)**: Das Datum, ab dem das Modell in der Produktion verf√ºgbar ist.
- **DSP (Datum des Support-Endes)**: Das prognostizierte Datum, ab dem das Modell nicht mehr gewartet wird. Ein Vorlaufzeitraum von 3 Monaten wird eingehalten, bevor das Modell endg√ºltig entfernt wird.

| Modell                  | Herausgeber               | Phase      | DMP        | DSP        |
| :---------------------- | :------------------------ | :--------- | :--------- | :--------- |
| deepcoder:14b           | Agentica x Together AI    | Produktion | 13/06/2025 | 30/06/2026 |
| cogito:32b              | Deep Cogito               | Produktion | 13/06/2025 | 30/06/2026 |
| cogito:3b               | Deep Cogito               | Produktion | 13/06/2025 | 30/06/2026 |
| cogito:8b               | Deep Cogito               | Produktion | 13/06/2025 | 30/06/2026 |
| deepseek-r1:14b         | DeepSeek AI               | Produktion | 13/06/2025 | 31/12/2025 |
| deepseek-r1:32b         | DeepSeek AI               | Produktion | 13/06/2025 | 31/12/2025 |
| gemma3:12b              | Google                    | Produktion | 13/06/2025 | 31/12/2026 |
| gemma3:1b               | Google                    | Produktion | 13/06/2025 | 31/12/2026 |
| gemma3:27b              | Google                    | Produktion | 13/06/2025 | 31/12/2026 |
| gemma3:4b               | Google                    | Produktion | 13/06/2025 | 31/12/2026 |
| granite-embedding:278m  | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3-guardian:2b    | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3-guardian:8b    | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3.2-vision:2b    | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3.3:2b           | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3.3:8b           | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| llama3.3:70b            | Meta                      | Produktion | 13/06/2025 | 31/12/2026 |
| magistral:24b           | Mistral AI                | Produktion | 13/06/2025 | 31/12/2026 |
| mistral-small3.1:24b    | Mistral AI                | Produktion | 13/06/2025 | 31/12/2026 |
| mistral-small3.2:24b    | Mistral AI                | Produktion | 23/06/2025 | 30/03/2026 |
| devstral:24b            | Mistral AI & All Hands AI | Produktion | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:32b           | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:3b            | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:72b           | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:7b            | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:0.6b              | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:1.7b              | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:14b               | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:30b-a3b           | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:4b                | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:8b                | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:32b               | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:235b              | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwq:32b                 | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |

### Deprecated Models

The world of LLMs is evolving rapidly. To ensure our customers have access to the most advanced technologies, we regularly deprecate models that no longer meet current standards or are no longer in use. The models listed below are no longer available on the public platform. However, they can be reactivated for specific projects upon request.

| Model              | Provider              | Status   | Deprecation Date     |
| :----------------- | :-------------------- | :------- | :------------------- |
| cogito:14b         | Deep Cogito           | Deprecated | 17/10/2025           |
| deepseek-r1:671b   | DeepSeek AI           | Deprecated | 17/10/2025           |
| deepseek-r1:70b    | DeepSeek AI           | Deprecated | 17/10/2025           |
| foundation-sec:8b  | Foundation AI ‚Äî Cisco | Deprecated | 17/10/2025           |
| granite3.1-moe:3b  | IBM                   | Deprecated | 17/10/2025           |
| llama3.1:8b        | Meta                  | Deprecated | 17/10/2025           |
| phi4-reasoning:14b | Microsoft             | Deprecated | 17/10/2025           |
| lucie-instruct:7b  | OpenLLM-France        | Deprecated | 17/10/2025           |
| qwen2.5:0.5b       | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:1.5b       | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:14b        | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:32b        | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:3b         | Qwen Team             | Deprecated | 17/10/2025           |

## üí° Best Practices

To get the most out of the LLMaaS API, it is essential to adopt strategies for optimizing costs, performance, and security.

### Kostenoptimierung

Die Kostenkontrolle basiert auf einer intelligenten Nutzung von Tokens und Modellen.

1.  **Modellauswahl**: Verwenden Sie kein √ºberm√§chtiges Modell f√ºr einfache Aufgaben. Ein gr√∂√üeres Modell ist leistungsf√§higer, aber auch langsamer und verbraucht deutlich mehr Energie, was sich direkt auf die Kosten auswirkt. Passen Sie die Modellgr√∂√üe an die Komplexit√§t Ihrer Anforderung an, um ein optimales Gleichgewicht zu erreichen.

    Beispiel: F√ºr die Verarbeitung einer Million Tokens:
    - **`Gemma 3 1B`** verbraucht **0,15 kWh**.
    - **`Llama 3.3 70B`** verbraucht **11,75 kWh**, also **78-mal mehr**.

    ```python
    # F√ºr eine Sentiment-Analyse reicht ein kompaktes Modell aus und ist wirtschaftlich.
    if task == "sentiment_analysis":
        model = "granite3.3:2b"
    # F√ºr eine komplexe juristische Analyse ist ein gr√∂√üeres Modell erforderlich.
    elif task == "legal_analysis":
        model = "deepseek-r1:70b"
    ```

2.  **Context-Management**: Der Gespr√§chsverlauf (`messages`) wird bei jedem Aufruf zur√ºckgesendet und verbraucht Eingabetokens. Bei langen Gespr√§chen sollten Strategien wie Zusammenfassung oder Fensterung in Betracht gezogen werden, um nur relevante Informationen zu speichern.
    ```python
    # Bei langen Gespr√§chen kann man die ersten Austausche zusammenfassen.
    messages = [
        {"role": "system", "content": "Sie sind ein KI-Assistent."},
        {"role": "user", "content": "Zusammenfassung der ersten 10 Austausche..."},
        {"role": "assistant", "content": "Ok, ich habe den Kontext."},
        {"role": "user", "content": "Hier ist meine neue Frage."}
    ]
    ```

3.  **Beschr√§nkung der Ausgabetokens**: Verwenden Sie stets den Parameter `max_tokens`, um zu lange und kostspielige Antworten zu vermeiden. Legen Sie eine angemessene Obergrenze basierend auf Ihren Erwartungen fest.
    ```python
    # Maximal 100 W√∂rter im Zusammenfassung erfordern.
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Fassen Sie dieses Dokument zusammen..."}],
        max_tokens=150,  # Puffer f√ºr etwa 100 W√∂rter
    )
    ```

### Performance

Die Reaktionsf√§higkeit Ihrer Anwendung h√§ngt davon ab, wie Sie API-Aufrufe verwalten.

1.  **Asynchrone Anfragen**: Um mehrere Anfragen zu verarbeiten, ohne auf das Ende jeder einzelnen zu warten, verwenden Sie asynchrone Aufrufe. Dies ist besonders n√ºtzlich f√ºr Backend-Anwendungen, die einen gro√üen Volumen an gleichzeitigen Anfragen verarbeiten m√ºssen.
    ```python
    import asyncio
    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key="...", base_url="...")

    async def process_prompt(prompt: str):
        # Verarbeitet eine einzelne Anfrage asynchron
        response = await client.chat.completions.create(model="granite3.3:8b", messages=[{"role": "user", "content": prompt}])
        return response.choices[0].message.content

    async def batch_requests(prompts: list):
        # Startet mehrere Aufgaben parallel und wartet auf deren Abschluss
        tasks = [process_prompt(p) for p in prompts]
        return await asyncio.gather(*tasks)
    ```

2.  **Streaming f√ºr die Benutzererfahrung (UX)**: F√ºr Benutzeroberfl√§chen (Chatbots, Assistenten) ist Streaming unerl√§sslich. Es erm√∂glicht die Echtzeit-Anzeige der Modellantwort Buchstabe f√ºr Buchstabe, was den Eindruck einer sofortigen Reaktionsf√§higkeit vermittelt, anstatt auf die vollst√§ndige Antwort warten zu m√ºssen.
    ```python
    # Zeigt die Antwort in Echtzeit in einer Benutzeroberfl√§che an
    response_stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Erz√§hl mir eine Geschichte."}],
        stream=True
    )
    for chunk in response_stream:
        if chunk.choices[0].delta.content:
            # Zeigt den Textabschnitt in der UI an
            print(chunk.choices[0].delta.content, end="", flush=True)
    ```

### Sicherheit

Die Sicherheit Ihrer Anwendung ist von entscheidender Bedeutung, besonders wenn Sie Benutzereingaben verarbeiten.

1.  **Validierung und Bereinigung von Eingaben (Sanitization)**: Vertrauen Sie niemals auf Benutzereingaben. Bereinigen Sie diese vor der √úbertragung an die API, um potenziell sch√§dlichen Code oder Anweisungen zur ‚ÄûPrompt-Injektion‚Äú zu entfernen. Begrenzen Sie au√üerdem die L√§nge, um Missbrauch zu verhindern.
    ```python
    def sanitize_input(user_input: str) -> str:
        # Einfaches Beispiel: Entfernen von Code-Formatierungen und Begrenzung der L√§nge.
        # F√ºr erweiterte Sanitization k√∂nnen robustere Bibliotheken verwendet werden.
        cleaned = user_input.replace("`", "").replace("'", "").replace("\"", "")
        return cleaned[:2000]  # Begrenzt die L√§nge auf 2000 Zeichen
    ```

2.  **Robuste Fehlerbehandlung**: Umgeben Sie alle API-Aufrufe immer mit `try...except`-Bl√∂cken, um Netzwerkfehler, API-Fehler (z.‚ÄØB. 429 Rate Limit, 500 Internal Server Error) zu behandeln und eine degradierte, aber funktionale Benutzererfahrung zu gew√§hrleisten.
    ```python
    from openai import APIError, APITimeoutError

    try:
        response = client.chat.completions.create(...)
    except APITimeoutError:
        # Behandlung des Falls, dass die Anfrage zu lange dauert
        return "Der Dienst ben√∂tigt l√§nger als erwartet, bitte versuchen Sie es erneut."
    except APIError as e:
        # Behandlung spezifischer API-Fehler
        logger.error(f"API-Fehler LLMaaS: {e.status_code} - {e.message}")
        return "Entschuldigung, es ist ein Fehler mit dem KI-Service aufgetreten."
    except Exception as e:
        # Behandlung aller anderen Fehler (Netzwerk usw.)
        logger.error(f"Ein unerwarteter Fehler ist aufgetreten: {e}")
        return "Entschuldigung, ein unerwarteter Fehler ist aufgetreten."
    ```
