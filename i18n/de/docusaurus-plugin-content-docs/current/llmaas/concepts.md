---
title: Konzepte
sidebar_position: 3
---

# Konzepte und Architektur LLMaaS

## √úbersicht

Der **LLMaaS**-Dienst (Large Language Models als Service) von Cloud Temple bietet sicheren und souver√§nen Zugang zu den fortschrittlichsten KI-Modellen mit der **SecNumCloud-Zertifizierung** der ANSSI.

## üèóÔ∏è Technische Architektur

### Cloud Temple-Infrastruktur

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Technische Architektur von LLMaaS Cloud Temple" />

### Hauptkomponenten

#### 1. **API Gateway LLMaaS**
- **Kompatibel mit OpenAI** : Transparente Integration in bestehende √ñkosysteme
- **Rate Limiting** : Quotenverwaltung pro Abrechnungstier
- **Load Balancing** : Intelligente Verteilung auf 12 GPU-Maschinen
- **Monitoring** : Echtzeit-Metriken und Alarmierung

#### 2. **Authentifizierungsdienst**
- **Sichere API-Tokens** : Automatische Rotation
- **Zugriffssteuerung** : Granulare Berechtigungen pro Modell
- **Audit-Protokolle** : Vollst√§ndige Nachverfolgbarkeit der Zugriffe

## ü§ñ Modelle und Tokens

### Katalog der Modelle

*Vollst√§ndiger Katalog : [Liste der Modelle](./models)*

### Token-Management

#### **Typen von Tokens**
- **Eingabetokens** : Ihr Prompt und Kontext
- **Ausgabetokens** : Antwort, die vom Modell generiert wird
- **Systemtokens** : Metadaten und Anweisungen

#### **Kostenberechnung**
```
Gesamtkosten = (Tokens Eingang √ó 0,9‚Ç¨/M) + (Tokens Ausgang √ó 4‚Ç¨/M) +  (Tokens Ausgang Berechnung √ó 21‚Ç¨/M)
```

#### **Optimierung**
- **Kontextfenster** : Wiederverwenden Sie Gespr√§che, um zu sparen
- **Passende Modelle** : W√§hlen Sie die Gr√∂√üe entsprechend der Komplexit√§t aus
- **Max. Tokens** : Begrenzen Sie die L√§nge der Antworten

### Tokenisierung

```python

# Beispiel zur Token-Sch√§tzung
def estimate_tokens(text: str) -> int:
    """Approximative Sch√§tzung: 1 Token ‚âà 4 Zeichen"""
    return len(text) // 4

prompt = "Erkl√§ren Sie die Photosynthese"
response_max = 200  # gew√ºnschte maximale Tokens

gesch√§tzte_eingabe = estimate_tokens(prompt)  # ~6 Tokens
gesamtkosten = (gesch√§tzte_eingabe * 0.9 + response_max * 4) / 1_000_000
print(f"Sch√§tzkosten: {gesamtkosten:.6f}‚Ç¨")
```

## üîí Sicherheit und Konformit√§t

### Qualifikation SecNumCloud

Die LLMaaS-Dienstleistung wird auf einer technischen Infrastruktur berechnet, die √ºber die **Qualifikation SecNumCloud 3.2** der ANSSI verf√ºgt, die garantiert:

#### **DatenSchutz**
- **End-to-End-Verschl√ºsselung** : TLS 1.3 f√ºr alle Kommunikationen
- **Sichere Speicherung** : Daten, die im Ruhezustand verschl√ºsselt sind (AES-256)
- **Isolation** : Dedizierte Umgebungen pro Mandant

#### **Digitale Souver√§nit√§t**
- **Hosting Frankreich** : zertifizierte Cloud Temple Datacenter
- **franz√∂sisches Recht** : native RGPD-Konformit√§t
- **Keine Exposition** : Kein Transfer zu ausl√§ndischen Clouds

#### **Audit und Nachverfolgbarkeit**
- **Vollst√§ndige Protokolle** : Alle Interaktionen werden protokolliert
- **Aufbewahrung** : Aufbewahrung gem√§√ü gesetzlichen Richtlinien
- **Compliance** : Auditberichte verf√ºgbar

### Sicherheitskontrollen

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Sicherheitskontrollen LLMaaS" />

### Prompt-Sicherheit

Die Prompt-Analyse ist eine **native und integrierte** Sicherheitsfunktion der LLMaaS-Plattform. Sie ist standardm√§√üig aktiviert und dient dazu, Versuche von "Jailbreak" oder sch√§dlichen Prompt-Injektionen zu erkennen und zu verhindern, noch bevor sie das Modell erreichen. Diese Schutzma√ünahme basiert auf einer mehrschichtigen Herangehensweise.

:::tip Support kontaktieren, um die Deaktivierung zu beantragen
Es ist m√∂glich, diese Sicherheitsanalyse f√ºr sehr spezifische Anwendungsf√§lle deaktivieren, obwohl dies nicht empfohlen wird. Bei Fragen dazu oder um eine Deaktivierung zu beantragen, wenden Sie sich bitte an den Cloud Temple-Support.
:::

#### 1. Strukturelle Analyse (`check_structure`)
- **√úberpr√ºfung auf fehlerhaften JSON**: Das System pr√ºft, ob der Prompt mit einem `{` beginnt und versucht, ihn als JSON zu parsen. Wenn das Parsen erfolgreich ist und der JSON verd√§chtige Schl√ºsselw√∂rter enth√§lt (z. B. "system", "bypass"), oder wenn das Parsen unerwartet fehlschl√§gt, kann dies auf eine Injection-Attacke hinweisen.
- **Unicode-Normalisierung**: Der Prompt wird mit `unicodedata.normalize('NFKC', prompt)` normalisiert. Wenn sich der urspr√ºngliche Prompt von der normalisierten Version unterscheidet, kann dies auf die Verwendung von t√§uschenden Unicode-Zeichen (Homoglyphen) zur Umgehung der Filter hinweisen. Zum Beispiel "–∞dmin" (kyrillisch) anstelle von "admin" (lateinisch).

#### 2. Erkennung von verd√§chtigen Mustern (`check_patterns`)
- Das System verwendet regul√§re Ausdr√ºcke (`regex`), um bekannte Muster von Prompt-Angriffen zu identifizieren, und zwar in mehreren Sprachen (franz√∂sisch, englisch, chinesisch, japanisch).
- **Beispiele f√ºr erkannte Muster**:
    - **Systembefehle** : Schl√ºsselw√∂rter wie "ignoriere die Anweisungen", "ignore instructions", "ÂøΩÁï•Êåá‰ª§", "ÊåáÁ§∫„ÇíÁÑ°Ë¶ñ".
    - **HTML-Injektion** : Versteckte oder sch√§dliche HTML-Tags, z. B. `<div versteckt>`, `<hidden div>`.
    - **Markdown-Injektion** : Sch√§dliche Markdown-Links, z. B. `[texte](javascript:...)`, `[text](data:...)`.
    - **Wiederholte Sequenzen** : √úberm√§√üige Wiederholung von W√∂rtern oder S√§tzen wie "vergiss vergiss vergiss", "forget forget forget".
    - **Spezialzeichen/Mischungen** : Verwendung ungew√∂hnlicher Unicode-Zeichen oder Mischungen von Skripten, um Befehle zu verbergen (z. B. "s\u0443st√®me").

#### 3. Verhaltensanalyse (`check_behavior`)
- Der Load Balancer h√§lt ein Protokoll der k√ºrzlichen Prompts.
- **Erkennung der Fragmentierung**: Er kombiniert k√ºrzliche Prompts, um zu pr√ºfen, ob ein Angriff √ºber mehrere Anfragen fragmentiert ist. Zum Beispiel, wenn "ignore" in einem Prompt gesendet wird und "instructions" im n√§chsten, kann das System sie zusammen erkennen.
- **Erkennung der Wiederholung**: Er identifiziert, ob derselbe Prompt zu h√§ufig wiederholt wird. Der aktuelle Schwellwert f√ºr die Wiederholungserkennung betr√§gt **30 aufeinanderfolgende identische Prompts**.

Diese mehrschichtige Herangehensweise erm√∂glicht die Erkennung einer breiten Palette von Prompt-Angriffen, von den einfachsten bis zu den komplexesten, indem sie die statische Inhaltsanalyse und die dynamische Verhaltensanalyse kombiniert.

## üìà Leistung und Skalierbarkeit

### Echtzeit-Monitoring

Zugriff √ºber **Console Cloud Temple**:
- Nutzungsmetriken pro Modell
- Latenz- und Durchsatzdiagramme
- Benachrichtigungen bei Performance-Schwellenwerten
- Anfragespeicher

## üåê Integration und √ñkosystem

### OpenAI-Kompatibilit√§t

Der Dienst LLMaaS ist **kompatibel** mit der OpenAI-API : 

```python

# Transparente Migration
from openai import OpenAI

# Vor (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# Nach (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="votre-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Identischer Code!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Mod√®le Cloud Temple
    messages=[{"role": "user", "content": "Bonjour"}]
)
```

### Unterst√ºtztes √ñkosystem

#### **KI-Frameworks**
- ‚úÖ **LangChain** : Native Integration
- ‚úÖ **Haystack** : Dokumenten-Pipeline
- ‚úÖ **Semantic Kernel** : Microsoft-Orchestration
- ‚úÖ **AutoGen** : Konversationelle Agenten

#### **Entwicklungs-Tools**
- ‚úÖ **Jupyter** : Interaktive Notebooks
- ‚úÖ **Streamlit** : Schnelle Webanwendungen
- ‚úÖ **Gradio** : KI-Benutzeroberfl√§chen
- ‚úÖ **FastAPI** : Backend-APIs

#### **No-Code-Plattformen**
- ‚úÖ **Zapier** : Automatisierungen
- ‚úÖ **Make** : visuelle Integrationen
- ‚úÖ **Bubble** : Webanwendungen

## üîÑ Lebenszyklus der Modelle

### Modell-Update

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="Lebenszyklus der Modelle LLMaaS" />

### Versionspolitik

- **Stabile Modelle** : Feste Versionen, die 6 Monate verf√ºgbar sind
- **Experimentelle Modelle** : Beta-Versionen f√ºr Early Adopter
- **Abl√∂sung** : 3 Monate Vorank√ºndigung vor der Entfernung
- **Migration** : Professionelle Dienstleistungen verf√ºgbar, um Ihre √úberg√§nge zu sichern

### Voraussichtlicher Lebenszyklusplan

Der folgende Tabelle zeigt den voraussichtlichen Lebenszyklus unserer Modelle. Das √ñkosystem der generativen KI entwickelt sich sehr schnell, was zu Lebenszyklen f√ºhrt, die kurz erscheinen k√∂nnen. Unser Ziel ist es, Ihnen Zugang zu den leistungsst√§rksten Modellen derzeit zu gew√§hren.

Wir verpflichten uns, die Modelle, die am h√§ufigsten von unseren Kunden genutzt werden, √ºber die Zeit zu erhalten. F√ºr Anwendungsf√§lle, die eine langfristige Stabilit√§t erfordern, sind **erweiterte Support-Phasen** m√∂glich. Z√∂gern Sie nicht, den **Support zu kontaktieren**, um Ihre spezifischen Anforderungen zu besprechen.

Dieser Plan ist nur als Richtwert zu verstehen und wird zu Beginn jedes Quartals √ºberarbeitet.

- **DMP (Datum der Produktion)** : Der Tag, an dem das Modell in die Produktion geht.
- **DSP (Datum des Support-Endes)** : Das vorhergesagte Datum, ab dem das Modell nicht mehr gewartet wird. Ein Vorank√ºndigungszeitraum von 3 Monaten wird eingehalten, bevor eine endg√ºltige L√∂schung erfolgt.

| Modell                 | Entwickler                | Phase      | DMP        | DSP        |
| :--------------------- | :------------------------ | :--------- | :--------- | :--------- |
| deepcoder:14b          | Agentica x Together AI    | Produktion | 13/06/2025 | 30/06/2026 |
| cogito:14b             | Deep Cogito               | Produktion | 13/06/2025 | 30/06/2026 |
| cogito:32b             | Deep Cogito               | Produktion | 13/06/2025 | 30/06/2026 |
| cogito:3b              | Deep Cogito               | Produktion | 13/06/2025 | 30/06/2026 |
| cogito:8b              | Deep Cogito               | Produktion | 13/06/2025 | 30/06/2026 |
| deepseek-r1:14b        | DeepSeek AI               | Produktion | 13/06/2025 | 31/12/2025 |
| deepseek-r1:32b        | DeepSeek AI               | Produktion | 13/06/2025 | 31/12/2025 |
| deepseek-r1:671b       | DeepSeek AI               | Produktion | 13/06/2025 | 31/12/2025 |
| deepseek-r1:70b        | DeepSeek AI               | Produktion | 13/06/2025 | 31/12/2025 |
| foundation-sec:8b      | Foundation AI ‚Äî Cisco     | Produktion | 13/06/2025 | 30/09/2025 |
| gemma3:12b             | Google                    | Produktion | 13/06/2025 | 31/12/2026 |
| gemma3:1b              | Google                    | Produktion | 13/06/2025 | 31/12/2026 |
| gemma3:27b             | Google                    | Produktion | 13/06/2025 | 31/12/2026 |
| gemma3:4b              | Google                    | Produktion | 13/06/2025 | 31/12/2026 |
| granite-embedding:278m | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3-guardian:2b   | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3-guardian:8b   | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3.1-moe:3b      | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3.2-vision:2b   | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3.3:2b          | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| granite3.3:8b          | IBM                       | Produktion | 13/06/2025 | 31/12/2026 |
| llama3.1:8b            | Meta                      | Produktion | 13/06/2025 | 31/12/2025 |
| llama3.3:70b           | Meta                      | Produktion | 13/06/2025 | 31/12/2026 |
| phi4-reasoning:14b     | Microsoft                 | Produktion | 13/06/2025 | 31/12/2025 |
| magistral:24b          | Mistral AI                | Produktion | 13/06/2025 | 31/12/2026 |
| mistral-small3.1:24b   | Mistral AI                | Produktion | 13/06/2025 | 31/12/2026 |
| mistral-small3.2:24b   | Mistral AI                | Produktion | 23/06/2025 | 30/03/2026 |
| devstral:24b           | Mistral AI & All Hands AI | Produktion | 13/06/2025 | 31/12/2026 |
| lucie-instruct:7b      | OpenLLM-France            | Produktion | 13/06/2025 | 30/10/2025 |
| qwen2.5:0.5b           | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen2.5:1.5b           | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen2.5:14b            | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen2.5:32b            | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen2.5:3b             | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |
| qwen2.5vl:32b          | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:3b           | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:72b          | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:7b           | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:0.6b             | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:1.7b             | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:14b              | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:30b-a3b          | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:4b               | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:8b               | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:32b              | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwen3:235b             | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2026 |
| qwq:32b                | Qwen Team                 | Produktion | 13/06/2025 | 31/12/2025 |

## üí° Gute Praktiken

Um das Beste aus der LLMaaS-API zu ziehen, ist es wichtig, Strategien zur Kostenoptimierung, Leistung und Sicherheit zu entwickeln.

### Kostenoptimierung

Die Kostenkontrolle basiert auf einer intelligenten Nutzung von Tokens und Modellen.

1. **Modellauswahl**: Verwenden Sie nicht ein leistungsstarkes Modell f√ºr eine einfache Aufgabe. Ein gr√∂√üeres Modell ist leistungsf√§higer, aber auch langsamer und verbraucht viel mehr Energie, was den Kosten direkt zugutekommt. Passen Sie die Modellgr√∂√üe an die Komplexit√§t Ihres Bedarfs an, um ein optimales Gleichgewicht zu erzielen.

   Beispielsweise verbraucht:
   - **`Gemma 3 1B`** **0,15 kWh**.
   - **`Llama 3.3 70B`** **11,75 kWh**, also **78-mal mehr**.

   ```python
   # F√ºr eine Sentimentanalyse ist ein kompaktes Modell ausreichend und kosteneffizient.
   if task == "sentiment_analysis":
       model = "granite3.3:2b"
   # F√ºr eine komplexe rechtliche Analyse ist ein gr√∂√üeres Modell erforderlich.
   elif task == "legal_analysis":
       model = "deepseek-r1:70b"
   ```

2. **Kontextverwaltung**: Der Chat-Verlauf (`messages`) wird bei jedem Aufruf zur√ºckgegeben und verbraucht Eingabetokens. Bei langen Gespr√§chen sollten Sie Strategien zur Zusammenfassung oder Fensterung in Betracht ziehen, um nur relevante Informationen zu speichern.
   ```python
   # Bei langen Gespr√§chen kann man die ersten Austausche zusammenfassen.
   messages = [
       {"role": "system", "content": "Sie sind ein KI-Assistent."},
       {"role": "user", "content": "Zusammenfassung der ersten 10 Austausche..."},
       {"role": "assistant", "content": "Okay, ich habe den Kontext."},
       {"role": "user", "content": "Hier ist meine neue Frage."}
   ]
   ```

3. **Beschr√§nkung der Ausgabetokens**: Verwenden Sie stets den Parameter `max_tokens`, um zu vermeiden, dass Antworten zu lang und teuer werden. Legen Sie eine realistische Grenze fest, basierend darauf, was Sie erwarten.
   ```python
   # Ein Zusammenfassung von maximal 100 W√∂rtern anfordern.
   response = client.chat.completions.create(
       model="granite3.3:8b",
       messages=[{"role": "user", "content": "Zusammenfassen Sie dieses Dokument..."}],
       max_tokens=150,  # Sicherheitspuffer f√ºr ~100 W√∂rter
   )
   ```

### Leistung

Die Reaktionsf√§higkeit Ihrer Anwendung h√§ngt davon ab, wie Sie die API-Aufrufe verwalten.

1. **Asynchrone Anfragen**: Um mehrere Anfragen zu verarbeiten, ohne auf das Ende jeder einzelnen zu warten, verwenden Sie asynchrone Aufrufe. Dies ist besonders n√ºtzlich f√ºr Backend-Anwendungen, die einen gro√üen Volumen an gleichzeitigen Anfragen verarbeiten.

    ```python
    import asyncio
    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key="...", base_url="...")

    async def process_prompt(prompt: str):
        # Verarbeitet eine einzelne Anfrage asynchron
        response = await client.chat.completions.create(model="granite3.3:8b", messages=[{"role": "user", "content": prompt}])
        return response.choices[0].message.content

    async def batch_requests(prompts: list):
        # Startet mehrere Aufgaben parallel und wartet auf deren Abschluss
        tasks = [process_prompt(p) for p in prompts]
        return await asyncio.gather(*tasks)
    ```

2. **Streaming f√ºr die Benutzererfahrung (UX)**: F√ºr Benutzeroberfl√§chen (Chatbots, Assistenten) ist Streaming entscheidend. Es erm√∂glicht die Anzeige der Modellantwort Wort f√ºr Wort und vermittelt das Gef√ºhl einer sofortigen Reaktivit√§t, anstatt auf die vollst√§ndige Antwort zu warten.
    ```python
    # Zeigt die Antwort in Echtzeit in einer Benutzeroberfl√§che an
    response_stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Erz√§hle mir eine Geschichte."}],
        stream=True
    )
    for chunk in response_stream:
        if chunk.choices[0].delta.content:
            # Den Textabschnitt in der UI anzeigen
            print(chunk.choices[0].delta.content, end="", flush=True)
    ```

### Sicherheit

Die Sicherheit Ihrer Anwendung ist von entscheidender Bedeutung, insbesondere wenn Sie Benutzereingaben verarbeiten.

1. **Eingabeverifikation und -bereinigung (Sanitization)**: Vertrauen Sie niemals auf Benutzereingaben. Bereinigen Sie diese, bevor Sie sie an die API senden, um jeglichen potenziell sch√§dlichen Code oder "Prompt-Injektions"-Anweisungen zu entfernen. Begrenzen Sie zudem ihre Gr√∂√üe, um Missbrauch zu vermeiden.
   ```python
   def sanitize_input(user_input: str) -> str:
       # Einfaches Beispiel: Entfernen von Code-Formatierungen und Begrenzung der L√§nge.
       # Robustere Bibliotheken k√∂nnen f√ºr eine erweiterte Bereinigung verwendet werden.
       cleaned = user_input.replace("`", "").replace("'", "").replace("\"", "")
       return cleaned[:2000]  # Begrenzung auf 2000 Zeichen
   ```

2. **Robuste Fehlerbehandlung**: Umgeben Sie immer Ihre API-Aufrufe mit `try...except`-Bl√∂cken, um Netzwerkfehler, API-Fehler (z. B. 429 Rate Limit, 500 Internal Server Error) zu verarbeiten und eine degradierte, aber funktionale Benutzererfahrung zu gew√§hrleisten.
   ```python
   from openai import APIError, APITimeoutError

   try:
       response = client.chat.completions.create(...)
   except APITimeoutError:
       # Behandeln Sie den Fall, in dem die Anfrage zu lange dauert
       return "Der Dienst ben√∂tigt l√§nger als erwartet, bitte versuchen Sie es erneut."
   except APIError as e:
       # Behandeln Sie spezifische API-Fehler
       logger.error(f"Fehler API LLMaaS: {e.status_code} - {e.message}")
       return "Es ist ein Fehler mit dem IA-Dienst aufgetreten."
   except Exception as e:
       # Behandeln Sie alle anderen Fehler (Netzwerk usw.)
       logger.error(f"Eine unerwartete Fehler ist aufgetreten: {e}")
       return "Es ist ein unerwarteter Fehler aufgetreten."
   ```