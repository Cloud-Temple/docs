---
title: Konzepte
sidebar_position: 3
---

# Konzepte und Architektur LLMaaS

## √úbersicht

Der Dienst **LLMaaS** (Large Language Models as a Service) von Cloud Temple bietet sicheren und souver√§nen Zugang zu den fortschrittlichsten KI-Modellen mit der **Zertifizierung SecNumCloud** der ANSSI.

## üèóÔ∏è Technische Architektur

### Cloud-Temple-Infrastruktur

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Technische Architektur von LLMaaS Cloud Temple" />

### Hauptkomponenten

#### 1. **API Gateway LLMaaS**
- **Kompatibel mit OpenAI** : Transparente Integration mit bestehendem √ñkosystem
- **Rate Limiting** : Quotenverwaltung pro Abrechnungstier
- **Load Balancing** : Intelligente Verteilung auf 12 GPU-Maschinen
- **Monitoring** : Echtzeit-Metriken und Alarmierung

#### 2. **Service d'Authentification**
- **Sichere API-Tokens** : Automatische Rotation
- **Zugriffssteuerung** : Granulare Berechtigungen pro Modell
- **Audit-Protokolle** : Vollst√§ndige Nachverfolgbarkeit der Zugriffe

## ü§ñ Modelle und Tokens

### Modellkatalog

*Vollst√§ndiger Katalog: [Modellliste](./models)*

### Token-Verwaltung

#### **Typen von Tokens**
- **Eingabetokens** : Ihr Prompt und Kontext
- **Ausgabetokens** : Antwort, die vom Modell generiert wird
- **Systemtokens** : Metadaten und Anweisungen

#### **Kostenberechnung**
```
Co√ªt total = (Tokens entr√©e √ó 0.9‚Ç¨/M) + (Tokens sortie √ó 4‚Ç¨/M) +  (Tokens sortie Raisonnement √ó 21‚Ç¨/M)
```

#### **Optimierung**
- **Kontextfenster** : Wiederholen Sie Gespr√§che, um zu sparen
- **Passende Modelle** : W√§hlen Sie die Gr√∂√üe entsprechend der Komplexit√§t
- **Max. Tokens** : Beschr√§nken Sie die L√§nge der Antworten

### Tokenisierung

```python
```

# Beispiel zur Token-Sch√§tzung
def estimate_tokens(text: str) -> int:
    """Approximative Sch√§tzung: 1 Token ‚âà 4 Zeichen"""
    return len(text) // 4

prompt = "Erkl√§ren Sie die Photosynthese"
response_max = 200  # gew√ºnschte maximale Tokens

estimated_input = estimate_tokens(prompt)  # ~6 Tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Kosten gesch√§tzt: {total_cost:.6f}‚Ç¨")
```

## üîí Sicherheit und Konformit√§t

### Zertifizierung SecNumCloud

Der LLMaaS-Service wird auf einer technischen Infrastruktur verarbeitet, die √ºber die **Zertifizierung SecNumCloud 3.2** der ANSSI verf√ºgt, die garantiert:

#### **DatenSchutz**
- **Ende-zu-Ende-Verschl√ºsselung** : TLS 1.3 f√ºr alle Kommunikationen
- **Sicherer Speicher** : Verschl√ºsselte Daten im Ruhezustand (AES-256)
- **Isolation** : Dedizierte Umgebungen pro Mandant

#### **Digitale Souver√§nit√§t**
- **Hosting Frankreich** : Zertifizierte Cloud Temple Datacenter
- **Franz√∂sisches Recht** : Native DSGVO-Konformit√§t
- **Keine Exposition** : Keine Daten√ºbertragung in ausl√§ndische Clouds

#### **Audit und Nachverfolgbarkeit**
- **Vollst√§ndige Logs** : Alle Interaktionen werden protokolliert
- **Aufbewahrung** : Speicherung gem√§√ü rechtlichen Richtlinien
- **Compliance** : Auditberichte sind verf√ºgbar

### Sicherheitskontrollen

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Sicherheitskontrollen LLMaaS" />

## üìà Leistung und Skalierbarkeit

### Leistungsmetriken

    In Arbeit

#### **Latenz**

    In Arbeit

#### **D√©bit**

    In Arbeit

### Echtzeit-Monitoring

Zugriff √ºber **Console Cloud Temple** :
- Metriken zur Nutzung pro Modell
- Graphiken zur Latenz und Durchsatz
- Benachrichtigungen zu Leistungsschwellenwerten
- Anfragespeicher

## üåê Integration und √ñkosystem

### OpenAI-Kompatibilit√§t

Der LLMaaS-Dienst ist **kompatibel** mit der OpenAI-API:

```python
```

# Transparente Migration
from openai import OpenAI

# Vor (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# Nach (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="votre-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Identischer Code!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Cloud Temple Modell
    messages=[{"role": "user", "content": "Bonjour"}]
)
```

### Unterst√ºtztes √ñkosystem

#### **Frameworks IA**
- ‚úÖ **LangChain** : Native Integration
- ‚úÖ **Haystack** : Pipeline von Dokumenten
- ‚úÖ **Semantic Kernel** : Orchestrierung von Microsoft
- ‚úÖ **AutoGen** : Konversationelle Agenten

#### **Tools Entwicklung**
- ‚úÖ **Jupyter** : interaktive Notebooks
- ‚úÖ **Streamlit** : schnelle Webanwendungen
- ‚úÖ **Gradio** : Benutzeroberfl√§chen f√ºr KI
- ‚úÖ **FastAPI** : Backend-APIs

#### **No-Code-Plattformen**
- ‚úÖ **Zapier** : Automatisierungen
- ‚úÖ **Make** : visuelle Integrationen
- ‚úÖ **Bubble** : Webanwendungen

## üîÑ Lebenszyklus der Modelle

### Modellaktualisierungen

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="Lebenszyklus der Modelle LLMaaS" />

### Versionierungspolitik

- **Stabile Modelle** : Festgelegte Versionen, die 6 Monate lang verf√ºgbar sind.
- **Experimentelle Modelle** : Beta-Versionen f√ºr Early Adopter.
- **Abl√∂sung** : 3 Monate Vorank√ºndigung vor der Entfernung.
- **Migration** : Professionelle Dienstleistungen stehen zur Verf√ºgung, um Ihre √úberg√§nge zu gew√§hrleisten.

## üí° Gute Praktiken

### Kostenoptimierung

1. **Modellauswahl**
   ```python
   # Einfache Aufgabe ‚Üí leichtes Modell
   if task_complexity == "simple":
       model = "llama3.2:3b"  # Billiger
   else:
       model = "llama3.1:70b"  # Leistungsst√§rker
   ```

2. **Kontextverwaltung**
   ```python
   # Unterhaltungen wiederverwenden
   messages = [
       {"role": "system", "content": "Sie sind ein KI-Assistent."},
       {"role": "user", "content": "Frage 1"},
       {"role": "assistant", "content": "Antwort 1"},
       {"role": "user", "content": "Frage 2"}  # Wiederverwendet den Kontext
   ]
   ```

3. **Tokenbegrenzung**
   ```python
   response = client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       max_tokens=100,  # Begrenzt die L√§nge
       temperature=0.7
   )
   ```

### Leistung

1. **Asynchrone Anfragen**
   ```python
   import asyncio
   import aiohttp
   
   async def batch_requests(prompts):
       tasks = [process_prompt(prompt) for prompt in prompts]
       return await asyncio.gather(*tasks)
   ```

2. **Streaming f√ºr UX**
   ```python
   # Echtzeit-Interface
   for chunk in client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       stream=True
   ):
       if chunk.choices[0].delta.content:
           print(chunk.choices[0].delta.content, end="")
   ```

### Sicherheit

1. **Eingabeverifikation**
   ```python
   def sanitize_input(user_input: str) -> str:
       # Potenzielle Injectionen bereinigen
       cleaned = user_input.replace("```", "")
       return cleaned[:1000]  # Gr√∂√üe begrenzen
   ```

2. **Fehlerbehandlung**
   ```python
   try:
       response = client.chat.completions.create(...)
   except Exception as e:
       logger.error(f"LLMaaS-Fehler: {e}")
       return "Entschuldigung, ein tempor√§rer Fehler."
   ```