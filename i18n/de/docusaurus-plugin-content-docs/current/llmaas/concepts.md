---
title: Konzepte
sidebar_position: 3
---

# Konzepte und Architektur LLMaaS

## √úbersicht

Der **LLMaaS**-Dienst (Large Language Models as a Service) von Cloud Temple bietet sicheren und souver√§nen Zugriff auf die leistungsst√§rksten KI-Modelle mit der **SecNumCloud-Zertifizierung** der ANSSI.

## üèóÔ∏è Technische Architektur

### Cloud Temple Infrastruktur

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Technische Architektur LLMaaS Cloud Temple" />

### Hauptkomponenten

#### 1. **API Gateway LLMaaS**
- **OpenAI-kompatibel** : Nahtlose Integration in bestehende √ñkosysteme
- **Rate Limiting** : Quotenverwaltung pro Abrechnungstier
- **Load Balancing** : Intelligente Verteilung auf 12 GPU-Maschinen
- **Monitoring** : Echtzeit-Metriken und Alarmierung

#### 2. **Authentifizierungsdienst**
- **Sichere API-Tokens** : Automatische Rotation
- **Zugriffssteuerung** : Granulare Berechtigungen pro Modell
- **Audit-Trails** : Vollst√§ndige Nachverfolgbarkeit der Zugriffe

## ü§ñ Modelle und Tokens

### Modellkatalog

*Vollst√§ndiger Katalog : [Liste der Modelle](./models)*

### Token-Verwaltung

#### **Token-Typen**
- **Eingabetokens** : Ihr Prompt und Kontext
- **Ausgabetokens** : Von dem Modell generierte Antwort
- **Systemtokens** : Metadaten und Anweisungen

#### **Kostenberechnung**
```
Gesamtkosten = (Eingabetokens √ó 0,9‚Ç¨/M) + (Ausgabetokens √ó 4‚Ç¨/M) +  (Ausgabetokens Reasoning √ó 21‚Ç¨/M)
```

#### **Optimierung**
- **Context Window** : Wiederholte Nutzung von Gespr√§chen zur Einsparung
- **Passende Modelle** : Gr√∂√üenwahl entsprechend der Komplexit√§t
- **Max Tokens** : Begrenzung der Antwortl√§nge

### Tokenisierung

```python
# Beispiel zur Token-Sch√§tzung
def estimate_tokens(text: str) -> int:
    """N√§herungswert: 1 Token ‚âà 4 Zeichen"""
    return len(text) // 4

prompt = "Erkl√§re die Photosynthese"
response_max = 200  # gew√ºnschte maximale Tokens

estimated_input = estimate_tokens(prompt)  # ~6 Tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Kostenabsch√§tzung: {total_cost:.6f}‚Ç¨")
```

## üîí Sicherheit und Compliance

### SecNumCloud-Zertifizierung

Der LLMaaS-Dienst wird auf einer technischen Infrastruktur betrieben, die die **SecNumCloud 3.2-Zertifizierung** der ANSSI besitzt, was folgendes gew√§hrleistet:

#### **DatenSchutz**
- **Ende-zu-Ende-Verschl√ºsselung** : TLS 1.3 f√ºr alle Austausche
- **Sichere Speicherung** : Daten verschl√ºsselt im Ruhezustand (AES-256)
- **Isolation** : Dedizierte Umgebungen pro Tenant

#### **Digitale Souver√§nit√§t**
- **Hosting in Frankreich** : Zertifizierte Cloud Temple Datacenters
- **Franz√∂sisches Recht** : Native RGPD-Konformit√§t
- **Keine Exposition** : Kein Daten√ºbertrag in ausl√§ndische Clouds

#### **Audit und Nachverfolgbarkeit**
- **Vollst√§ndige Logs** : Alle Interaktionen nachverfolgbar
- **Retentionspolitik** : Speicherung gem√§√ü gesetzlichen Vorgaben
- **Compliance** : Audit-Berichte verf√ºgbar

### Sicherheitskontrollen

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Sicherheitskontrollen LLMaaS" />

## üìà Leistung und Skalierbarkeit

### Leistungs-Metriken

    In progress


#### **Latenz**

    In progress

#### **Durchsatz**

    In progress


### Echtzeit-Monitoring

Zugriff √ºber die **Cloud Temple Console** :
- Nutzungsmetriken pro Modell
- Latenz- und Durchsatz-Graphen
- Alarme bei Leistungs-Schwellwerten
- Anfragespeicher

## üåê Integration und √ñkosystem

### OpenAI-Kompatibilit√§t

Der LLMaaS-Dienst ist **kompatibel** mit der OpenAI-API :

```python
# Transparente Migration
from openai import OpenAI

# Vorher (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# Nachher (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="Ihr-Cloud-Temple-Token",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Identischer Code!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Cloud Temple Modell
    messages=[{"role": "user", "content": "Hallo"}]
)
```

### Unterst√ºtztes √ñkosystem

#### **IA-Frameworks**
- ‚úÖ **LangChain** : Native Integration
- ‚úÖ **Haystack** : Dokumenten-Pipeline
- ‚úÖ **Semantic Kernel** : Microsoft-Orchestrierung
- ‚úÖ **AutoGen** : Konversationelle Agenten

#### **Entwicklertools**
- ‚úÖ **Jupyter** : Interaktive Notebooks
- ‚úÖ **Streamlit** : Schnelle Web-Anwendungen
- ‚úÖ **Gradio** : IA-Benutzeroberfl√§chen
- ‚úÖ **FastAPI** : Backend-APIs

#### **No-Code-Plattformen**
- ‚úÖ **Zapier** : Automatisierungen
- ‚úÖ **Make** : Visuelle Integrationen
- ‚úÖ **Bubble** : Web-Anwendungen

## üîÑ Modell-Lebenszyklus

### Modell-Updates

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="Modell-Lebenszyklus LLMaaS" />

### Versionierungspolitik

- **Stabile Modelle** : Fixe Versionen f√ºr 6 Monate verf√ºgbar
- **Experimentelle Modelle** : Beta-Versionen f√ºr Early Adopters
- **Einstellung** : 3 Monate Vorank√ºndigung vor Entfernung
- **Migration** : Professionelle Dienste f√ºr Ihre √úberg√§nge verf√ºgbar

## üí° Best Practices

### Kostenoptimierung

1. **Modellwahl**
   ```python
   # Einfache Aufgabe ‚Üí leichtes Modell
   if task_complexity == "einfach":
       model = "llama3.2:3b"  # Billiger
   else:
       model = "llama3.1:70b"  # Leistungsst√§rker
   ```

2. **Kontextverwaltung**
   ```python
   # Wiederholte Nutzung von Gespr√§chen
   messages = [
       {"role": "system", "content": "Sie sind ein IA-Assistent."},
       {"role": "user", "content": "Frage 1"},
       {"role": "assistant", "content": "Antwort 1"},
       {"role": "user", "content": "Frage 2"}  # Wiederholung des Kontexts
   ]
   ```

3. **Token-Begrenzung**
   ```python
   response = client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       max_tokens=100,  # Begrenzung der L√§nge
       temperature=0.7
   )
   ```

### Leistung

1. **Asynchrone Anfragen**
   ```python
   import asyncio
   import aiohttp
   
   async def batch_requests(prompts):
       tasks = [process_prompt(prompt) for prompt in prompts]
       return await asyncio.gather(*tasks)
   ```

2. **Streaming f√ºr UX**
   ```python
   # Echtzeit-Interface
   for chunk in client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       stream=True
   ):
       if chunk.choices[0].delta.content:
           print(chunk.choices[0].delta.content, end="")
   ```

### Sicherheit

1. **Eingabeverifikation**
   ```python
   def sanitize_input(user_input: str) -> str:
       # Bereinigung von potenziellen Injection-Angriffen
       cleaned = user_input.replace("```", "")
       return cleaned[:1000]  # Begrenzung der Gr√∂√üe
   ```

2. **Fehlerbehandlung**
   ```python
   try:
       response = client.chat.completions.create(...)
   except Exception as e:
       logger.error(f"LLMaaS-Fehler: {e}")
       return "Leider ein tempor√§rer Fehler."
   ```