---
title: Konzepte
sidebar_position: 3
---

# Konzepte und Architektur LLMaaS

## üèóÔ∏è Technische Architektur

### Cloud-Temple-Infrastruktur

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Technische Architektur von LLMaaS Cloud Temple" />

### Hauptkomponenten

#### 1. **API Gateway LLMaaS**
- **Kompatibel mit OpenAI** : Transparente Integration in das bestehende √ñkosystem
- **Rate Limiting** : Quotenverwaltung pro Abrechnungstier
- **Load Balancing** : Intelligente Verteilung auf 12 GPU-Maschinen
- **Monitoring** : Echtzeit-Metriken und Alarmierung

#### 2. **Authentifizierungsdienst**
- **Sichere API-Tokens** 
- **Zugriffssteuerung** : Granulare Berechtigungen pro Modell
- **Audit-Protokolle** : Vollst√§ndige Nachverfolgbarkeit der Zugriffe

## ü§ñ Modelle und Tokens

### Katalog der Modelle

*Vollst√§ndiger Katalog: [Liste der Modelle](./models)*

### Token-Management

#### **Typen von Tokens**
- **Eingabetokens** : Ihr Prompt und Kontext
- **Ausgabetokens** : Antwort, die vom Modell generiert wird
- **Systemtokens** : Metadaten und Anweisungen

#### **Kostenberechnung**
```
Co√ªt total = (Tokens entr√©e √ó 0.9‚Ç¨/M) + (Tokens sortie √ó 4‚Ç¨/M) +  (Tokens sortie Raisonnement √ó 21‚Ç¨/M)
```

#### **Optimierung**
- **Context window** : Wiederholen Sie Gespr√§che, um zu sparen
- **Mod√®les appropri√©s** : W√§hlen Sie die Gr√∂√üe entsprechend der Komplexit√§t
- **Max tokens** : Begrenzen Sie die L√§nge der Antworten

### Tokenisierung

```python
```

# Beispiel f√ºr eine Token-Sch√§tzung
def estimate_tokens(text: str) -> int:
    """N√§herungssch√§tzung: 1 Token ‚âà 4 Zeichen"""
    return len(text) // 4

prompt = "Expliquez la photosynth√®se"
response_max = 200  # maximale gew√ºnschte Tokens

estimated_input = estimate_tokens(prompt)  # ~6 Tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Gesch√§tzte Kosten: {total_cost:.6f}‚Ç¨")
```

## üîí Sicherheit und Konformit√§t

### SecNumCloud-Zertifizierung

Der LLMaaS-Dienst wird auf einer IaaS-Cloud-Temple-Infrastruktur berechnet, die √ºber die **SecNumCloud 3.2-Zertifizierung** der ANSSI verf√ºgt, die folgendes garantiert:

#### **DatenSchutz**
- **End-to-End-Verschl√ºsselung** : TLS 1.3 f√ºr alle Austausche
- **Sichere Speicherung** : verschl√ºsselte Daten im Ruhezustand
- **Umgebungsisolation**

#### **Digitale Souver√§nit√§t**
- **Hosting in Frankreich** : Zertifizierte Cloud Temple Datacenter
- **Franz√∂sisches Recht** : RGPD-Konformit√§t
- **Kein Datenexport** : Keine √úbertragung in ausl√§ndische Clouds und keine Speicherung der Daten

#### **Audit und Nachverfolgbarkeit**
- **Vollst√§ndige Protokolle** : Alle Interaktionen protokolliert
- **Aufbewahrung** : Aufbewahrung gem√§√ü gesetzlichen Richtlinien
- **Compliance**

### Sicherheitskontrollen

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Sicherheitskontrollen LLMaaS" />

## üìà Leistung und Skalierbarkeit

### Metriken der Leistung

#### **Latenz**
- **Durchschnittliche Antwortzeit** : < 2 Sekunden f√ºr 8B-Modelle
- **Zeit bis zum ersten Token** : < 1500 ms
- **Streaming-Durchsatz** : 15‚Äì100 Tokens pro Sekunde je nach Modell

#### **Durchsatz**
- **Gleichzeitige Anfragen** : Bis zu 1000 Anfragen/Minute pro Mandant
- **Automatisches Skalieren** : Lastanpassung in Echtzeit basierend auf den angeforderten Modellen
- **Verf√ºgbarkeit** : SLA-Zielwert von 99,9 % monatlicher Verf√ºgbarkeit

### Echtzeit-Monitoring

Zugriff √ºber **Console Cloud Temple**:
- Nutzungsmetriken pro Modell
- Latenz- und Durchsatzdiagramme
- Warnungen bei Leistungsschwellenwerten
- Anfragespeicher

## üåê Integration und √ñkosystem

### OpenAI-Kompatibilit√§t

Der LLMaaS-Dienst ist **kompatibel** mit der OpenAI-API:

```python
```

# Transparente Migration
from openai import OpenAI

# Vor (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# Nach (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="votre-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# Code identisch!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Cloud Temple Modell
    messages=[{"role": "user", "content": "Bonjour"}]
)
```

### Unterst√ºtztes √ñkosystem

#### **KI-Frameworks**
- ‚úÖ **LangChain** : Native Integration
- ‚úÖ **Haystack** : Dokumenten-Pipeline
- ‚úÖ **Semantic Kernel** : Microsoft-Orchestrierung
- ‚úÖ **AutoGen** : Konversationelle Agenten

#### **Entwicklungs-Tools**
- ‚úÖ **Jupyter** : Interaktive Notizb√ºcher
- ‚úÖ **Streamlit** : Schnelle Webanwendungen
- ‚úÖ **Gradio** : KI-Benutzeroberfl√§chen
- ‚úÖ **FastAPI** : Backend-APIs

#### **No-Code-Plattformen**
- ‚úÖ **Zapier** : Automatisierungen
- ‚úÖ **Make** : Visuelle Integrationen
- ‚úÖ **Bubble** : Webanwendungen

## üîÑ Lebenszyklus der Modelle

### Modellaktualisierungen

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="LLMaaS-Modell-Lebenszyklus" />

### Versionspolitik

- **Stabile Modelle**: Festgelegte Versionen, die 6 Monate verf√ºgbar sind
- **Experimentelle Modelle**: Beta-Versionen f√ºr Early Adopter
- **Abl√∂sung**: 3 Monate Vorank√ºndigung vor dem R√ºckzug
- **Migration**: Professionelle Dienstleistungen verf√ºgbar, um Ihre √úberg√§nge zu sichern

## üí° Gute Praktiken

### Kostenoptimierung

1. **Modellauswahl**
   ```python
   # Einfache Aufgabe ‚Üí leichtes Modell
   if task_complexity == "simple":
       model = "llama3.2:3b"  # Billiger
   else:
       model = "llama3.1:70b"  # Leistungsf√§higer
   ```

2. **Kontextverwaltung**
   ```python
   # Unterhaltungen wiederverwenden
   messages = [
       {"role": "system", "content": "Sie sind ein KI-Assistent."},
       {"role": "user", "content": "Frage 1"},
       {"role": "assistant", "content": "Antwort 1"},
       {"role": "user", "content": "Frage 2"}  # Wiederverwendet den Kontext
   ]
   ```

3. **Token-Begrenzung**
   ```python
   response = client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       max_tokens=100,  # Begrenzt die L√§nge
       temperature=0.7
   )
   ```

### Leistung

1. **Asynchrone Anfragen**
   ```python
   import asyncio
   import aiohttp
   
   async def batch_requests(prompts):
       tasks = [process_prompt(prompt) for prompt in prompts]
       return await asyncio.gather(*tasks)
   ```

2. **Streaming f√ºr UX**
   ```python
   # Echtzeit-Interface
   for chunk in client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       stream=True
   ):
       if chunk.choices[0].delta.content:
           print(chunk.choices[0].delta.content, end="")
   ```

### Sicherheit

1. **Eingabeverifizierung**
   ```python
   def sanitize_input(user_input: str) -> str:
       # Potenzielle Injectionen bereinigen
       cleaned = user_input.replace("```", "")
       return cleaned[:1000]  # Gr√∂√üe begrenzen
   ```

2. **Fehlerbehandlung**
   ```python
   try:
       response = client.chat.completions.create(...)
   except Exception as e:
       logger.error(f"LLMaaS-Fehler: {e}")
       return "Leider, ein tempor√§rer Fehler."
   ```