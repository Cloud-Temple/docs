---
title: Conceptos
sidebar_position: 3
---

# Conceptos y Arquitectura LLMaaS

## üèóÔ∏è Arquitectura T√©cnica

### Infraestructura Cloud Templo

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Arquitectura T√©cnica LLMaaS Cloud Templo" />

### Componentes Principales

#### 1. **API Gateway LLMaaS**
- **Compatible con OpenAI** : Integraci√≥n transparente con el ecosistema existente
- **L√≠mite de tasas** : Gesti√≥n de los l√≠mites por nivel de facturaci√≥n
- **Balanceo de carga** : Distribuci√≥n inteligente en 12 m√°quinas GPU
- **Monitoreo** : M√©tricas en tiempo real y alertas

#### 2. **Servicio de Autenticaci√≥n**
- **Tokens API seguros** 
- **Control de acceso** : Permisos granulares por modelo
- **Auditor√≠as** : Rastreabilidad completa de los accesos

## ü§ñ Modelos y Tokens

### Cat√°logo de Modelos

*Cat√°logo completo : [Lista de modelos](./models)*

### Gesti√≥n de Tokens

#### **Tipos de Tokens**
- **Tokens de entrada** : Su prompt y contexto
- **Tokens de salida** : Respuesta generada por el modelo
- **Tokens del sistema** : Metadatos e instrucciones

#### **C√°lculo de Costos**
```
Co√ªt total = (Tokens entr√©e √ó 0.9‚Ç¨/M) + (Tokens sortie √ó 4‚Ç¨/M) +  (Tokens sortie Raisonnement √ó 21‚Ç¨/M)
```

#### **Optimizaci√≥n**
- **Ventana de contexto** : Reutilice las conversaciones para ahorrar
- **Modelos adecuados** : Elija el tama√±o seg√∫n la complejidad
- **M√°ximo de tokens** : Limite la longitud de las respuestas

### Tokenizaci√≥n

```python
```

# Ejemplo de estimaci√≥n de tokens
def estimate_tokens(text: str) -> int:
    """Estimaci√≥n aproximada: 1 token ‚âà 4 caracteres"""
    return len(text) // 4

prompt = "Explique la fotos√≠ntesis"
response_max = 200  # tokens m√°ximos deseados

estimated_input = estimate_tokens(prompt)  # ~6 tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Costo estimado: {total_cost:.6f}‚Ç¨")
```

## üîí Seguridad y Conformidad

### Calificaci√≥n SecNumCloud

El servicio LLMaaS se ejecuta en una infraestructura IaaS Cloud Temple que cuenta con la **calificaci√≥n SecNumCloud 3.2** de la ANSSI, garantizando :

#### **Protecci√≥n de datos**
- **Cifrado extremo a extremo** : TLS 1.3 para todos los intercambios
- **Almacenamiento seguro** : Datos cifrados en reposo 
- **Aislamiento** del entorno

#### **Soberan√≠a Num√©rica**
- **Almacenamiento en Francia** : Datacenters Cloud Temple certificados
- **Derecho franc√©s** : Cumplimiento RGPD 
- **Sin exposici√≥n** : Ning√∫n transferencia a nubes extranjeras y ning√∫n almacenamiento de datos

#### **Auditor√≠a y Trazabilidad**
- **Logs completos** : Todas las interacciones registradas
- **Retenci√≥n** : Conservaci√≥n seg√∫n pol√≠ticas legales
- **Cumplimiento**

### Controles de Seguridad

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Controles de Seguridad LLMaaS" />

## üìà Rendimiento y Escalabilidad

### M√©tricas de rendimiento

#### **Latencia**
- **Tiempo de respuesta promedio** : < 2 segundos para modelos 8B
- **Tiempo del primer token** : < 1500 ms
- **D√©bito de transmisi√≥n** : 15-100 tokens por segundo seg√∫n el modelo

#### **Ancho de banda**
- **Peticiones simult√°neas** : Hasta 1000 peticiones por minuto por inquilino
- **Escalado autom√°tico** : Adaptaci√≥n de la carga en tiempo real seg√∫n los modelos solicitados
- **Disponibilidad** : Objetivo de SLA del 99,9% de disponibilidad mensual

### Monitoreo en Tiempo Real

Acceso a trav√©s de **Console Cloud Temple** :
- M√©tricas de uso por modelo
- Gr√°ficos de latencia y rendimiento
- Alertas sobre umbrales de rendimiento
- Historial de consultas

## üåê Integraci√≥n y Ecosistema

### Compatibilidad OpenAI

El servicio LLMaaS es **compatible** con la API OpenAI :

```python
```

# Migraci√≥n transparente
from openai import OpenAI

# Antes (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# Despu√©s (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="votre-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# C√≥digo id√©ntico !
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Modelo Cloud Temple
    messages=[{"role": "user", "content": "Bonjour"}]
)
```

### Ecosistema Soportado

#### **Frameworks de IA**
- ‚úÖ **LangChain** : Integraci√≥n nativa
- ‚úÖ **Haystack** : Pipeline de documentos
- ‚úÖ **Semantic Kernel** : Orquestaci√≥n de Microsoft
- ‚úÖ **AutoGen** : Agentes conversacionales

#### **Herramientas de Desarrollo**
- ‚úÖ **Jupyter** : Notebooks interactivos
- ‚úÖ **Streamlit** : Aplicaciones web r√°pidas
- ‚úÖ **Gradio** : Interfaces de usuario de IA
- ‚úÖ **FastAPI** : APIs backend

#### **Plataformas No-Code**
- ‚úÖ **Zapier** : Automatizaciones
- ‚úÖ **Make** : Integraciones visuales
- ‚úÖ **Bubble** : Aplicaciones web

## üîÑ Ciclo de Vida de los Modelos

### Actualizaci√≥n de Modelos

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="Ciclo de Vida de los Modelos LLMaaS" />

### Pol√≠tica de Versioning

- **Modelos estables** : Versiones fijas disponibles durante 6 meses
- **Modelos experimentales** : Versiones beta para early adopters
- **Depreciaci√≥n** : Aviso de 3 meses antes del retiro
- **Migraci√≥n** : Servicios profesionales disponibles para garantizar sus transiciones

## üí° Buenas pr√°cticas

### Optimizaci√≥n de Costos

1. **Selecci√≥n del modelo**
   ```python
   # T√¢che simple ‚Üí mod√®le l√©ger
   if task_complexity == "simple":
       model = "llama3.2:3b"  # Moins cher
   else:
       model = "llama3.1:70b"  # Plus capable
   ```

2. **Gesti√≥n del contexto**
   ```python
   # R√©utiliser les conversations
   messages = [
       {"role": "system", "content": "Vous √™tes un assistant IA."},
       {"role": "user", "content": "Question 1"},
       {"role": "assistant", "content": "R√©ponse 1"},
       {"role": "user", "content": "Question 2"}  # R√©utilise le contexte
   ]
   ```

3. **Limitaci√≥n de tokens**
   ```python
   response = client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       max_tokens=100,  # Limite la longueur
       temperature=0.7
   )
   ```

### Rendimiento

1. **Solicitudes as√≠ncronas**
   ```python
   import asyncio
   import aiohttp
   
   async def batch_requests(prompts):
       tasks = [process_prompt(prompt) for prompt in prompts]
       return await asyncio.gather(*tasks)
   ```

2. **Streaming para UX**
   ```python
   # Interfaz en tiempo real
   for chunk in client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       stream=True
   ):
       if chunk.choices[0].delta.content:
           print(chunk.choices[0].delta.content, end="")
   ```

### Seguridad

1. **Validaci√≥n de entradas**
   ```python
   def sanitize_input(user_input: str) -> str:
       # Limpiar las inyecciones potenciales
       cleaned = user_input.replace("```", "")
       return cleaned[:1000]  # Limitar el tama√±o
   ```

2. **Gesti√≥n de errores**
   ```python
   try:
       response = client.chat.completions.create(...)
   except Exception as e:
       logger.error(f"LLMaaS error: {e}")
       return "D√©sol√©, erreur temporaire."
   ```