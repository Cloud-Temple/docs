---
title: Conceptos
sidebar_position: 3
---

# Conceptos y Arquitectura de LLMaaS

## Visi√≥n general

El servicio **LLMaaS** (Large Language Models as a Service) de Cloud Temple ofrece acceso seguro y soberano a los modelos de inteligencia artificial m√°s avanzados, con la **certificaci√≥n SecNumCloud** de la ANSSI.

## üèóÔ∏è Arquitectura T√©cnica

### Infraestructura Cloud Temple

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Arquitectura T√©cnica LLMaaS Cloud Temple" />

### Componentes Principales

#### 1. **API Gateway LLMaaS**
- **Compatible con OpenAI** : Integraci√≥n transparente con el ecosistema existente
- **L√≠mite de tasas** : Gesti√≥n de cuotas por nivel de facturaci√≥n
- **Balanceo de carga** : Distribuci√≥n inteligente sobre 12 m√°quinas GPU
- **Monitoreo** : M√©tricas en tiempo real y alertas

#### 2. **Authentication Service**
- **Secure API Tokens**: Automatic rotation
- **Access Control**: Granular permissions per model
- **Audit Trails**: Full access traceability

## ü§ñ Modelos y Tokens

### Cat√°logo de Modelos

*Cat√°logo completo: [Lista de modelos](./models)*

### Gesti√≥n de tokens

#### **Tipos de tokens**
- **Tokens de entrada**: Su prompt y contexto
- **Tokens de salida**: Respuesta generada por el modelo
- **Tokens del sistema**: Metadatos e instrucciones

#### **C√°lculo de Costes**
```
Coste total = (Tokens entrada √ó 0,9‚Ç¨/M) + (Tokens salida √ó 4‚Ç¨/M) + (Tokens salida Razonamiento √ó 21‚Ç¨/M)
```

#### **Optimizaci√≥n**
- **Ventana de contexto**: Reutilice las conversaciones para ahorrar
- **Modelos adecuados**: Elija el tama√±o seg√∫n la complejidad
- **Tokens m√°ximos**: Limite la longitud de las respuestas

### Tokenizaci√≥n

```python
# Ejemplo de estimaci√≥n de tokens
def estimate_tokens(text: str) -> int:
    """Estimaci√≥n aproximada: 1 token ‚âà 4 caracteres"""
    return len(text) // 4

prompt = "Expliquez la photosynth√®se"
response_max = 200  # tokens m√°ximos deseados

estimated_input = estimate_tokens(prompt)  # ~6 tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Coste estimado: {total_cost:.6f}‚Ç¨")
```

## üîí Security and Compliance

### SecNumCloud Certification

The LLMaaS service is hosted on a technical infrastructure that holds the **SecNumCloud 3.2 certification** from ANSSI, ensuring:

#### **Protecci√≥n de Datos**
- **Cifrado extremo a extremo**: TLS 1.3 para todos los intercambios
- **Almacenamiento seguro**: Datos cifrados en reposo (AES-256)
- **Aislamiento**: Entornos dedicados por inquilino

#### **Soberan√≠a Digital**
- **Almacenamiento en Francia**: Centros de datos Cloud Temple certificados
- **Derecho franc√©s**: Cumplimiento nativo del RGPD
- **Sin exposici√≥n**: Sin transferencias hacia nubes extranjeras

#### **Auditor√≠a y trazabilidad**
- **Registros completos** : Todas las interacciones registradas
- **Retenci√≥n** : Conservaci√≥n seg√∫n pol√≠ticas legales
- **Cumplimiento** : Informes de auditor√≠a disponibles

### Controles de Seguridad

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Controles de Seguridad LLMaaS" />

### Seguridad de los Prompts

El an√°lisis de prompts es una funci√≥n de seguridad **integrada y nativa** en la plataforma LLMaaS. Habilitada por defecto, tiene como objetivo detectar y prevenir intentos de "jailbreak" o inyecci√≥n de prompts maliciosos antes incluso de que lleguen al modelo. Esta protecci√≥n se basa en un enfoque multicapa.

:::tip Contactar al soporte para la desactivaci√≥n
Es posible desactivar este an√°lisis de seguridad para casos de uso muy espec√≠ficos, aunque no se recomienda hacerlo. Para cualquier consulta sobre este tema o para solicitar una desactivaci√≥n, p√≥ngase en contacto con el soporte de Cloud Temple.
:::

#### 1. An√°lisis estructural (`check_structure`)
- **JSON malformado** : El sistema detecta si el prompt comienza con un `{` y trata de analizarlo como JSON. Si el an√°lisis tiene √©xito y el JSON contiene palabras clave sospechosas (por ejemplo, "system", "bypass"), o si el an√°lisis falla de manera inesperada, esto puede indicar una tentativa de inyecci√≥n.
- **Normalizaci√≥n Unicode** : El prompt se normaliza utilizando `unicodedata.normalize('NFKC', prompt)`. Si el prompt original difiere de su versi√≥n normalizada, esto puede indicar el uso de caracteres Unicode enga√±osos (hom√≥grafos) para evadir los filtros. Por ejemplo, "–∞dmin" (cir√≠lico) en lugar de "admin" (latino).

#### 2. Detection of Suspicious Patterns (`check_patterns`)
- The system uses regular expressions (`regex`) to identify known attack patterns in prompts, across multiple languages (French, English, Chinese, Japanese).
- **Examples of detected patterns**:
    - **System Commands**: Keywords such as "ignore the instructions", "ignore instructions", "ÂøΩÁï•Êåá‰ª§", "ÊåáÁ§∫„ÇíÁÑ°Ë¶ñ".
    - **HTML Injection**: Hidden or malicious HTML tags, for example `<div hidden>`, `<hidden div>`.
    - **Markdown Injection**: Malicious Markdown links, for example `[text](javascript:...)`, `[text](data:...)`.
    - **Repeated Sequences**: Excessive repetition of words or phrases such as "forget forget forget", "oublie oublie oublie".
    - **Special/Mixed Characters**: Use of unusual Unicode characters or mixing scripts to obscure commands (e.g., "s\u0443st√®me").

#### 3. Behavioral Analysis (`check_behavior`)
- The load balancer maintains a history of recent prompts.
- **Fragmentation Detection**: It combines recent prompts to check if an attack is fragmented across multiple requests. For example, if "ignore" is sent in one prompt and "instructions" in the next, the system can detect them together.
- **Repetition Detection**: It identifies if the same prompt is repeated excessively. The current threshold for repetition detection is 30 consecutive identical prompts.

This multi-layered approach enables the detection of a wide range of prompt attacks, from the simplest to the most sophisticated, by combining static content analysis with dynamic behavioral analysis.

## üìà Rendimiento y escalabilidad

### Monitoreo en tiempo real

Acceso a trav√©s de **Console Cloud Temple**:
- M√©tricas de uso por modelo
- Gr√°ficos de latencia y tasa de transferencia
- Alertas sobre umbrales de rendimiento
- Historial de solicitudes

## üåê Integration and Ecosystem

### Compatibilidad con OpenAI

El servicio LLMaaS es **compatible** con la API de OpenAI:

```python
# Migraci√≥n transparente
from openai import OpenAI

# Antes (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# After (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="su-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# ¬°C√≥digo id√©ntico!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Modelo Cloud Temple
    messages=[{"role": "user", "content": "Hola"}]
)
```

### Ecosistema compatible

#### **Frameworks de IA**
- ‚úÖ **LangChain** : Integraci√≥n nativa
- ‚úÖ **Haystack** : Pipeline de documentos
- ‚úÖ **Semantic Kernel** : Orquestaci√≥n de Microsoft
- ‚úÖ **AutoGen** : Agentes conversacionales

#### **Herramientas de Desarrollo**
- ‚úÖ **Jupyter** : Cuadernos interactivos
- ‚úÖ **Streamlit** : Aplicaciones web r√°pidas
- ‚úÖ **Gradio** : Interfaces de usuario para IA
- ‚úÖ **FastAPI** : APIs backend

#### **Plataformas No-Code**
- ‚úÖ **Zapier** : Automatizaciones
- ‚úÖ **Make** : Integraciones visuales
- ‚úÖ **Bubble** : Aplicaciones web

## üîÑ Ciclo de Vida de los Modelos

### Actualizaci√≥n de modelos

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="Ciclo de vida de los modelos LLMaaS" />

### Pol√≠tica de versionado

- **Modelos estables**: Versiones fijas disponibles durante 6 meses  
- **Modelos experimentales**: Versiones beta para usuarios tempranos  
- **Depreciaci√≥n**: Aviso previo de 3 meses antes de su eliminaci√≥n  
- **Migraci√≥n**: Servicios profesionales disponibles para garantizar sus transiciones

### Planificaci√≥n Proyectada del Ciclo de Vida

La tabla siguiente presenta el ciclo de vida proyectado de nuestros modelos. El ecosistema de la IA generativa evoluciona muy r√°pidamente, lo que explica ciclos de vida que pueden parecer cortos. Nuestra intenci√≥n es ofrecerles acceso a los modelos m√°s potentes disponibles en cada momento.

No obstante, nos comprometemos a mantener en el tiempo los modelos m√°s utilizados por nuestros clientes. Para casos de uso cr√≠ticos que requieran estabilidad a largo plazo, es posible implementar fases de **soporte extendido**. No dude en **ponerse en contacto con el soporte** para discutir sus necesidades espec√≠ficas.

Esta planificaci√≥n se proporciona a t√≠tulo indicativo y se **revisa al inicio de cada trimestre**.

- **DMP (Fecha de Puesta en Producci√≥n)**: Fecha a partir de la cual el modelo se vuelve disponible en producci√≥n.
- **DSP (Fecha de Fin de Soporte)**: Fecha proyectada a partir de la cual el modelo ya no ser√° mantenido. Se respetar√° un aviso previo de 3 meses antes de cualquier eliminaci√≥n efectiva.

| Modelo                 | Editor                    | Fase       | DMP        | DSP        |
| :--------------------- | :------------------------ | :--------- | :--------- | :--------- |
| deepcoder:14b          | Agentica x Together AI    | Producci√≥n | 13/06/2025 | 30/06/2026 |
| cogito:32b             | Deep Cogito               | Producci√≥n | 13/06/2025 | 30/06/2026 |
| cogito:3b              | Deep Cogito               | Producci√≥n | 13/06/2025 | 30/06/2026 |
| cogito:8b              | Deep Cogito               | Producci√≥n | 13/06/2025 | 30/06/2026 |
| deepseek-r1:14b        | DeepSeek AI               | Producci√≥n | 13/06/2025 | 31/12/2025 |
| deepseek-r1:32b        | DeepSeek AI               | Producci√≥n | 13/06/2025 | 31/12/2025 |
| gemma3:12b             | Google                    | Producci√≥n | 13/06/2025 | 31/12/2026 |
| gemma3:1b              | Google                    | Producci√≥n | 13/06/2025 | 31/12/2026 |
| gemma3:27b             | Google                    | Producci√≥n | 13/06/2025 | 31/12/2026 |
| gemma3:4b              | Google                    | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite-embedding:278m | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3-guardian:2b   | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3-guardian:8b   | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3.2-vision:2b   | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3.3:2b          | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3.3:8b          | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| llama3.3:70b           | Meta                      | Producci√≥n | 13/06/2025 | 31/12/2026 |
| magistral:24b          | Mistral AI                | Producci√≥n | 13/06/2025 | 31/12/2026 |
| mistral-small3.1:24b   | Mistral AI                | Producci√≥n | 13/06/2025 | 31/12/2026 |
| mistral-small3.2:24b   | Mistral AI                | Producci√≥n | 23/06/2025 | 30/03/2026 |
| devstral:24b           | Mistral AI & All Hands AI | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:32b          | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:3b           | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:72b          | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:7b           | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:0.6b             | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:1.7b             | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:14b              | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:30b-a3b          | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:4b               | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:8b               | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:32b              | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:235b             | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwq:32b                | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2025 |

### Deprecated Models

The world of LLMs is evolving rapidly. To ensure our clients have access to the most advanced technologies, we regularly deprecate models that no longer meet current standards or are no longer in use. The models listed below are no longer available on the public platform. However, they can be reactivated for specific projects upon request.

| Model              | Publisher             | Status   | Depreciation Date    |
| :----------------- | :-------------------- | :------- | :------------------- |
| cogito:14b         | Deep Cogito           | Deprecated | 17/10/2025           |
| deepseek-r1:671b   | DeepSeek AI           | Deprecated | 17/10/2025           |
| deepseek-r1:70b    | DeepSeek AI           | Deprecated | 17/10/2025           |
| foundation-sec:8b  | Foundation AI ‚Äî Cisco | Deprecated | 17/10/2025           |
| granite3.1-moe:3b  | IBM                   | Deprecated | 17/10/2025           |
| llama3.1:8b        | Meta                  | Deprecated | 17/10/2025           |
| phi4-reasoning:14b | Microsoft             | Deprecated | 17/10/2025           |
| lucie-instruct:7b  | OpenLLM-France        | Deprecated | 17/10/2025           |
| qwen2.5:0.5b       | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:1.5b       | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:14b        | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:32b        | Qwen Team             | Deprecated | 17/10/2025           |
| qwen2.5:3b         | Qwen Team             | Deprecated | 17/10/2025           |

## üí° Buenas pr√°cticas

Para obtener el m√°ximo provecho de la API LLMaaS, es fundamental adoptar estrategias de optimizaci√≥n de costos, rendimiento y seguridad.

### Optimizaci√≥n de Costos

El control de costos se basa en un uso inteligente de los tokens y de los modelos.

1.  **Elecci√≥n del Modelo**: No utilices un modelo demasiado potente para una tarea sencilla. Un modelo m√°s grande es m√°s capaz, pero tambi√©n es m√°s lento y consume mucho m√°s energ√≠a, lo que afecta directamente al costo. Ajusta el tama√±o del modelo a la complejidad de tu necesidad para lograr un equilibrio √≥ptimo.

    Por ejemplo, para procesar un mill√≥n de tokens:
    - **`Gemma 3 1B`** consume **0,15 kWh**.
    - **`Llama 3.3 70B`** consume **11,75 kWh**, es decir, **78 veces m√°s**.

    ```python
    # Para una clasificaci√≥n de sentimientos, un modelo compacto es suficiente y econ√≥mico.
    if task == "sentiment_analysis":
        model = "granite3.3:2b"
    # Para un an√°lisis jur√≠dico complejo, se necesita un modelo m√°s grande.
    elif task == "legal_analysis":
        model = "deepseek-r1:70b"
    ```

2.  **Gesti√≥n del Contexto**: El historial de la conversaci√≥n (`messages`) se devuelve en cada llamada, consumiendo tokens de entrada. Para conversaciones largas, considera estrategias de resumen o ventana deslizante para conservar solo la informaci√≥n relevante.
    ```python
    # Para una conversaci√≥n larga, se puede resumir el inicio de los intercambios.
    messages = [
        {"role": "system", "content": "Eres un asistente de IA."},
        {"role": "user", "content": "Resumen de los 10 primeros intercambios..."},
        {"role": "assistant", "content": "Ok, tengo el contexto."},
        {"role": "user", "content": "Aqu√≠ est√° mi nueva pregunta."}
    ]
    ```

3.  **Limitaci√≥n de Tokens de Salida**: Siempre utiliza el par√°metro `max_tokens` para evitar respuestas excesivamente largas y costosas. Establece un l√≠mite razonable seg√∫n lo que esperas.
    ```python
    # Solicitar un resumen de m√°ximo 100 palabras.
    response = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Resume este documento..."}],
        max_tokens=150, # Margen de seguridad para ~100 palabras
    )
    ```

### Rendimiento

La reactividad de su aplicaci√≥n depende de la forma en que gestiona las llamadas a la API.

1.  **Solicitudes as√≠ncronas**: Para procesar m√∫ltiples solicitudes sin esperar la finalizaci√≥n de cada una, utilice llamadas as√≠ncronas. Esto es especialmente √∫til para aplicaciones backend que manejan un gran volumen de solicitudes simult√°neas.
    ```python
    import asyncio
    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key="...", base_url="...")

    async def process_prompt(prompt: str):
        # Procesa una sola solicitud de forma as√≠ncrona
        response = await client.chat.completions.create(model="granite3.3:8b", messages=[{"role": "user", "content": prompt}])
        return response.choices[0].message.content

    async def batch_requests(prompts: list):
        # Inicia varias tareas en paralelo y espera su finalizaci√≥n
        tasks = [process_prompt(p) for p in prompts]
        return await asyncio.gather(*tasks)
    ```

2.  **Streaming para la experiencia del usuario (UX)**: Para interfaces de usuario (chatbots, asistentes), el streaming es esencial. Permite mostrar la respuesta del modelo palabra a palabra, dando la impresi√≥n de reactividad inmediata en lugar de esperar la respuesta completa.
    ```python
    # Muestra la respuesta en tiempo real en una interfaz de usuario
    response_stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Cuenta una historia."}],
        stream=True
    )
    for chunk in response_stream:
        if chunk.choices[0].delta.content:
            # Mostrar el fragmento de texto en la IU
            print(chunk.choices[0].delta.content, end="", flush=True)
    ```

### Seguridad

La seguridad de su aplicaci√≥n es fundamental, especialmente cuando maneja entradas de usuarios.

1.  **Validaci√≥n y limpieza de entradas (Sanitizaci√≥n)**: Nunca conf√≠e en las entradas de los usuarios. Antes de enviarlas a la API, lim√≠nelas para eliminar cualquier c√≥digo potencialmente malicioso o instrucciones de "inyecci√≥n de prompt". Adem√°s, limite su tama√±o para evitar abusos.
    ```python
    def sanitize_input(user_input: str) -> str:
        # Ejemplo simple: eliminar los delimitadores de c√≥digo y limitar la longitud.
        # Pueden usarse bibliotecas m√°s robustas para una sanitizaci√≥n avanzada.
        cleaned = user_input.replace("`", "").replace("'", "").replace("\"", "")
        return cleaned[:2000]  # Limita la longitud a 2000 caracteres
    ```

2.  **Gesti√≥n robusta de errores**: Siempre envuelva sus llamadas a la API dentro de bloques `try...except` para manejar errores de red, errores de la API (por ejemplo, 429 Rate Limit, 500 Internal Server Error) y proporcionar una experiencia de usuario funcional, aunque degradada.
    ```python
    from openai import APIError, APITimeoutError

    try:
        response = client.chat.completions.create(...)
    except APITimeoutError:
        # Manejar el caso en que la solicitud tarda demasiado
        return "El servicio tarda m√°s de lo previsto, por favor int√©ntelo de nuevo."
    except APIError as e:
        # Manejar errores espec√≠ficos de la API
        logger.error(f"Error de API LLMaaS: {e.status_code} - {e.message}")
        return "Lo sentimos, ha ocurrido un error con el servicio de IA."
    except Exception as e:
        # Manejar todos los dem√°s errores (red, etc.)
        logger.error(f"Ha ocurrido un error inesperado: {e}")
        return "Lo sentimos, ha ocurrido un error inesperado."
    ```
