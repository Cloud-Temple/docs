---
title: Conceptos
sidebar_position: 3
---

# Conceptos y Arquitectura LLMaaS

## Visi√≥n general

El servicio **LLMaaS** (Large Language Models as a Service) de Cloud Temple proporciona un acceso seguro y soberano a los modelos de inteligencia artificial m√°s avanzados, con la **calificaci√≥n SecNumCloud** de la ANSSI.

## üèóÔ∏è Arquitectura T√©cnica

### Infraestructura Cloud Temple

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Arquitectura T√©cnica LLMaaS Cloud Temple" />

### Componentes Principales

#### 1. **API Gateway LLMaaS**
- **Compatible con OpenAI**: Integraci√≥n transparente con ecosistema existente
- **L√≠mite de tasas**: Gesti√≥n de cuotas por nivel de facturaci√≥n
- **Balanceo de carga**: Distribuci√≥n inteligente en 12 m√°quinas GPU
- **Monitoreo**: M√©tricas en tiempo real y alertas

#### 2. **Servicio de Autenticaci√≥n**
- **Tokens API seguros** : Rotaci√≥n autom√°tica
- **Control de acceso** : Permisos granulares por modelo
- **Audit trails** : Rastreabilidad completa de los accesos

## ü§ñ Modelos y Tokens

### Cat√°logo de Modelos

*Cat√°logo completo : [Lista de modelos](./models)*

### Gesti√≥n de Tokens

#### **Tipos de Tokens**
- **Tokens de entrada** : Su prompt y contexto
- **Tokens de salida** : Respuesta generada por el modelo
- **Tokens del sistema** : Metadatos e instrucciones

#### **C√°lculo de los Costos**
```
Costo total = (Tokens de entrada √ó 0.9‚Ç¨/M) + (Tokens de salida √ó 4‚Ç¨/M) +  (Tokens de razonamiento √ó 21‚Ç¨/M)
```

#### **Optimizaci√≥n**
- **Ventana de contexto** : Reutilice las conversaciones para ahorrar
- **Modelos adecuados** : Elija el tama√±o seg√∫n la complejidad
- **Max tokens** : Limite la longitud de las respuestas

### Tokenizaci√≥n

```python

# Ejemplo de estimaci√≥n de tokens
def estimate_tokens(text: str) -> int:
    """Estimaci√≥n aproximada: 1 token ‚âà 4 caracteres"""
    return len(text) // 4

prompt = "Explique la fotos√≠ntesis"
response_max = 200  # tokens m√°ximos deseados

estimated_input = estimate_tokens(prompt)  # ~6 tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Costo estimado: {total_cost:.6f}‚Ç¨")
```

## üîí Seguridad y Conformidad

### Calificaci√≥n SecNumCloud

El servicio LLMaaS se ejecuta en una infraestructura t√©cnica que cuenta con la **calificaci√≥n SecNumCloud 3.2** de la ANSSI, garantizando:

#### **Protecci√≥n de los Datos**
- **Cifrado extremo a extremo** : TLS 1.3 para todos los intercambios
- **Almacenamiento seguro** : Datos cifrados en reposo (AES-256)
- **Aislamiento** : Entornos dedicados por inquilino

#### **Soberan√≠a Digital**
- **Almacenamiento en Francia** : Datacenters Cloud Temple certificados
- **Derecho franc√©s** : Conformidad con el RGPD nativa
- **Sin exposici√≥n** : Ning√∫n transferencia a nubes extranjeras

#### **Auditor√≠a y Rastreabilidad**
- **Registros completos** : Todas las interacciones registradas
- **Retenci√≥n** : Conservaci√≥n seg√∫n pol√≠ticas legales
- **Cumplimiento** : Informes de auditor√≠a disponibles

### Controles de Seguridad

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Controles de Seguridad LLMaaS" />

### Seguridad de los Prompts

El an√°lisis de prompts es una **funcionalidad de seguridad nativa e integrada** en la plataforma LLMaaS. Activada por defecto, tiene como objetivo detectar y prevenir intentos de "jailbreak" o inyecci√≥n de prompts maliciosos antes de que lleguen al modelo. Esta protecci√≥n se basa en un enfoque multicapa.

:::tip P√≥ngase en contacto con el soporte para desactivar
Es posible desactivar este an√°lisis de seguridad para casos de uso muy espec√≠ficos, aunque no se recomienda. Para cualquier consulta al respecto o para solicitar una desactivaci√≥n, p√≥ngase en contacto con el soporte de Cloud Temple.
:::

#### 1. An√°lisis estructural (`check_structure`)
- **Verificaci√≥n de JSON malformado**: El sistema detecta si el prompt comienza con un `{` y intenta analizarlo como JSON. Si el an√°lisis tiene √©xito y el JSON contiene palabras clave sospechosas (ej: "system", "bypass"), o si el an√°lisis falla de manera inesperada, esto puede indicar una tentativa de inyecci√≥n.
- **Normalizaci√≥n Unicode**: El prompt se normaliza utilizando `unicodedata.normalize('NFKC', prompt)`. Si el prompt original difiere de su versi√≥n normalizada, esto puede indicar el uso de caracteres Unicode enga√±osos (homoglifos) para evadir los filtros. Por ejemplo, "–∞dmin" (cir√≠lico) en lugar de "admin" (latino).

#### 2. Detecci√≥n de Patrones Sospechosos (`check_patterns`)
- El sistema utiliza expresiones regulares (`regex`) para identificar patrones conocidos de ataques de prompts, en varios idiomas (franc√©s, ingl√©s, chino, japon√©s).
- **Ejemplos de patrones detectados** :
    - **Comandos del Sistema** : Palabras clave como "ignore las instrucciones", "ignore instructions", "ÂøΩÁï•Êåá‰ª§", "ÊåáÁ§∫„ÇíÁÑ°Ë¶ñ".
    - **Inyecci√≥n HTML** : Etiquetas HTML ocultas o maliciosas, por ejemplo `<div oculto>`, `<hidden div>`.
    - **Inyecci√≥n Markdown** : Enlaces Markdown maliciosos, por ejemplo `[texto](javascript:...)`, `[texto](data:...)`.
    - **Secuencias Repetidas** : Repetici√≥n excesiva de palabras o frases como "olvida olvida olvida", "forget forget forget".
    - **Caracteres Especiales/Mixtos** : Uso de caracteres Unicode inusuales o mezcla de scripts para ocultar comandos (ej: "s\u0443st√®me").

#### 3. An√°lisis Comportamental (`check_behavior`)
- El balanceador de carga mantiene un historial de prompts recientes.
- **Detecci√≥n de Fragmentaci√≥n**: Combina los prompts recientes para ver si un ataque est√° fragmentado en varias solicitudes. Por ejemplo, si "ignore" se env√≠a en un prompt y "instructions" en el siguiente, el sistema puede detectarlos juntos.
- **Detecci√≥n de Repetici√≥n**: Identifica si el mismo prompt se repite de manera excesiva. El umbral actual para la detecci√≥n de repetici√≥n es de **30 prompts consecutivos id√©nticos**.

Este enfoque multicapa permite detectar una amplia gama de ataques de prompts, desde los m√°s simples hasta los m√°s sofisticados, combinando el an√°lisis est√°tico del contenido y el an√°lisis din√°mico del comportamiento.

## üìà Rendimiento y Escalabilidad

### Monitoreo en Tiempo Real

Acceso a trav√©s de **Console Cloud Temple** :
- M√©tricas de uso por modelo
- Gr√°ficos de latencia y rendimiento
- Alertas sobre umbrales de rendimiento
- Historial de consultas

## üåê Integraci√≥n y Ecosistema

### Compatibilidad OpenAI

El servicio LLMaaS es **compatible** con la API OpenAI :

```python

# Migraci√≥n transparente
from openai import OpenAI

# Antes (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# Despu√©s (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="votre-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# C√≥digo id√©ntico !
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Modelo Cloud Temple
    messages=[{"role": "user", "content": "Bonjour"}]
)
```

### Ecosistema Soportado

#### **Frameworks de IA**
- ‚úÖ **LangChain** : Integraci√≥n nativa
- ‚úÖ **Haystack** : Pipeline de documentos
- ‚úÖ **Semantic Kernel** : Orquestaci√≥n de Microsoft
- ‚úÖ **AutoGen** : Agentes conversacionales

#### **Herramientas de Desarrollo**
- ‚úÖ **Jupyter** : Cuadernos interactivos
- ‚úÖ **Streamlit** : Aplicaciones web r√°pidas
- ‚úÖ **Gradio** : Interfaces de usuario de IA
- ‚úÖ **FastAPI** : APIs backend

#### **Plataformas No-Code**
- ‚úÖ **Zapier** : Automatizaciones
- ‚úÖ **Make** : Integraciones visuales
- ‚úÖ **Bubble** : Aplicaciones web

## üîÑ Ciclo de Vida de los Modelos

### Actualizaci√≥n de Modelos

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="Ciclo de Vida de los Modelos LLMaaS" />

### Pol√≠tica de Versioning

- **Modelos estables**: Versiones fijas disponibles durante 6 meses
- **Modelos experimentales**: Versiones beta para adoptantes tempranas
- **Depreciaci√≥n**: Aviso de 3 meses antes del retiro
- **Migraci√≥n**: Servicios profesionales disponibles para garantizar sus transiciones

### Planificaci√≥n del Ciclo de Vida Previsional

La tabla siguiente presenta el ciclo de vida previsional de nuestros modelos. El ecosistema de la IA generativa evoluciona muy r√°pidamente, lo que explica ciclos de vida que pueden parecer cortos. Nuestra intenci√≥n es brindarle acceso a los modelos m√°s potentes disponibles en el momento.

Sin embargo, nos comprometemos a preservar en el tiempo los modelos que son m√°s utilizados por nuestros clientes. Para casos de uso cr√≠ticos que requieren estabilidad a largo plazo, es posible realizar fases de **soporte extendido**. No dude en **contactar al soporte** para discutir sus necesidades espec√≠ficas.

Esta planificaci√≥n se proporciona a t√≠tulo informativo y se **revisa al comienzo de cada trimestre**.

- **DMP (Fecha de Puesta en Producci√≥n)**: Fecha en la que el modelo se vuelve disponible en producci√≥n.
- **DSP (Fecha de Fin de Soporte)**: Fecha prevista a partir de la cual el modelo ya no ser√° mantenido. Se respetar√° un aviso de 3 meses antes de cualquier eliminaci√≥n efectiva.

| Modelo                 | Editor                    | Fase       | DMP        | DSP        |
| :--------------------- | :------------------------ | :--------- | :--------- | :--------- |
| deepcoder:14b          | Agentica x Together AI    | Producci√≥n | 13/06/2025 | 30/06/2026 |
| cogito:14b             | Deep Cogito               | Producci√≥n | 13/06/2025 | 30/06/2026 |
| cogito:32b             | Deep Cogito               | Producci√≥n | 13/06/2025 | 30/06/2026 |
| cogito:3b              | Deep Cogito               | Producci√≥n | 13/06/2025 | 30/06/2026 |
| cogito:8b              | Deep Cogito               | Producci√≥n | 13/06/2025 | 30/06/2026 |
| deepseek-r1:14b        | DeepSeek AI               | Producci√≥n | 13/06/2025 | 31/12/2025 |
| deepseek-r1:32b        | DeepSeek AI               | Producci√≥n | 13/06/2025 | 31/12/2025 |
| deepseek-r1:671b       | DeepSeek AI               | Producci√≥n | 13/06/2025 | 31/12/2025 |
| deepseek-r1:70b        | DeepSeek AI               | Producci√≥n | 13/06/2025 | 31/12/2025 |
| foundation-sec:8b      | Foundation AI ‚Äî Cisco     | Producci√≥n | 13/06/2025 | 30/09/2025 |
| gemma3:12b             | Google                    | Producci√≥n | 13/06/2025 | 31/12/2026 |
| gemma3:1b              | Google                    | Producci√≥n | 13/06/2025 | 31/12/2026 |
| gemma3:27b             | Google                    | Producci√≥n | 13/06/2025 | 31/12/2026 |
| gemma3:4b              | Google                    | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite-embedding:278m | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3-guardian:2b   | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3-guardian:8b   | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3.1-moe:3b      | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3.2-vision:2b   | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3.3:2b          | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| granite3.3:8b          | IBM                       | Producci√≥n | 13/06/2025 | 31/12/2026 |
| llama3.1:8b            | Meta                      | Producci√≥n | 13/06/2025 | 31/12/2025 |
| llama3.3:70b           | Meta                      | Producci√≥n | 13/06/2025 | 31/12/2026 |
| phi4-reasoning:14b     | Microsoft                 | Producci√≥n | 13/06/2025 | 31/12/2025 |
| magistral:24b          | Mistral AI                | Producci√≥n | 13/06/2025 | 31/12/2026 |
| mistral-small3.1:24b   | Mistral AI                | Producci√≥n | 13/06/2025 | 31/12/2026 |
| mistral-small3.2:24b   | Mistral AI                | Producci√≥n | 23/06/2025 | 30/03/2026 |
| devstral:24b           | Mistral AI & All Hands AI | Producci√≥n | 13/06/2025 | 31/12/2026 |
| lucie-instruct:7b      | OpenLLM-France            | Producci√≥n | 13/06/2025 | 30/10/2025 |
| qwen2.5:0.5b           | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2025 |
| qwen2.5:1.5b           | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2025 |
| qwen2.5:14b            | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2025 |
| qwen2.5:32b            | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2025 |
| qwen2.5:3b             | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2025 |
| qwen2.5vl:32b          | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:3b           | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:72b          | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen2.5vl:7b           | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:0.6b             | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:1.7b             | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:14b              | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:30b-a3b          | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:4b               | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:8b               | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:32b              | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwen3:235b             | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2026 |
| qwq:32b                | Qwen Team                 | Producci√≥n | 13/06/2025 | 31/12/2025 |

## üí° Buenas Pr√°cticas

Para aprovechar al m√°ximo la API LLMaaS, es esencial adoptar estrategias de optimizaci√≥n de costos, rendimiento y seguridad.

### Optimizaci√≥n de Costos

La gesti√≥n de costos se basa en un uso inteligente de los tokens y los modelos.

1. **Selecci√≥n del Modelo**: No utilice un modelo potente para una tarea sencilla. Un modelo m√°s grande es m√°s capaz, pero tambi√©n es m√°s lento y consume mucho m√°s energ√≠a, lo que afecta directamente al costo. Ajuste el tama√±o del modelo a la complejidad de su necesidad para un equilibrio √≥ptimo.

   Por ejemplo, para procesar un mill√≥n de tokens:
   - **`Gemma 3 1B`** consume **0.15 kWh**.
   - **`Llama 3.3 70B`** consume **11.75 kWh**, es decir, **78 veces m√°s**.

   ```python
   # Para una clasificaci√≥n de sentimiento, un modelo compacto es suficiente y econ√≥mico.
   if task == "sentiment_analysis":
       model = "granite3.3:2b"
   # Para un an√°lisis jur√≠dico complejo, se necesita un modelo m√°s grande.
   elif task == "legal_analysis":
       model = "deepseek-r1:70b"
   ```

2. **Gesti√≥n del Contexto**: El historial de la conversaci√≥n (`messages`) se devuelve en cada llamada, consumiendo tokens de entrada. Para conversaciones largas, considere estrategias de resumen o ventana para conservar solo la informaci√≥n relevante.
   ```python
   # Para una conversaci√≥n larga, se puede resumir los primeros intercambios.
   messages = [
       {"role": "system", "content": "Eres un asistente de IA."},
       {"role": "user", "content": "Resumen de los 10 primeros intercambios..."},
       {"role": "assistant", "content": "Entendido, tengo el contexto."},
       {"role": "user", "content": "Aqu√≠ est√° mi nueva pregunta."}
   ]
   ```

3. **Limitaci√≥n de Tokens de Salida**: Siempre utilice el par√°metro `max_tokens` para evitar respuestas excesivamente largas y costosas. Establezca un l√≠mite razonable seg√∫n lo que espere.
   ```python
   # Solicitar un resumen de m√°ximo 100 palabras.
   response = client.chat.completions.create(
       model="granite3.3:8b",
       messages=[{"role": "user", "content": "Resume este documento..."}],
       max_tokens=150, # Margen de seguridad para ~100 palabras
   )
   ```

### Rendimiento

La reactividad de su aplicaci√≥n depende de la forma en que gestiona las llamadas a la API.

1.  **Solicitudes as√≠ncronas** : Para procesar varias solicitudes sin esperar el final de cada una, utilice llamadas as√≠ncronas. Esto es especialmente √∫til para aplicaciones backend que tratan un gran volumen de solicitudes simult√°neas.
    ```python
    import asyncio
    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key="...", base_url="...")

    async def process_prompt(prompt: str):
        # Procesa una sola solicitud de manera as√≠ncrona
        response = await client.chat.completions.create(model="granite3.3:8b", messages=[{"role": "user", "content": prompt}])
        return response.choices[0].message.content

    async def batch_requests(prompts: list):
        # Lanza varias tareas en paralelo y espera su finalizaci√≥n
        tasks = [process_prompt(p) for p in prompts]
        return await asyncio.gather(*tasks)
    ```

2.  **Streaming para la experiencia de usuario (UX)** : Para interfaces de usuario (chatbots, asistentes), el streaming es esencial. Permite mostrar la respuesta del modelo palabra por palabra, dando una sensaci√≥n de reactividad inmediata en lugar de esperar la respuesta completa.
    ```python
    # Muestra la respuesta en tiempo real en una interfaz de usuario
    response_stream = client.chat.completions.create(
        model="granite3.3:8b",
        messages=[{"role": "user", "content": "Cu√©ntame un cuento."}],
        stream=True
    )
    for chunk in response_stream:
        if chunk.choices[0].delta.content:
            # Mostrar el fragmento de texto en la UI
            print(chunk.choices[0].delta.content, end="", flush=True)
    ```

### Seguridad

La seguridad de su aplicaci√≥n es primordial, especialmente cuando maneja entradas de usuarios.

1. **Validaci√≥n y limpieza de entradas (Sanitizaci√≥n)**: Nunca conf√≠e en las entradas de los usuarios. Antes de enviarlas a la API, lim√≠telas para eliminar cualquier c√≥digo potencialmente malicioso o instrucciones de "inyecci√≥n de prompt". Tambi√©n limite su tama√±o para evitar abusos.
    ```python
    def sanitize_input(user_input: str) -> str:
        # Ejemplo simple: eliminar los delimitadores de c√≥digo y limitar la longitud.
        # Se pueden utilizar bibliotecas m√°s robustas para una sanitizaci√≥n avanzada.
        cleaned = user_input.replace("`", "").replace("'", "").replace("\"", "")
        return cleaned[:2000]  # Limita el tama√±o a 2000 caracteres
    ```

2. **Gesti√≥n robusta de errores**: Siempre envuelva sus llamadas a la API en bloques `try...except` para manejar errores de red, errores de la API (por ejemplo, 429 L√≠mite de velocidad, 500 Error Interno del Servidor) y proporcionar una experiencia de usuario funcional pero degradada.
    ```python
    from openai import APIError, APITimeoutError

    try:
        response = client.chat.completions.create(...)
    except APITimeoutError:
        # Manejar el caso donde la solicitud tarda demasiado
        return "El servicio est√° tomando m√°s tiempo del esperado, por favor intente de nuevo."
    except APIError as e:
        # Manejar errores espec√≠ficos de la API
        logger.error(f"Error de API LLMaaS: {e.status_code} - {e.message}")
        return "Lo sentimos, ha ocurrido un error con el servicio de IA."
    except Exception as e:
        # Manejar todos los dem√°s errores (red, etc.)
        logger.error(f"Ha ocurrido un error inesperado: {e}")
        return "Lo sentimos, ha ocurrido un error inesperado."
    ```