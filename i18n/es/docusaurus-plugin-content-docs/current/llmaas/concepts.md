---
title: Conceptos
sidebar_position: 3
---

# Conceptos y Arquitectura LLMaaS

## Visi√≥n General

El servicio **LLMaaS** (Modelos de Lenguaje a Gran Escala como Servicio) de Cloud Temple proporciona un acceso seguro y soberano a los modelos de inteligencia artificial m√°s avanzados, con la **calificaci√≥n SecNumCloud** de la ANSSI.

## üèóÔ∏è Arquitectura T√©cnica

### Infraestructura del Templo en la Nube

import ArchitectureLLMaaS from './images/llmaas_architecture_001.png';

<img src={ArchitectureLLMaaS} alt="Arquitectura T√©cnica de LLMaaS del Templo en la Nube" />

### Componentes Principales

#### 1. **API Gateway LLMaaS**
- **Compatible con OpenAI** : Integraci√≥n transparente con ecosistema existente
- **L√≠mite de tasas** : Gesti√≥n de cuotas por nivel de facturaci√≥n
- **Balanceo de carga** : Distribuci√≥n inteligente sobre 12 m√°quinas GPU
- **Monitoreo** : M√©tricas en tiempo real y alertas

#### 2. **Servicio de Autenticaci√≥n**
- **Tokens API seguros** : Rotaci√≥n autom√°tica
- **Control de acceso** : Permisos granulares por modelo
- **Registros de auditor√≠a** : Rastreabilidad completa de los accesos

## ü§ñ Modelos y Tokens

### Cat√°logo de Modelos

*Cat√°logo completo : [Lista de modelos](./models)*

### Gesti√≥n de Tokens

#### **Tipos de Tokens**
- **Tokens de entrada** : Su prompt y contexto
- **Tokens de salida** : Respuesta generada por el modelo
- **Tokens del sistema** : Metadatos e instrucciones

#### **C√°lculo de Costos**
```
Co√ªt total = (Tokens entr√©e √ó 0.9‚Ç¨/M) + (Tokens sortie √ó 4‚Ç¨/M) +  (Tokens sortie Raisonnement √ó 21‚Ç¨/M)
```

#### **Optimizaci√≥n**
- **Ventana de contexto** : Reutilice las conversaciones para ahorrar
- **Modelos adecuados** : Elija el tama√±o seg√∫n la complejidad
- **M√°ximos tokens** : Limite la longitud de las respuestas

### Tokenizaci√≥n

```python
```

# Ejemplo de estimaci√≥n de tokens
def estimate_tokens(text: str) -> int:
    """Estimaci√≥n aproximada: 1 token ‚âà 4 caracteres"""
    return len(text) // 4

prompt = "Expliquez la photosynth√®se"
response_max = 200  # tokens m√°ximos deseados

estimated_input = estimate_tokens(prompt)  # ~6 tokens
total_cost = (estimated_input * 0.9 + response_max * 4) / 1_000_000
print(f"Costo estimado: {total_cost:.6f}‚Ç¨")
```

## üîí Seguridad y Conformidad

### Calificaci√≥n SecNumCloud

El servicio LLMaaS se ejecuta en una infraestructura t√©cnica que cuenta con la **calificaci√≥n SecNumCloud 3.2** de la ANSSI, garantizando :

#### **Protecci√≥n de datos**
- **Cifrado extremo a extremo** : TLS 1.3 para todos los intercambios
- **Almacenamiento seguro** : Datos cifrados en reposo (AES-256)
- **Aislamiento** : Entornos dedicados por inquilino

#### **Soberan√≠a Num√©rica**
- **Almacenamiento en Francia** : Datacenters Cloud Temple certificados
- **Derecho franc√©s** : Conformidad RGPD nativa
- **Sin exposici√≥n** : Ning√∫n transferencia a nubes extranjeras

#### **Auditor√≠a y Rastreabilidad**
- **Registros completos** : Todas las interacciones registradas
- **Retenci√≥n** : Conservaci√≥n seg√∫n pol√≠ticas legales
- **Cumplimiento** : Informes de auditor√≠a disponibles

### Controles de Seguridad

import SecurityControls from './images/llmaas_security_002.png';

<img src={SecurityControls} alt="Controles de Seguridad LLMaaS" />

## üìà Rendimiento y Escalabilidad

### M√©tricas de rendimiento

    En progreso

#### **Latencia**

    En progreso

#### **D√©bito**

    En progreso

### Monitoreo en Tiempo Real

Acceso a trav√©s de **Console Cloud Temple**:
- M√©tricas de uso por modelo
- Gr√°ficos de latencia y rendimiento
- Alertas sobre umbrales de rendimiento
- Historial de consultas

## üåê Integraci√≥n y Ecosistema

### Compatibilidad OpenAI

El servicio LLMaaS es **compatible** con la API de OpenAI :

```python
```

# Migraci√≥n transparente
from openai import OpenAI

# Antes (OpenAI)
client_openai = OpenAI(api_key="sk-...")

# Despu√©s (Cloud Temple LLMaaS)
client_ct = OpenAI(
    api_key="votre-token-cloud-temple",
    base_url="https://api.ai.cloud-temple.com/v1"
)

# C√≥digo id√©ntico!
response = client_ct.chat.completions.create(
    model="granite3.3:8b",  # Modelo Cloud Temple
    messages=[{"role": "user", "content": "Bonjour"}]
)

### Ecosistema Soportado

#### **Frameworks de IA**
- ‚úÖ **LangChain** : Integraci√≥n nativa
- ‚úÖ **Haystack** : Pipeline de documentos
- ‚úÖ **Semantic Kernel** : Orquestaci√≥n de Microsoft
- ‚úÖ **AutoGen** : Agentes conversacionales

#### **Herramientas de Desarrollo**
- ‚úÖ **Jupyter** : Cuadernos interactivos
- ‚úÖ **Streamlit** : Aplicaciones web r√°pidas
- ‚úÖ **Gradio** : Interfaces de usuario de IA
- ‚úÖ **FastAPI** : APIs de backend

#### **Plataformas No-Code**
- ‚úÖ **Zapier** : Automatizaciones
- ‚úÖ **Make** : Integraciones visuales
- ‚úÖ **Bubble** : Aplicaciones web

## üîÑ Ciclo de Vida de los Modelos

### Actualizaci√≥n de Modelos

import ModelLifecycle from './images/llmaas_lifecycle_003.png';

<img src={ModelLifecycle} alt="Ciclo de Vida de los Modelos LLMaaS" />

### Pol√≠tica de Versioning

- **Modelos estables**: Versiones fijas disponibles durante 6 meses
- **Modelos experimentales**: Versiones beta para adoptadores tempranos
- **Depreciaci√≥n**: Aviso de 3 meses antes del retiro
- **Migraci√≥n**: Servicios profesionales disponibles para garantizar sus transiciones

## üí° Buenas Pr√°cticas

### Optimizaci√≥n de Costos

1. **Selecci√≥n del modelo**
   ```python
   # Tarea simple ‚Üí modelo ligero
   if task_complexity == "simple":
       model = "llama3.2:3b"  # M√°s barato
   else:
       model = "llama3.1:70b"  # M√°s potente
   ```

2. **Gesti√≥n del contexto**
   ```python
   # Reutilizar las conversaciones
   messages = [
       {"role": "system", "content": "Eres un asistente de IA."},
       {"role": "user", "content": "Pregunta 1"},
       {"role": "assistant", "content": "Respuesta 1"},
       {"role": "user", "content": "Pregunta 2"}  # Reutiliza el contexto
   ]
   ```

3. **Limitaci√≥n de tokens**
   ```python
   response = client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       max_tokens=100,  # Limita la longitud
       temperature=0.7
   )
   ```

### Rendimiento

1. **Solicitudes as√≠ncronas**
   ```python
   import asyncio
   import aiohttp
   
   async def batch_requests(prompts):
       tasks = [process_prompt(prompt) for prompt in prompts]
       return await asyncio.gather(*tasks)
   ```

2. **Streaming para UX**
   ```python
   # Interfaz en tiempo real
   for chunk in client.chat.completions.create(
       model="granite3.3:8b",
       messages=messages,
       stream=True
   ):
       if chunk.choices[0].delta.content:
           print(chunk.choices[0].delta.content, end="")
   ```

### Seguridad

1. **Validaci√≥n de entradas**
   ```python
   def sanitize_input(user_input: str) -> str:
       # Limpiar inyecciones potenciales
       cleaned = user_input.replace("```", "")
       return cleaned[:1000]  # Limitar el tama√±o
   ```

2. **Gesti√≥n de errores**
   ```python
   try:
       response = client.chat.completions.create(...)
   except Exception as e:
       logger.error(f"LLMaaS error: {e}")
       return "Lo siento, error temporal."
   ```