---
title: Gu√≠a de Inicio
sidebar_position: 3
---

# Gu√≠a de Inicio R√°pido

Esta gu√≠a le permite realizar su primera solicitud a la API LLMaaS en menos de 5 minutos.

## Requisitos previos

- Acceso a la Consola Cloud Temple
- Cuenta con permisos LLMaaS activados

## Paso 1: Generar una clave API

1. Inicie sesi√≥n en la Consola Cloud Temple
2. Acceda a la configuraci√≥n de su cuenta
3. Genere una nueva clave API LLMaaS
4. Copie y guarde la clave (solo se mostrar√° una vez)

## Paso 2: Probar la conexi√≥n

Verifique que su clave funcione listando los modelos disponibles:

```bash
curl -X GET "https://api.ai.cloud-temple.com/v1/models" \
  -H "Authorization: Bearer VUESTRA_CLAVE_API"
```

Deber√≠a recibir una lista JSON de los modelos disponibles.

## Paso 3: Primera solicitud

Realice su primera generaci√≥n de texto con un modelo r√°pido:

```bash
curl -X POST "https://api.ai.cloud-temple.com/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer VUESTRA_CLAVE_API" \
  -d '{
    "model": "granite3.3:8b",
    "messages": [
      {
        "role": "user",
        "content": "Escribe un haiku sobre la tecnolog√≠a."
      }
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

## Paso 4: Probar con Python

Instale la biblioteca requests y pruebe con c√≥digo Python:

```bash
pip install requests
```

```python
import requests
import json

# Configuraci√≥n
API_KEY = "VUESTRA_CLAVE_API"
BASE_URL = "https://api.ai.cloud-temple.com/v1"

# Encabezados
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {API_KEY}"
}

# Solicitud
payload = {
    "model": "granite3.3:8b",
    "messages": [
        {
            "role": "user",
            "content": "Explica la fotos√≠ntesis en 3 frases."
        }
    ],
    "max_tokens": 150,
    "temperature": 0.7
}

response = requests.post(
    f"{BASE_URL}/chat/completions",
    headers=headers,
    json=payload
)

if response.status_code == 200:
    result = response.json()
    print(result["choices"][0]["message"]["content"])
else:
    print(f"Error: {response.status_code}")
    print(response.text)
```

## Selecci√≥n del modelo

Para su primer prueba, use uno de estos modelos recomendados:

| Modelo | Uso | Velocidad | Nota |
|--------|--------|---------|------|
| `granite3.3:8b` | Uso general, equilibrado | R√°pido | Recomendado para principiantes |
| `qwen3:14b` | Tareas complejas | Medio | Modo "pensando" visible |
| `gemma3:4b` | Pruebas r√°pidas, prototipado | Muy r√°pido | Respuestas detalladas |

Consulte el [cat√°logo completo de modelos](./models) para m√°s opciones.

## Par√°metros recomendados

Para comenzar, use estos par√°metros:

```json
{
  "temperature": 0.7,    // Creatividad moderada
  "max_tokens": 200,     // Respuestas concisas
  "top_p": 1.0,         // Diversidad est√°ndar
  "stream": false       // Respuesta completa de una vez
}
```

## Gesti√≥n de errores comunes

### Error 401 - No autorizado
```json
{"error": {"message": "Clave API inv√°lida", "type": "invalid_request_error"}}
```
**Soluci√≥n**: Verifique su clave API en la Consola Cloud Temple.

### Error 400 - Modelo no encontrado
```json
{"error": {"message": "Modelo no encontrado", "type": "invalid_request_error"}}
```
**Soluci√≥n**: Use `/v1/models` para listar los modelos disponibles.

### Error 429 - L√≠mite de velocidad
```json
{"error": {"message": "L√≠mite de velocidad excedido", "type": "rate_limit_error"}}
```
**Soluci√≥n**: Espere unos segundos y pruebe nuevamente.

## Monitoreo del uso

En la Consola Cloud Temple, puede:
- Ver sus solicitudes en tiempo real
- Consultar su consumo de tokens
- Configurar alertas de costo
- Analizar el rendimiento por modelo

## Avanzado: Ejemplos de Tool Calling y Visi√≥n

Esta secci√≥n proporciona ejemplos de scripts Python simples y aut√≥nomos para ilustrar funciones espec√≠ficas de la API LLMaaS. Cada ejemplo est√° dise√±ado para ejecutarse directamente, con instrucciones claras para la configuraci√≥n y el uso.

---

## üí° Ejemplos de C√≥digo Aut√≥nomo

Esta secci√≥n proporciona ejemplos de scripts Python simples y aut√≥nomos para ilustrar funciones espec√≠ficas de la API LLMaaS. Cada ejemplo est√° dise√±ado para ejecutarse directamente, con instrucciones claras para la configuraci√≥n y el uso.

### 1. Ejemplo Simple de Tool Calling

El "Tool Calling" (o llamada de funci√≥n) permite a un modelo de lenguaje solicitar la ejecuci√≥n de una funci√≥n que ha definido en su c√≥digo. Es una funci√≥n poderosa para conectar los LLMs con herramientas externas (API, bases de datos, etc.).

El flujo es el siguiente:
1. El usuario hace una pregunta que requiere una herramienta (ej: "¬øqu√© tiempo hace?").
2. Env√≠a la pregunta y la lista de herramientas disponibles a la API.
3. El modelo, en lugar de responder directamente, devuelve una solicitud `tool_calls` pidiendo ejecutar una funci√≥n espec√≠fica con ciertos argumentos.
4. Su c√≥digo ejecuta la funci√≥n solicitada.
5. Vuelve a enviar el resultado de la funci√≥n al modelo.
6. El modelo utiliza este resultado para formular una respuesta final al usuario.

**Estructura de archivos**

Para este ejemplo, cree un directorio `simple_tool_calling` con los siguientes archivos:

- `test_tool_calling.py`: El script principal.
- `requirements.txt`: Las dependencias Python.
- `.env.example`: Un modelo para su archivo de configuraci√≥n.

**`requirements.txt`**
```txt
httpx
python-dotenv
```

**`.env.example`**
```env
# URL base de la API LLMaaS
API_URL="https://api.ai.cloud-temple.com/v1"

# Su clave API LLMaaS
API_KEY="su_clave_api_aqui"

# Opcional: Modelo predeterminado a utilizar para la prueba
# Aseg√∫rese de que este modelo sea compatible con el "tool calling"
DEFAULT_MODEL="qwen3:30b-a3b"
```

**C√≥digo fuente (`test_tool_calling.py`)**

```python
# -*- coding: utf-8 -*-
"""
Ejemplo simple de Tool Calling con la API LLMaaS.

Este script muestra c√≥mo definir una herramienta simple (una calculadora),
enviarla a un modelo compatible y interpretar la respuesta del modelo
para ejecutar la herramienta y devolver el resultado.
"""
import os
import json
import httpx
from dotenv import load_dotenv

# --- Configuraci√≥n ---
# Cargar las variables de entorno desde un archivo .env
load_dotenv()

API_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
API_KEY = os.getenv("API_KEY")
# Usar un modelo conocido para manejar bien el tool calling
MODEL = os.getenv("DEFAULT_MODEL", "qwen3:30b-a3b")

# --- Definici√≥n de la herramienta ---

def calculator(expression: str) -> str:
    """
    Eval√∫a una expresi√≥n matem√°tica simple.
    Ejemplo: "2 + 2 * 10"
    """
    try:
        # Seguridad: no usar eval() directamente en producci√≥n sin validaci√≥n estricta.
        # Para este ejemplo, limitamos los caracteres permitidos.
        allowed_chars = "0123456789+-*/(). "
        if not all(char in allowed_chars for char in expression):
            return "Error: La expresi√≥n contiene caracteres no permitidos."
        # eval() se usa aqu√≠ por simplicidad del ejemplo.
        result = eval(expression)
        return str(result)
    except Exception as e:
        return f"Error de c√°lculo: {str(e)}"

# Descripci√≥n de la herramienta en el formato esperado por la API
TOOLS_AVAILABLE = [
    {
        "type": "function",
        "function": {
            "name": "calculator",
            "description": "Eval√∫a una expresi√≥n matem√°tica. Por ejemplo, '2+2*10'.",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "La expresi√≥n matem√°tica a evaluar."
                    }
                },
                "required": ["expression"],
            },
        },
    }
]

# Mapeo entre el nombre de la herramienta y la funci√≥n Python a llamar
TOOL_FUNCTIONS_MAP = {
    "calculator": calculator
}

# --- L√≥gica principal ---

def run_chat_with_tool_calling():
    """
    Funci√≥n principal que ejecuta el escenario de prueba.
    """
    if not API_KEY:
        print("‚ùå Error: La variable de entorno API_KEY no est√° definida.")
        print("Por favor, cree un archivo .env o exp√≥rtela en su sesi√≥n.")
        return

    print(f"ü§ñ Modelo utilizado : {MODEL}")
    print("-" * 30)

    # 1. Primer llamado a la API con la pregunta del usuario
    # ---------------------------------------------------------
    print("‚û°Ô∏è Paso 1: Env√≠o de la solicitud inicial al LLM...")

    # El historial de mensajes comienza con la pregunta del usuario
    messages = [
        {"role": "user", "content": "Hola, puedes calcular 15 + (3 * 5) ?"}
    ]

    payload = {
        "model": MODEL,
        "messages": messages,
        "tools": TOOLS_AVAILABLE,
        "tool_choice": "auto",  # El modelo decide si debe usar una herramienta
    }

    try:
        with httpx.Client() as client:
            response = client.post(
                f"{API_URL}/chat/completions",
                headers={"Authorization": f"Bearer {API_KEY}"},
                json=payload,
                timeout=60,
            )
            response.raise_for_status()
            response_data = response.json()

    except httpx.HTTPStatusError as e:
        print(f"‚ùå Error API (Estado HTTP) en el paso 1: {e}")
        print(f"Respuesta de la API : {e.response.text}")
        return
    except httpx.RequestError as e:
        print(f"‚ùå Error API (Solicitud) en el paso 1: {e}")
        return

    # El mensaje del asistente contiene la solicitud de llamada de herramienta
    assistant_message = response_data["choices"][0]["message"]
    messages.append(assistant_message)

    # 2. Verificaci√≥n y ejecuci√≥n de la llamada de herramienta
    # ------------------------------------------------
    print("\n‚úÖ El LLM ha solicitado usar una herramienta.")

    if "tool_calls" not in assistant_message:
```
print("ü§î El modelo no pidi√≥ usar una herramienta. Respuesta directa:")
        print(assistant_message.get("content", "Sin contenido."))
        return

    tool_call = assistant_message["tool_calls"][0]
    function_name = tool_call["function"]["name"]
    function_args_str = tool_call["function"]["arguments"]
    tool_call_id = tool_call["id"]

    print(f"   - Herramienta a llamar : {function_name}")
    print(f"   - Argumentos       : {function_args_str}")

    if function_name in TOOL_FUNCTIONS_MAP:
        function_to_call = TOOL_FUNCTIONS_MAP[function_name]
        try:
            # Los argumentos son una cadena JSON, hay que analizarlos
            function_args = json.loads(function_args_str)
            tool_result = function_to_call(**function_args)
            print(f"   - Resultado de la herramienta : {tool_result}")
        except Exception as e:
            print(f"‚ùå Error al ejecutar la herramienta: {e}")
            tool_result = f"Error: {e}"
    else:
        print(f"‚ùå Herramienta desconocida : {function_name}")
        tool_result = f"Error: Herramienta '{function_name}' no encontrada."

    # 3. Segundo llamado a la API con el resultado de la herramienta
    # ----------------------------------------------------
    print("\n‚û°Ô∏è Paso 2: Env√≠o del resultado de la herramienta al LLM...")

    # A√±adimos el resultado de la herramienta al historial de mensajes
    messages.append(
        {
            "role": "tool",
            "tool_call_id": tool_call_id,
            "content": tool_result
        }
    )

    # Hacemos un nuevo llamado SIN herramientas esta vez para obtener la respuesta final
    payload_final = {
        "model": MODEL,
        "messages": messages,
    }

    try:
        with httpx.Client() as client:
            response_final = client.post(
                f"{API_URL}/chat/completions",
                headers={"Authorization": f"Bearer {API_KEY}"},
                json=payload_final,
                timeout=60,
            )
            response_final.raise_for_status()
            final_data = response_final.json()

    except (httpx.HTTPStatusError, httpx.RequestError) as e:
        print(f"‚ùå Error de API en el paso 2: {e}")
        return

    final_answer = final_data["choices"][0]["message"]["content"]
    print("\n‚úÖ Respuesta final del LLM :")
    print(f"üí¨ \"{final_answer}\"")


if __name__ == "__main__":
    run_chat_with_tool_calling()
```

**Uso**

1.  **Instale las dependencias :**
    ```bash
    pip install -r tests/llmaas/requirements.txt
    ```
2.  **Configure su clave API :**
    Copie `tests/llmaas/.env.example` a `tests/llmaas/.env` y reemplace `"su_clave_api_aqui"` con su clave API LLMaaS.
3.  **Ejecute el script :**
    ```bash
    python tests/llmaas/test_tool_calling.py
    ```

### 2. Ejemplo Simple de Visi√≥n (Multimodal)

Los modelos multimodales pueden analizar tanto texto como im√°genes. Este ejemplo muestra c√≥mo enviar una imagen y una pregunta al modelo para obtener una descripci√≥n de lo que "ve".

**Estructura de archivos**

Cree un directorio `simple_vision` con los siguientes archivos :

-   `test_vision.py`: El script principal.
-   `requirements.txt`: Las dependencias (incluyendo `Pillow` para generar la imagen).
-   `.env.example`: El modelo de configuraci√≥n.
-   `image_example.png`: La imagen a analizar (el script la generar√° para usted si falta).

**`requirements.txt`**
```txt
httpx
python-dotenv
Pillow
```

**`.env.example`**
```env
# URL base de la API LLMaaS
API_URL="https://api.ai.cloud-temple.com/v1"

# Su clave API LLMaaS
API_KEY="su_clave_api_aqui"

# Opcional: Modelo predeterminado a usar para la prueba
# Aseg√∫rese de que este modelo sea multimodal (visi√≥n)
DEFAULT_MODEL="granite3.2-vision:2b"
```

**C√≥digo fuente (`test_vision.py`)**

```python
# -*- coding: utf-8 -*-
"""
Ejemplo simple de uso de la API de Visi√≥n de LLMaaS.

Este script muestra c√≥mo enviar una imagen local con una pregunta
a un modelo de visi√≥n (multimodal) y mostrar su respuesta.
"""
import os
import base64
import httpx
from dotenv import load_dotenv

# --- Configuraci√≥n ---
# Cargar las variables de entorno desde un archivo .env
load_dotenv()

API_URL = os.getenv("API_URL", "https://api.ai.cloud-temple.com/v1")
API_KEY = os.getenv("API_KEY")
# Usar un modelo de visi√≥n.
MODEL = os.getenv("DEFAULT_MODEL", "granite3.2-vision:2b")
IMAGE_PATH = "image_example.png" # La imagen debe estar en el mismo directorio

# --- Funciones ---

def encode_image_to_base64(image_path: str) -> str:
    """
    Codifica una imagen en base64 para incluirla en la solicitud API.
    """
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except FileNotFoundError:
        print(f"‚ùå Error: El archivo de imagen '{image_path}' no se encontr√≥.")
        return ""
    except Exception as e:
        print(f"‚ùå Error al codificar la imagen: {e}")
        return ""

def generate_example_image():
    """
    Genera una imagen simple para la prueba si no existe.
    Requiere la biblioteca Pillow (PIL).
    """
    try:
        from PIL import Image, ImageDraw
        if not os.path.exists(IMAGE_PATH):
            print(f"üñºÔ∏è  La imagen '{IMAGE_PATH}' no existe, generando... ")
            # Crear una imagen simple: un cuadrado rojo sobre fondo blanco
            img = Image.new('RGB', (200, 200), color = 'white')
            draw = ImageDraw.Draw(img)
            draw.rectangle([50, 50, 150, 150], fill='red', outline='black')
            img.save(IMAGE_PATH)
            print("‚úÖ Imagen de ejemplo generada.")
    except ImportError:
        print("‚ö†Ô∏è  Advertencia: La biblioteca Pillow no est√° instalada.")
        print("   Por favor instale 'Pillow' (`pip install Pillow`) para generar la imagen de ejemplo,")
        print(f"   o coloque manualmente un archivo llamado '{IMAGE_PATH}' en este directorio.")
    except Exception as e:
        print(f"‚ùå Error al generar la imagen: {e}")


# --- L√≥gica principal ---

def run_vision_test():
    """
    Funci√≥n principal que ejecuta el escenario de prueba de visi√≥n.
    """
    if not API_KEY:
        print("‚ùå Error: La variable de entorno API_KEY no est√° definida.")
        return

    # Generar la imagen de ejemplo si es necesario
    generate_example_image()

    # Codificar la imagen en base64
    base64_image = encode_image_to_base64(IMAGE_PATH)
    if not base64_image:
        return

    print(f"ü§ñ Modelo utilizado : {MODEL}")
    print(f"üñºÔ∏è Imagen enviada : {IMAGE_PATH}")
    print("-" * 30)

    # Construcci√≥n del payload en formato multimodal
    payload = {
        "model": MODEL,
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "¬øQu√© ves en esta imagen? Describe la forma y el color principal."
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "max_tokens": 500 # Limitar la longitud de la descripci√≥n
    }

    print("‚û°Ô∏è Env√≠o de la solicitud al LLM de visi√≥n...")
    try:
        with httpx.Client() as client:
            response = client.post(
                f"{API_URL}/chat/completions",
                headers={"Authorization": f"Bearer {API_KEY}"},
                json=payload,
                timeout=120, # Los modelos de visi√≥n pueden tardar m√°s
            )
            response.raise_for_status()
            response_data = response.json()

    except httpx.HTTPStatusError as e:
        print(f"‚ùå Error de API (C√≥digo de estado HTTP): {e}")
        print(f"Respuesta de la API : {e.response.text}")
        return
    except httpx.RequestError as e:
        print(f"‚ùå Error de API (Solicitud): {e}")
        return

    final_answer = response_data["choices"][0]["message"]["content"]
    print("\n‚úÖ Respuesta del modelo :")
    print(f"üí¨ \"{final_answer}\"")


if __name__ == "__main__":
    run_vision_test()
```

**Uso**

1.  **Instale las dependencias :**
    ```bash
    pip install -r tests/llmaas/requirements.txt
    ```
2.  **Configure su clave API :**
    Copie `tests/llmaas/.env.example` a `tests/llmaas/.env` y reemplace `"su_clave_api_aqui"` con su clave API LLMaaS.
3.  **Ejecute el script :**
    ```bash
    python tests/llmaas/test_vision.py
    ```
    El script generar√° autom√°ticamente una imagen `image_example.png` si no existe.

---
## Pr√≥ximos pasos

Una vez que su primer prueba sea exitosa :

1. **Explore los modelos** : Pruebe diferentes modelos seg√∫n sus necesidades
2. **Optimice los prompts** : Mejore la calidad de las respuestas
3. **Int√©grelo en su aplicaci√≥n** : Consulte la [documentaci√≥n API](./api)
4. **Casos de uso avanzados** : Vea los [tutoriales](./tutorials)

## Soporte

En caso de problema :
- Consulte la [documentaci√≥n API completa](./api)
- Verifique el estado del servicio en la Consola
- Contacte al soporte a trav√©s de la Consola Cloud Temple